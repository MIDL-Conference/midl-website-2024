[
    {
        "number": 10,
        "UID": "O-10",
        "forum": "https://openreview.net/forum?id=DDHRGHfwji",
        "Track": "Full Paper",
        "Session": "Oral 1.1 - Segmentation",
        "Final Decision": "Oral",
        "title": "Efficiently correcting patch-based segmentation errors to control image-level performance in retinal images",
        "abstract": "Segmentation models which are deployed into clinical practice need to meet a quality standard for each image. Even when models perform well on average, they may fail at segmenting individual images with a sufficiently high quality. We propose a combined quality control and error correction framework to reach the desired segmentation quality in each image. Our framework recommends the necessary number of local patches for manual review and estimates the impact of the intervention on the Dice Score of the corrected segmentation. This allows to trade off segmentation quality against time invested into manual review. We select the patches based on uncertainty maps obtained from an ensemble of segmentation models. We evaluated our method on retinal vessel segmentation on fundus images, where the Dice Score increased substantially after reviewing only a few patches. Our method accurately estimated the review\u2019s impact on the Dice Score and we found that our framework controls the quality standard efficiently, i.e. reviewing as little as necessary.",
        "authors": "Patrick K\u00f6hler, Jeremiah Fadugba, Philipp Berens, Lisa M. Koch",
        "Time": "Day 1 \u2014 9:15-10:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - O.01",
        "Chairs": "Veronika Cheplygina & Francesco Ciompi"
    },
    {
        "number": 10,
        "UID": "F-10",
        "forum": "https://openreview.net/forum?id=DDHRGHfwji",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Efficiently correcting patch-based segmentation errors to control image-level performance in retinal images",
        "abstract": "Segmentation models which are deployed into clinical practice need to meet a quality standard for each image. Even when models perform well on average, they may fail at segmenting individual images with a sufficiently high quality. We propose a combined quality control and error correction framework to reach the desired segmentation quality in each image. Our framework recommends the necessary number of local patches for manual review and estimates the impact of the intervention on the Dice Score of the corrected segmentation. This allows to trade off segmentation quality against time invested into manual review. We select the patches based on uncertainty maps obtained from an ensemble of segmentation models. We evaluated our method on retinal vessel segmentation on fundus images, where the Dice Score increased substantially after reviewing only a few patches. Our method accurately estimated the review\u2019s impact on the Dice Score and we found that our framework controls the quality standard efficiently, i.e. reviewing as little as necessary.",
        "authors": "Patrick K\u00f6hler, Jeremiah Fadugba, Philipp Berens, Lisa M. Koch",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - O.01"
    },
    {
        "number": 203,
        "UID": "O-203",
        "forum": "https://openreview.net/forum?id=vCEMidGVbv",
        "Track": "Full Paper",
        "Session": "Oral 1.1 - Segmentation",
        "Final Decision": "Oral",
        "title": "Re-DiffiNet: Modeling discrepancy in tumor segmentation using diffusion models",
        "abstract": "Identification of tumor margins is essential for surgical decision-making for glioblastoma patients and provides reliable assistance for neurosurgeons. Despite improvements in deep learning architectures for tumor segmentation over the years, creating a fully autonomous system suitable for clinical floors remains a formidable challenge because the model predictions have not yet reached the desired level of accuracy and generalizability for clinical applications. Generative modeling techniques have seen significant improvements in recent times. Specifically, Generative Adversarial Networks (GANs) and Denoising diffusion probabilistic models (DDPMs) have been used to generate higher-quality images with fewer artifacts and finer attributes. In this work, we introduce a framework called Re-Diffinet for modeling the discrepancy between the outputs of a segmentation model like U-Net and the ground truth, using DDPMs. By explicitly modeling the discrepancy, the results show an average improvement of 0.55\\% in the Dice score and 16.28\\% in 95\\% Hausdorff Distance from cross-validation over 5-folds, compared to the state-of-the-art U-Net segmentation model. The code is available:",
        "authors": "Tianyi Ren, Abhishek Sharma, Juampablo E Heras Rivera, Lakshmi Harshitha Rebala, Ethan Honey, Agamdeep Chopra, Mehmet Kurt",
        "Time": "Day 1 \u2014 9:15-10:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - O.05",
        "Chairs": "Veronika Cheplygina & Francesco Ciompi"
    },
    {
        "number": 203,
        "UID": "F-203",
        "forum": "https://openreview.net/forum?id=vCEMidGVbv",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Re-DiffiNet: Modeling discrepancy in tumor segmentation using diffusion models",
        "abstract": "Identification of tumor margins is essential for surgical decision-making for glioblastoma patients and provides reliable assistance for neurosurgeons. Despite improvements in deep learning architectures for tumor segmentation over the years, creating a fully autonomous system suitable for clinical floors remains a formidable challenge because the model predictions have not yet reached the desired level of accuracy and generalizability for clinical applications. Generative modeling techniques have seen significant improvements in recent times. Specifically, Generative Adversarial Networks (GANs) and Denoising diffusion probabilistic models (DDPMs) have been used to generate higher-quality images with fewer artifacts and finer attributes. In this work, we introduce a framework called Re-Diffinet for modeling the discrepancy between the outputs of a segmentation model like U-Net and the ground truth, using DDPMs. By explicitly modeling the discrepancy, the results show an average improvement of 0.55\\% in the Dice score and 16.28\\% in 95\\% Hausdorff Distance from cross-validation over 5-folds, compared to the state-of-the-art U-Net segmentation model. The code is available:",
        "authors": "Tianyi Ren, Abhishek Sharma, Juampablo E Heras Rivera, Lakshmi Harshitha Rebala, Ethan Honey, Agamdeep Chopra, Mehmet Kurt",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - O.05"
    },
    {
        "number": 256,
        "UID": "O-256",
        "forum": "https://openreview.net/forum?id=3vmB43oqIO",
        "Track": "Full Paper",
        "Session": "Oral 1.1 - Segmentation",
        "Final Decision": "Oral",
        "title": "HoVer-NeXt: A Fast Nuclei Segmentation and Classification Pipeline for Next Generation Histopathology",
        "abstract": "In cancer, a variety of cell types, along with their local density and spatial organization within tissues, play a key role in driving cancer progression and modulating patient outcomes. At the basis of cancer diagnosis is the histopathological assessment of tissues, stained by hematoxylin & eosin (H&E), which gives the nuclei of cells a dark purple appearance, making them particularly distinguishable and quantifiable. The identification of individual nuclei, whether in a proliferating (mitosis) or resting state, and their further phenotyping (e.g. immune cells) is the foundation on which histopathology images can be used for further investigations into cellular interaction, prognosis or response prediction. To this end, we develop a H&E based nuclei segmentation and classification model that is both fast (1.8s/mm2 at 0.5mpp, 3.2s/mm2 at 0.25mpp) and accurate (0.84 binary F1, 0.758 mean balanced Accuracy) which allows us to investigate the cellular composition of large-scale colorectal cancer (CRC) cohorts. We extend the publicly available Lizard CRC nuclei dataset with a mitosis class and publish further validation data for the rarest classes: mitosis and eosinophils. Moreover, our pipeline is 5\u00d7 faster than the CellViT pipeline, 17\u00d7 faster than the HoVer-Net pipeline, and performs competitively on the PanNuke pan-cancer nuclei dataset (47.7 mPQTiss, +3% over HoVer-Net). Our work paves the way towards extensive single-cell information directly from H&E slides, leading to a quantitative view of whole slide images. Code, model weights as well as all additional training and validation data, are publicly available on github.",
        "authors": "Elias Baumann, Bastian Dislich, Josef Lorenz Rumberger, Iris D. Nagtegaal, Maria Rodriguez Martinez, Inti Zlobec",
        "Time": "Day 1 \u2014 9:15-10:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - O.03",
        "Chairs": "Veronika Cheplygina & Francesco Ciompi"
    },
    {
        "number": 256,
        "UID": "F-256",
        "forum": "https://openreview.net/forum?id=3vmB43oqIO",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "HoVer-NeXt: A Fast Nuclei Segmentation and Classification Pipeline for Next Generation Histopathology",
        "abstract": "In cancer, a variety of cell types, along with their local density and spatial organization within tissues, play a key role in driving cancer progression and modulating patient outcomes. At the basis of cancer diagnosis is the histopathological assessment of tissues, stained by hematoxylin & eosin (H&E), which gives the nuclei of cells a dark purple appearance, making them particularly distinguishable and quantifiable. The identification of individual nuclei, whether in a proliferating (mitosis) or resting state, and their further phenotyping (e.g. immune cells) is the foundation on which histopathology images can be used for further investigations into cellular interaction, prognosis or response prediction. To this end, we develop a H&E based nuclei segmentation and classification model that is both fast (1.8s/mm2 at 0.5mpp, 3.2s/mm2 at 0.25mpp) and accurate (0.84 binary F1, 0.758 mean balanced Accuracy) which allows us to investigate the cellular composition of large-scale colorectal cancer (CRC) cohorts. We extend the publicly available Lizard CRC nuclei dataset with a mitosis class and publish further validation data for the rarest classes: mitosis and eosinophils. Moreover, our pipeline is 5\u00d7 faster than the CellViT pipeline, 17\u00d7 faster than the HoVer-Net pipeline, and performs competitively on the PanNuke pan-cancer nuclei dataset (47.7 mPQTiss, +3% over HoVer-Net). Our work paves the way towards extensive single-cell information directly from H&E slides, leading to a quantitative view of whole slide images. Code, model weights as well as all additional training and validation data, are publicly available on github.",
        "authors": "Elias Baumann, Bastian Dislich, Josef Lorenz Rumberger, Iris D. Nagtegaal, Maria Rodriguez Martinez, Inti Zlobec",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - O.03"
    },
    {
        "number": 67,
        "UID": "O-67",
        "forum": "https://openreview.net/forum?id=xkqLQoFQbl",
        "Track": "Full Paper",
        "Session": "Oral 1.1 - Segmentation",
        "Final Decision": "Oral",
        "title": "Semi-Supervised Segmentation via Embedding Matching",
        "abstract": "Deep convolutional neural networks are widely used in medical image segmentation but require many labeled images for training. Annotating three-dimensional medical images is a time-consuming and costly process. To overcome this limitation, we propose a novel semi-supervised segmentation method that leverages mostly unlabeled images and a small set of labeled images in training. \n\nOur approach involves assessing prediction uncertainty to identify reliable predictions on unlabeled voxels from the teacher model. These voxels serve as pseudo-labels for training the student model. In voxels where the teacher model produces unreliable predictions, pseudo-labeling is carried out based on voxel-wise embedding correspondence using reference voxels from labeled images.\n\nWe applied this method to automate hip bone segmentation in CT images, achieving notable results with just 4 CT scans. The proposed approach yielded a Hausdorff distance with 95th percentile (HD95) of 3.30 and IoU of 0.929, surpassing existing methods achieving HD95 (4.07) and IoU (0.927) at their best.",
        "authors": "weiyi xie, Nathalie Willems, Nikolas Lessmann, Tom Gibbons, Daniele De Massari",
        "Time": "Day 1 \u2014 9:15-10:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - O.02",
        "Chairs": "Veronika Cheplygina & Francesco Ciompi"
    },
    {
        "number": 67,
        "UID": "F-67",
        "forum": "https://openreview.net/forum?id=xkqLQoFQbl",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Semi-Supervised Segmentation via Embedding Matching",
        "abstract": "Deep convolutional neural networks are widely used in medical image segmentation but require many labeled images for training. Annotating three-dimensional medical images is a time-consuming and costly process. To overcome this limitation, we propose a novel semi-supervised segmentation method that leverages mostly unlabeled images and a small set of labeled images in training. \n\nOur approach involves assessing prediction uncertainty to identify reliable predictions on unlabeled voxels from the teacher model. These voxels serve as pseudo-labels for training the student model. In voxels where the teacher model produces unreliable predictions, pseudo-labeling is carried out based on voxel-wise embedding correspondence using reference voxels from labeled images.\n\nWe applied this method to automate hip bone segmentation in CT images, achieving notable results with just 4 CT scans. The proposed approach yielded a Hausdorff distance with 95th percentile (HD95) of 3.30 and IoU of 0.929, surpassing existing methods achieving HD95 (4.07) and IoU (0.927) at their best.",
        "authors": "weiyi xie, Nathalie Willems, Nikolas Lessmann, Tom Gibbons, Daniele De Massari",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - O.02"
    },
    {
        "number": 70,
        "UID": "O-70",
        "forum": "https://openreview.net/forum?id=rVx9DiR5Ha",
        "Track": "Full Paper",
        "Session": "Oral 1.1 - Segmentation",
        "Final Decision": "Oral",
        "title": "Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance Segmentation",
        "abstract": "Semi-supervised segmentation methods have demonstrated promising results in natural scenarios, providing a solution to reduce dependency on manual annotation. However, these methods face significant challenges when directly applied to pathological images due to the subtle color differences between nuclei and tissues, as well as the significant morphological variations among nuclei. Consequently, the generated pseudo-labels often contain much noise, especially at the nuclei boundaries. To address the above problem, this paper proposes a boundary-aware contrastive learning network to denoise the boundary noise in a semi-supervised nuclei segmentation task. The model has two key designs: a low-resolution denoising (LRD) module and a cross-RoI contrastive learning (CRC) module. The LRD improves the smoothness of the nuclei boundary by pseudo-labels denoising, and the CRC enhances the discrimination between foreground and background by boundary feature contrastive learning. We conduct extensive experiments to demonstrate the superiority of our proposed method over existing semi-supervised instance segmentation methods.",
        "authors": "Ye Zhang, Ziyue Wang, Yifeng Wang, Hao Bian, Linghan Cai, Hengrui Li, Lingbo Zhang, Yongbing Zhang",
        "Time": "Day 1 \u2014 9:15-10:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - O.04",
        "Chairs": "Veronika Cheplygina & Francesco Ciompi"
    },
    {
        "number": 70,
        "UID": "F-70",
        "forum": "https://openreview.net/forum?id=rVx9DiR5Ha",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance Segmentation",
        "abstract": "Semi-supervised segmentation methods have demonstrated promising results in natural scenarios, providing a solution to reduce dependency on manual annotation. However, these methods face significant challenges when directly applied to pathological images due to the subtle color differences between nuclei and tissues, as well as the significant morphological variations among nuclei. Consequently, the generated pseudo-labels often contain much noise, especially at the nuclei boundaries. To address the above problem, this paper proposes a boundary-aware contrastive learning network to denoise the boundary noise in a semi-supervised nuclei segmentation task. The model has two key designs: a low-resolution denoising (LRD) module and a cross-RoI contrastive learning (CRC) module. The LRD improves the smoothness of the nuclei boundary by pseudo-labels denoising, and the CRC enhances the discrimination between foreground and background by boundary feature contrastive learning. We conduct extensive experiments to demonstrate the superiority of our proposed method over existing semi-supervised instance segmentation methods.",
        "authors": "Ye Zhang, Ziyue Wang, Yifeng Wang, Hao Bian, Linghan Cai, Hengrui Li, Lingbo Zhang, Yongbing Zhang",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - O.04"
    },
    {
        "number": 105,
        "UID": "F-105",
        "forum": "https://openreview.net/forum?id=OzhonBbVAn",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Annotation-Efficient Strategy for Segmentation of 3D Body Composition",
        "abstract": "Body composition as a diagnostic and prognostic biomarker is gaining importance in various medical fields such as oncology. Therefore, accurate quantification methods are necessary, like analyzing CT images. While several studies introduced deep learning approaches to automatically segment a single slice, quantifying body composition in 3D remains understudied due to the high required annotation effort. This study proposes an annotation-efficient strategy using an iterative self-learning approach with sparse annotations to develop a segmentation model for the abdomen and pelvis, significantly reducing manual annotation needs. The developed model demonstrates outstanding performance with Dice scores for skeletal muscle (SM): 0.97+/-0.01, inter-/intra-muscular adipose tissue (IMAT): 0.83 +/- 0.07, visceral adipose tissue (VAT): 0.94 +/-0.04, and subcutaneous adipose tissue (SAT): 0.98 +/-0.02. A reader study supported these findings, indicating that most cases required negligible to no correction for accurate segmentation for SM, VAT and SAT. The variability in reader evaluations for IMAT underscores the challenge of achieving consensus on its quantification and signals a gap in our understanding of the precision required for accurately assessing this tissue through CT imaging. Moreover, the findings from this study offer advancements in annotation efficiency and present a robust tool for body composition analysis, with potential applications in enhancing diagnostic and prognostic assessments in clinical settings.",
        "authors": "Lena Philipp, Maarten de Rooij, John Hermans, Matthieu Rutten, Horst Karl Hahn, Bram van Ginneken, Alessa Hering",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.04"
    },
    {
        "number": 127,
        "UID": "F-127",
        "forum": "https://openreview.net/forum?id=yLySzM5yxs",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "SepVAE: a contrastive VAE to separate pathological patterns from healthy ones",
        "abstract": "Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders (VAEs) that aims at separating the common factors of variation between a \\textit{background} dataset (BG) (\\textit{i.e.,} healthy subjects) and a \\textit{target} dataset (TG) (\\textit{i.e.,} patients) from the ones that only exist in the target dataset. To do so, these methods separate the latent space into a set of \\textbf{salient} features (\\textit{i.e.,} proper to the target dataset) and a set of \\textbf{common} features (\\textit{i.e.,} exist in both datasets). Currently, all CA-VAEs models fail to prevent sharing of information between the latent spaces and to capture all salient factors of variation. To this end, we introduce two crucial regularization losses: a disentangling term between common and salient representations and a classification term between background and target samples in the salient space. We show a better performance than previous CA-VAEs methods on three medical applications and a natural images dataset (CelebA).",
        "authors": "Robin Louiset, Edouard Duchesnay, Grigis Antoine, Benoit Dufumier, Pietro Gori",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.13"
    },
    {
        "number": 142,
        "UID": "F-142",
        "forum": "https://openreview.net/forum?id=JExWRXgZIu",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "MFIF-Net: A Multi-Focal Image Fusion Network for Implantation Outcome Prediction of Blastocyst",
        "abstract": "Accurately predicting implantation outcomes based on blastocyst developmental potential is valuable in in-vitro fertilization (IVF). Clinically, embryologists analyze multiple focal-plane images (FP-images) to comprehensively assess embryo grades, which is extremely cumbersome and easily prone to inconsistency. Developing automatic computer-aided methods for analyzing embryo images is highly desirable. However, effectively fusing multiple FP-images for prediction remains a largely under-explored issue. To this end, we propose a novel Multiple Focal-plane Image Fusion Network, called MFIF-Net, to predict implantation outcomes of blastocyst. Specifically, our MFIF-Net consists of two sub-networks: a Core Image Generation Network (CI-Gen) and a Key Feature Fusion Network (KFFNet). In CI-Gen, we fuse multiple FP-images to generate a core image by pixel-wise weighting since different FP-images can have different focus positions. To further capture key features in each FP-image, we propose KFFNet to extract key information from the FP-images again and fuse them with the core image. In KFFNet, a Fusion Module is designed to capture key information of each FP-image, for which Squeeze Multi-Headed Attention is developed to exchange features and mitigate computationally intensive issues in attention. Comprehensive experiments validate the superiority and the rationality of our MFIF-Net approach over state-of-the-art methods in various metrics. Ablation studies also confirm the positive impact of each component in our MFIF-Net.",
        "authors": "Yi Cheng, Tingting Chen, Yaojun Hu, Xiangqian Meng, Zuozhu Liu, Danny Chen, Jian Wu, Haochao Ying",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.14"
    },
    {
        "number": 145,
        "UID": "F-145",
        "forum": "https://openreview.net/forum?id=mXAzv35dnm",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "A recurrent network for segmenting the thrombus on brain MRI in patients with hyper-acute ischemic stroke",
        "abstract": "In the stroke workflow, timely decision-making is crucial. Identifying, localizing, and measuring occlusive arterial thrombi during initial imaging is a critical step that triggers the choice of therapeutic treatment for optimizing vascular re-canalization. We present a recurrent model that segments the thrombus in patients suffering from a hyper-acute stroke. A cross-attention module is defined to merge the diffusion and susceptibility-weighted modalities available in Magnetic Resonance Imaging (MRI), which are fed to a modified version of the Convolutional Long-Short-Term Memory (CLSTM) model. It detects almost all the thrombi with a Dice higher than 0.6. The lesion segmentation prediction reduces the false positives to almost zero and the performance is comparable between distal and proximal occlusions.",
        "authors": "Sofia Vargas Ibarra, Vincent Martin VIGNERON, Sonia Garcia Salicetti, Hichem Maaref, Jonathan Kobold, Nicolas Chausson, Yann Lhermitte, Didier Smadja",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.15"
    },
    {
        "number": 146,
        "UID": "F-146",
        "forum": "https://openreview.net/forum?id=2yrhjc2iZl",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Active Learning with the nnUNet and Sample Selection with Uncertainty-Aware Submodular Mutual Information Measure",
        "abstract": "Annotating medical images for segmentation tasks is a time-consuming process that requires\nexpert knowledge. Active learning can reduce this annotation cost and achieve optimal\nmodel performance by selecting only the most informative samples for annotation. However, the e\u001bectiveness of active learning sample selection strategies depends on the model\narchitecture and training procedure used. The nnUNet has achieved impressive results in\nvarious automated medical image segmentation tasks due to its self-configuring pipeline\nfor automated model design and training. This raises the question of whether the nnUNet\nis applicable in an active learning setting to avoid cumbersome manual configuration of\nthe training process and improve accessibility for non-experts in deep learning-based segmentation. This paper compares various sample selection strategies in an active learning\nsetting in which the self-configuring nnUNet is used as the segmentation model. Additionally, we propose a new sample selection strategy for UNet-like architectures: USIM - Uncertainty-Aware Submodular Mutual Information Measure. The method combines\nuncertainty and submodular mutual information to select batches of uncertain, diverse,\nand representative samples. We evaluate the performance gain and labeling costs on three\nmedical image segmentation tasks with different segmentation challenges. Our findings\ndemonstrate that utilizing nnUNet as the segmentation model in an active learning setting is feasible, and most sampling strategies outperform random sampling. Furthermore,\nwe demonstrate that our proposed method yields a significant improvement compared to\nexisting baseline methods.",
        "authors": "Bernhard F\u00f6llmer, Kenrick Schulze, Christian Wald, Sebastian Stober, Wojciech Samek, Marc Dewey",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.01"
    },
    {
        "number": 152,
        "UID": "F-152",
        "forum": "https://openreview.net/forum?id=263qZjQ2xC",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Nuclei Segmentation in Histopathological Images with  Enhanced U-Net3+",
        "abstract": "In the rapidly evolving field of nuclei segmentation, there is an increasing trend towards developing a universal segmentation model capable of delivering top-tier results across diverse datasets. While achieving this is the ultimate goal, we argue that such a model should also outperform dataset-specific specialized models. To this end, we propose a task-specific feature sensitive U-Net model, that sets a baseline standard in segmentation of nuclei in histopathological images. We meticulously select and optimize the underlying U-Net3+ model, using adaptive feature selection to capture both short- and long-range dependencies. Max blur pooling is included to achieve scale and position invariance, while DropBlock is utilized to mitigate overfitting by selectively obscuring feature map regions. Additionally, a Guided Filter Block is employed to delineate fine-grained details in nuclei structures. Furthermore, we apply various data augmentation techniques, along with stain normalization, to reduce inconsistencies and thus resulting in significantly outperforming the state-of-the-art performance and paving the way for precise nuclear segmentation essential for cancer diagnosis and possible treatment strategies.",
        "authors": "Bishal Ranjan Swain, Kyung Joo Cheoi, Jaepil Ko",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.05"
    },
    {
        "number": 169,
        "UID": "F-169",
        "forum": "https://openreview.net/forum?id=cRmz96uGD6",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Multimodal Image Registration Guided by Few Segmentations from One Modality",
        "abstract": "Registration of multimodal images is challenging, especially when dealing with different anatomical structures and samples without segmentations. The main difficulty arises from the use of registration loss functions that are inadequate in the absence of corresponding regions. In this work, we present the first registration and segmentation approach tailored to this challenge. In particular, we assume the practically highly relevant scenario that only a limited number of segmentations are available for one modality and none for the other. First, we augment our few segmented samples using unsupervised deep registration within one modality, thereby providing many anatomically plausible samples to train a segmentation network. The resulting segmentation network then allows us to train a segmentation network on the target modality without available segmentations by using an unsupervised domain adaptation architecture. Finally, we train a deep registration network to register multimodal image pairs purely based on predictions of their segmentation networks. Our work demonstrates that using a small number of segmentations from one modality enables training a segmentation network on a target modality without the need for additional manual segmentations on that modality. Additionally, we show that registration based on these segmentations provides smooth and accurate deformation fields on anatomically different image pairs, unlike previous methods. We evaluate our approach on 2D medical image segmentation and registration between knee DXA and X-ray images. Our experiments show that our approach outperforms existing methods. Code is available at https://github.com/uncbiag/SegGuidedMMReg.",
        "authors": "Basar Demir, Marc Niethammer",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.16"
    },
    {
        "number": 174,
        "UID": "F-174",
        "forum": "https://openreview.net/forum?id=5TWfxGVFWc",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Skin Malignancy Classification Using Patients\u2019 Skin Images and Meta-data: Multimodal Fusion for Improving Fairness",
        "abstract": "Skin cancer image classification across skin tones is a challenging problem due to the fact that skin cancer can present differently on different skin tones. This study evaluates the performance of image only models and fusion models in skin malignancy classification. The fusion models we consider are able to take in additional patient data, such as an indicator of their skin tone, and merge this information with the features provided by the image-only model. Results from the experiment show that fusion models perform substantially better than image-only models. In particular, we find that a form of multiplicative fusion results in the best performing models. This finding suggests that skin tones add predictive value in skin malignancy prediction problems. We further demonstrate that feature fusion methods reduce, but do not entirely eliminate, the disparity in performance of the model on patients with different skin tones.",
        "authors": "KE WANG, Ningyuan Shan, Henry Gouk, Iris Szu-Szu Ho",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.17"
    },
    {
        "number": 212,
        "UID": "F-212",
        "forum": "https://openreview.net/forum?id=2iQI8lVoe9",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Reducing Uncertainty in 3D Medical Image Segmentation under Limited Annotations through Contrastive Learning",
        "abstract": "Despite recent successes in semi-supervised learning for natural image segmentation, \napplying these methods to medical images presents challenges in obtaining discriminative representations from limited annotations. While contrastive learning frameworks excel in similarity measures for classification, their transferability to precise pixel-level segmentation in medical images is hindered, particularly when confronted with inherent prediction uncertainty.\nTo overcome this issue, our approach incorporates two subnetworks to rectify erroneous predictions. The first network identifies uncertain predictions, generating an uncertainty attention map. The second network employs an uncertainty-aware descriptor to refine the representation of uncertain regions, enhancing the accuracy of predictions. Additionally, to adaptively recalibrate the representation of uncertain candidates, we define class prototypes based on reliable predictions. We then aim to minimize the discrepancy between class prototypes and uncertain predictions through a deep contrastive learning strategy.\nOur experimental results on organ segmentation from clinical MRI and CT scans demonstrate the effectiveness of our approach compared to state-of-the-art methods.",
        "authors": "Sanaz Jarimijafarbigloo, Reza Azad, Amirhossein Kazerouni, Dorit Merhof",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.06"
    },
    {
        "number": 218,
        "UID": "F-218",
        "forum": "https://openreview.net/forum?id=W9AInb1Dju",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Uncertainty-aware retinal layer segmentation in OCT through probabilistic signed distance functions",
        "abstract": "In this paper, we present a new approach for uncertainty-aware retinal layer segmentation in Optical Coherence Tomography (OCT) scans using probabilistic signed distance functions (SDF). Traditional pixel-wise and regression-based methods primarily encounter difficulties in precise segmentation and lack of geometrical grounding respectively. To address these shortcomings, our methodology refines the segmentation by predicting a signed distance function (SDF) that effectively parameterizes the retinal layer shape via level set. We further enhance the framework by integrating probabilistic modeling, applying Gaussian distributions to encapsulate the uncertainty in the shape parameterization. This ensures a robust representation of the retinal layer morphology even in the presence of ambiguous input, imaging noise, and unreliable segmentations. Both quantitative and qualitative evaluations demonstrate superior performance when compared to other methods. Additionally, we conducted experiments on artificially distorted datasets with various noise types\u2014shadowing, blinking, speckle, and motion\u2014common in OCT scans to showcase the effectiveness of our uncertainty estimation. Our findings demonstrate the possibility of obtaining reliable segmentation of retinal layers, as well as an initial step towards the characterization of layer integrity, a key biomarker for disease progression. Our code is available at \\url{https://github.com/niazoys/RLS_PSDF}.",
        "authors": "Mohammad Mohaiminul Islam, Coen de Vente, Bart Liefers, Caroline Klaver, Erik J Bekkers, Clara I. S\u00e1nchez",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.07"
    },
    {
        "number": 227,
        "UID": "F-227",
        "forum": "https://openreview.net/forum?id=2wrQLJtygh",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Registration Quality Evaluation Metric with Self-Supervised Siamese Networks",
        "abstract": "Registration is one of the most preliminary steps in many medical imaging downstream tasks. The registration quality determines the quality of the downstream task. Traditionally, registration quality evaluation is performed with pixel-wise metrics like Mean Squared Error (MSE) and Structural Similarity Index (SSIM). These pixel-wise measures are sometimes susceptible to local minima, providing sub-optimal and inconsistent quality evaluation. Moreover, it might be essential to incorporate semantic features crucial for human visual perception of the registration quality. Towards this end, we propose a data-driven approach to learn the semantic similarity between the registered and target images to ensure a perceptual and consistent evaluation of the registration quality. In this work, we train a Siamese network to classify registered and synthetically misaligned pairs of images. We leverage the latent Siamese encodings to formulate a semantic registration evaluation metric, SiamRegQC. We analyze SiamRegQC from different perspectives: robustness to local minima or smoothness of evaluation metric, sensitivity to smaller misalignment errors, consistency with visual inspection, and statistically significant evaluation of registration algorithms with a p-value $<$ 0.05. We demonstrate the effectiveness of SiamRegQC on two downstream tasks; (i) Rigid registration of 2D histological serial sections, where evaluating sub-pixel misalignment errors is critical for accurate 3D volume reconstruction. SiamRegQC provides a more realistic quality evaluation sensitive to smaller errors and consistent with visual inspection illustrated with more perceptual semantic feature maps rather than pixel-wise MSE maps. (ii) Unsupervised multimodal non-rigid registration, where the registration framework trained with SiamRegQC as a loss function exhibits a maximum average SSIM value of 0.825 over previously proposed deep similarity metrics.",
        "authors": "Tanvi Kulkarni, Sriprabha Ramanarayanan, Keerthi Ram, Mohanasankar Sivaprakasam",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.18"
    },
    {
        "number": 23,
        "UID": "F-23",
        "forum": "https://openreview.net/forum?id=JtoixYZn74",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Shape of my heart: Cardiac models through learned signed distance functions",
        "abstract": "The efficient construction of anatomical models is one of the major challenges of patientspecific in-silico models of the human heart. Current methods frequently rely on linear statistical models, allowing no advanced topological changes, or requiring medical image segmentation followed by a meshing pipeline, which strongly depends on image resolution, quality, and modality. These approaches are therefore limited in their transferability to other imaging domains. In this work, the cardiac shape is reconstructed by means of threedimensional deep signed distance functions with Lipschitz regularity. For this purpose, the shapes of cardiac MRI reconstructions are learned to model the spatial relation of multiple chambers. We demonstrate that this approach is also capable of reconstructing anatomical models from partial data, such as point clouds from a single ventricle, or modalities different from the trained MRI, such as the electroanatomical mapping (EAM).",
        "authors": "Jan Verh\u00fclsdonk, Thomas Grandits, Francisco Sahli Costabal, Thomas Pinetz, Rolf Krause, Angelo Auricchio, Gundolf Haase, Simone Pezzuto, Alexander Effland",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.11"
    },
    {
        "number": 249,
        "UID": "F-249",
        "forum": "https://openreview.net/forum?id=WDZg4P97gr",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "ADAPT: Multimodal Learning for Detecting Physiological Changes under Missing Modalities",
        "abstract": "Multimodality has recently gained attention in the medical domain, where imaging or video modalities may be integrated with biomedical signals or health records. Yet, two challenges remain: balancing the contributions of modalities, especially in cases with a limited amount of data available, and tackling missing modalities. To address both issues, in this paper, we introduce the AnchoreD multimodAl Physiological Transformer (ADAPT), a multimodal, scalable framework with two key components: (i) aligning all modalities in the space of the strongest, richest modality (called anchor) to learn a joint embedding space, \nand (ii) a Masked Multimodal Transformer, leveraging both inter- and intra-modality correlations while handling missing modalities. We focus on detecting physiological changes in two real-life scenarios: stress in individuals induced by specific triggers and fighter pilots' loss of consciousness induced by g-forces. We validate the generalizability of ADAPT through extensive experiments on two datasets for these tasks, where we set the new state of the art while demonstrating its robustness across various modality scenarios and its high potential for real-life applications. Our code is available at https://github.com/jumdc/ADAPT.git.",
        "authors": "Julie Mordacq, Leo Milecki, Maria Vakalopoulou, Steve Oudot, Vicky Kalogeiton",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.19"
    },
    {
        "number": 282,
        "UID": "F-282",
        "forum": "https://openreview.net/forum?id=uJc5fugAwG",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Distance-Aware Non-IID Federated Learning for Generalization and Personalization in Medical Imaging Segmentation",
        "abstract": "Federated learning (FL) in healthcare suffers from non-identically distributed (non-IID) data, impacting model convergence and performance. While existing solutions for the non-IID problem often do not quantify the degree of non-IID nature between clients in the federation, assessing it can improve training experiences and outcomes, particularly in real-world scenarios with unfamiliar datasets. The paper presents a practical non-IID assessment methodology for a medical segmentation problem, highlighting its significance in medical FL. We propose a simple yet effective solution that utilizes distance measurements in the embedding space of medical images and statistical measurements calculated over their metadata. Our method, designed for medical imaging and integrated into federated averaging, improves model generalization by downgrading the contribution from the most distant client, treating it as an outlier. Additionally, it enhances model personalization by introducing distance-based clustering of clients. To the best of our knowledge, this method is the first to use distance-based techniques for providing a practical solution to the non-IID problem within the medical imaging FL domain. Furthermore, we validate our approach on three public FL imaging radiology datasets (FeTS, Prostate, and Fed-KITS2019) to demonstrate its effectiveness across various radiology imaging scenarios.",
        "authors": "Julia Alekseenko, Alexandros Karargyris, Nicolas Padoy",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.08"
    },
    {
        "number": 320,
        "UID": "F-320",
        "forum": "https://openreview.net/forum?id=xHNTAKhY2W",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Efficient Anatomy Segmentation in Laparoscopic Surgery using Multi-Teacher Knowledge Distillation",
        "abstract": "Automatic segmentation of anatomical structures in laparoscopic images or videos is an important prerequisite for visual assistance tools which are designed to increase efficiency and safety during an intervention.\nIn order to be used in a realistic clinical scenario, both high accuracy and real-time capability are required.\nCurrent deep learning networks for anatomy segmentation show high accuracy, but are not suitable for real-time clinical application due to their large size.\nAs smaller, real-time capable deep learning networks show lower segmentation performance, we propose a multi-teacher knowledge distillation approach applicable to partially labeled datasets.\nWe leverage the knowledge of multiple anatomy-specific, high-accuracy teacher networks to improve the segmentation performance of a single and efficient student network capable of segmenting multiple anatomies simultaneously.\nTo do so, we minimize the Kullback-Leibler divergence between the normalized anatomy-specific teacher logits and the respective normalized logits of the student.\nWe conduct experiments on the Dresden Surgical Anatomy Dataset, which provides multiple subsets of binary segmented anatomical structures.\nResults show that our approach can increase the overall Dice score for different real-time capable network architectures for anatomy segmentation.",
        "authors": "Lennart Maack, Finn Behrendt, Debayan Bhattacharya, Sarah Latus, Alexander Schlaefer",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.09"
    },
    {
        "number": 34,
        "UID": "F-34",
        "forum": "https://openreview.net/forum?id=sz9baxSuxF",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "PAAN: Pyramid Attention Augmented Network for polyp segmentation",
        "abstract": "Polyp segmentation is a task of segmenting polyp lesion regions from normal tissues in medical images, which is crucial for medical diagnosis and treatment planning. However, existing methods still suffer from low accuracy in polyp boundary delineation and insufficient suppression of irrelevant background due to the blur boundaries and textures of polyps. To overcome these limitations, in this paper a Pyramid Attention Augmented Network (PAAN) is proposed, in which a pyramid feature diversion structure with spatial attention mechanism is developed so that good feature representation with low information loss can be achieved by conducting channel attention-based feature diversion and inter-layer fusion, while reducing computational complexity. Also, our framework includes an Enhanced Spatial Attention module (ESA), which can improve the quality of initial polyp segmentation predictions through spatial self-attention mechanism and multi-scale feature fusion. Our approach is evaluated on five challenging polyp datasets\u2014 Kvasir, CVC-ClinicDB, CVC-300, ETIS, and CVC-colonDB and achieves excellent results. In particular, we achieve 94.2% Dice and 89.7% IoU on Kvasir, outperforming other state-of-the-art methods.",
        "authors": "Sida Yi, Yuesheng Zhu, Guibo Luo",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.02"
    },
    {
        "number": 44,
        "UID": "F-44",
        "forum": "https://openreview.net/forum?id=inACgoTK0O",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models",
        "abstract": "We introduce MultiMedEval, an open-source toolkit for fair and reproducible evaluation of large, medical vision-language models (VLM). MultiMedEval comprehensively assesses the models\u2019 performance on a broad array of six multi-modal tasks, conducted over 23 datasets, and spanning over 11 medical domains. The chosen tasks and performance metrics are based on their widespread adoption in the community and their diversity, ensuring a thorough evaluation of the model\u2019s overall generalizability. We open-source a Python toolkit (https://anonymous.4open.science/r/MultiMedEval-C780) with a simple interface and setup process, enabling the evaluation of any VLM in just a few lines of code. Our goal is to simplify the intricate landscape of VLM evaluation, thus promoting fair and uniform benchmarking of future VLMs.",
        "authors": "Corentin Royer, bjoern menze, Anjany Sekuboyina",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.12"
    },
    {
        "number": 5,
        "UID": "F-5",
        "forum": "https://openreview.net/forum?id=NrlGzhFgKj",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "UltraMAE: Multi-modal Masked Autoencoder for Ultrasound Pre-training",
        "abstract": "Pre-training on a large dataset such as ImageNet followed by supervised fine-tuning has brought success in various deep learning-based tasks. However, the modalities of natural images and ultrasound images have considerable differences, making pre-training on natural images ineffective for ultrasound-related tasks. In this paper, we introduce a unified masking-based model for both ultrasound images and videos that learns better visual representation than the network with single-modality representations. This is the first large-scale generalized ultrasound pre-training network that simultaneously utilizes 100,000+ videos and images of different parts of the human anatomy such as the liver, bones, heart, thyroids, nerves, etc, making the network an effective benchmark pretrained model for any ultrasound-specific downstream tasks. We propose a novel method for ultrasound image analysis that utilizes an ultrasound-specific confidence map to guide low-level representation learning through masked feature acquisition. Our pre-trained network has demonstrated remarkable efficacy and versatility in tackling both classification and segmentation tasks across a range of ultrasound pathologies, highlighting its potential for widespread adoption and impact in the ultrasound field. In addition, we show that our pre-training model can be leveraged to learn efficiently with a small number of labeled ultrasound images.",
        "authors": "Aimon Rahman, Vishal M. Patel",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.10"
    },
    {
        "number": 71,
        "UID": "F-71",
        "forum": "https://openreview.net/forum?id=7gFODPjwOe",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Video Polyp Segmentation using Implicit Networks",
        "abstract": "Polyp segmentation in endoscopic videos is an essential task in medical image and video analysis, requiring pixel-level accuracy to accurately identify and localize polyps within the video sequences. Addressing this task unveils the intricate interplay of dynamic changes in the video and the complexities involved in tracking polyps across frames. Our research presents an innovative approach to effectively meet these challenges that integrates, at test time, a pre-trained image (2D) model with a new form of implicit representation. By leveraging the temporal understanding provided by implicit networks and enhancing it with optical flow-based temporal losses, we significantly enhance the precision and consistency of polyp segmentation across sequential frames. Our proposed framework demonstrates excellent performance across various medical benchmarks and datasets, setting a new standard in video polyp segmentation with high spatial and temporal consistency. Our code is publicly available at https://github.com/AviadDahan/VPS-implicit.",
        "authors": "Aviad Dahan, Tal Shaharabany, Raja Giryes, Lior Wolf",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - F.03"
    },
    {
        "number": 241,
        "UID": "F-241",
        "forum": "https://openreview.net/forum?id=hV2WtoJKGp",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Predicting Atrial Fibrillation Treatment Outcome with Siamese Multi-modal Fusion and Cardiac Digital Twins",
        "abstract": "Atrial fibrillation, the most common heart rhythm disorder, presents challenges in treatment due to difficulty pinpointing the patient-specific regions of abnormal electrical activity. \nWhile biophysical simulations of cardiac electrophysiology create a digital twin of atrial electrical activity based on CT or MRI scans, testing various treatment strategies on them is time-consuming and impractical on clinical timescales.\nOur proposed pipeline, incorporating Siamese architecture, fuses latent representations of multi-modal features extracted from atrial digital twin before any therapy and predicts the outcomes of several treatment strategies.\nA large in-silico dataset of 1000 virtual patients, generated from clinical data, was utilized to provide the biophysical simulations before (used for feature extraction) and after (used for calculating ground truth labels depending on whether atrial fibrillation terminates or not) various treatment strategies.  \nBy accurately predicting freedom from atrial fibrillation, our pipeline paves the way for personalized atrial fibrillation therapy with a fast and precise selection of optimal treatments.",
        "authors": "Alexander M. Zolotarev, Abbas Khan Rayabat Khan, Gregory Slabaugh, Caroline Roney",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.2 - O.02"
    },
    {
        "number": 241,
        "UID": "O-241",
        "forum": "https://openreview.net/forum?id=hV2WtoJKGp",
        "Track": "Full Paper",
        "Session": "Oral 1.2 - Representation Learning and Multimodal Methods",
        "Final Decision": "Oral",
        "title": "Predicting Atrial Fibrillation Treatment Outcome with Siamese Multi-modal Fusion and Cardiac Digital Twins",
        "abstract": "Atrial fibrillation, the most common heart rhythm disorder, presents challenges in treatment due to difficulty pinpointing the patient-specific regions of abnormal electrical activity. \nWhile biophysical simulations of cardiac electrophysiology create a digital twin of atrial electrical activity based on CT or MRI scans, testing various treatment strategies on them is time-consuming and impractical on clinical timescales.\nOur proposed pipeline, incorporating Siamese architecture, fuses latent representations of multi-modal features extracted from atrial digital twin before any therapy and predicts the outcomes of several treatment strategies.\nA large in-silico dataset of 1000 virtual patients, generated from clinical data, was utilized to provide the biophysical simulations before (used for feature extraction) and after (used for calculating ground truth labels depending on whether atrial fibrillation terminates or not) various treatment strategies.  \nBy accurately predicting freedom from atrial fibrillation, our pipeline paves the way for personalized atrial fibrillation therapy with a fast and precise selection of optimal treatments.",
        "authors": "Alexander M. Zolotarev, Abbas Khan Rayabat Khan, Gregory Slabaugh, Caroline Roney",
        "Time": "Day 1 \u2014 14:30-15:15",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.2 - O.02",
        "Chairs": "Veronika Cheplygina & Francesco Ciompi"
    },
    {
        "number": 272,
        "UID": "F-272",
        "forum": "https://openreview.net/forum?id=z0r388Sbv3",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Feasibility and benefits of joint learning from MRI databases with different brain diseases and modalities for segmentation",
        "abstract": "Models for segmentation of brain lesions in multi-modal MRI are commonly trained for a specific pathology using a single database with a predefined set of MRI modalities, determined by a protocol for the specific disease.\nThis work explores the following open questions: Is it feasible to train a model using multiple databases that contain varying sets of MRI modalities and annotations for different brain pathologies? Will this joint learning benefit performance on the sets of modalities and pathologies available during training? Will it enable analysis of new databases with different sets of modalities and pathologies? We develop and compare different methods and show that promising results can be achieved with appropriate, simple and practical alterations to the model and training framework. We experiment with 7 databases containing 5 types of brain pathologies and different sets of MRI modalities. Results demonstrate, for the first time, that joint training on multi-modal MRI databases with different brain pathologies and sets of modalities is feasible and offers practical benefits. It enables a single model to segment pathologies encountered during training in diverse sets of modalities, while facilitating segmentation of new types of pathologies such as via follow-up fine-tuning. The insights this study provides into the potential and limitations of this paradigm should prove useful for guiding future advances in the direction. \nCode and pretrained models: https://github.com/WenTXuL/MultiUnet",
        "authors": "Wentian Xu, Matthew Moffat, Thalia Seale, Ziyun Liang, Felix Wagner, Daniel Whitehouse, David Menon, Virginia Newcombe, Natalie Voets, Abhirup Banerjee, Konstantinos Kamnitsas",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.2 - O.03"
    },
    {
        "number": 272,
        "UID": "O-272",
        "forum": "https://openreview.net/forum?id=z0r388Sbv3",
        "Track": "Full Paper",
        "Session": "Oral 1.2 - Representation Learning and Multimodal Methods",
        "Final Decision": "Oral",
        "title": "Feasibility and benefits of joint learning from MRI databases with different brain diseases and modalities for segmentation",
        "abstract": "Models for segmentation of brain lesions in multi-modal MRI are commonly trained for a specific pathology using a single database with a predefined set of MRI modalities, determined by a protocol for the specific disease.\nThis work explores the following open questions: Is it feasible to train a model using multiple databases that contain varying sets of MRI modalities and annotations for different brain pathologies? Will this joint learning benefit performance on the sets of modalities and pathologies available during training? Will it enable analysis of new databases with different sets of modalities and pathologies? We develop and compare different methods and show that promising results can be achieved with appropriate, simple and practical alterations to the model and training framework. We experiment with 7 databases containing 5 types of brain pathologies and different sets of MRI modalities. Results demonstrate, for the first time, that joint training on multi-modal MRI databases with different brain pathologies and sets of modalities is feasible and offers practical benefits. It enables a single model to segment pathologies encountered during training in diverse sets of modalities, while facilitating segmentation of new types of pathologies such as via follow-up fine-tuning. The insights this study provides into the potential and limitations of this paradigm should prove useful for guiding future advances in the direction. \nCode and pretrained models: https://github.com/WenTXuL/MultiUnet",
        "authors": "Wentian Xu, Matthew Moffat, Thalia Seale, Ziyun Liang, Felix Wagner, Daniel Whitehouse, David Menon, Virginia Newcombe, Natalie Voets, Abhirup Banerjee, Konstantinos Kamnitsas",
        "Time": "Day 1 \u2014 14:30-15:15",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.2 - O.03",
        "Chairs": "Veronika Cheplygina & Francesco Ciompi"
    },
    {
        "number": 6,
        "UID": "F-6",
        "forum": "https://openreview.net/forum?id=V5XDYSRcQP",
        "Track": "Full Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "SINR: Spline-enhanced implicit neural representation for multi-modal registration",
        "abstract": "Deformable image registration has undergone a transformative shift with the advent of deep learning. While convolutional neural networks (CNNs) allow for accelerated registration, they exhibit reduced accuracy compared to iterative pairwise optimization methods and require extensive training cohorts. Based on the advances in representing signals with neural networks, implicit neural representations (INRs) have emerged in the registration community to model dense displacement fields continuously. Using a pairwise registration setup, INRs mitigate the bias learned over a cohort of patients while leveraging advanced methodology and gradient-based optimization. However, the coordinate sampling scheme makes dense transformation parametrization with an INR prone to generating physiologically implausible configurations resulting in spatial folding. In this paper, we introduce SINR - a method to parameterize the continuous deformable transformation represented by an INR using Free Form Deformations (FFD). SINR allows for multi-modal deformable registration while mitigating folding issues found in current INR-based registration methods. SINR outperforms existing state-of-the-art methods on both 3D mono- and multi-modal brain registration on the CamCAN dataset, demonstrating its capabilities for pairwise mono- and multi-modal image registration.",
        "authors": "Vasiliki Sideri-Lampretsa, Julian McGinnis, Huaqi Qiu, Magdalini Paschali, Walter Simson, Daniel Rueckert",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.2 - O.01"
    },
    {
        "number": 6,
        "UID": "O-6",
        "forum": "https://openreview.net/forum?id=V5XDYSRcQP",
        "Track": "Full Paper",
        "Session": "Oral 1.2 - Representation Learning and Multimodal Methods",
        "Final Decision": "Oral",
        "title": "SINR: Spline-enhanced implicit neural representation for multi-modal registration",
        "abstract": "Deformable image registration has undergone a transformative shift with the advent of deep learning. While convolutional neural networks (CNNs) allow for accelerated registration, they exhibit reduced accuracy compared to iterative pairwise optimization methods and require extensive training cohorts. Based on the advances in representing signals with neural networks, implicit neural representations (INRs) have emerged in the registration community to model dense displacement fields continuously. Using a pairwise registration setup, INRs mitigate the bias learned over a cohort of patients while leveraging advanced methodology and gradient-based optimization. However, the coordinate sampling scheme makes dense transformation parametrization with an INR prone to generating physiologically implausible configurations resulting in spatial folding. In this paper, we introduce SINR - a method to parameterize the continuous deformable transformation represented by an INR using Free Form Deformations (FFD). SINR allows for multi-modal deformable registration while mitigating folding issues found in current INR-based registration methods. SINR outperforms existing state-of-the-art methods on both 3D mono- and multi-modal brain registration on the CamCAN dataset, demonstrating its capabilities for pairwise mono- and multi-modal image registration.",
        "authors": "Vasiliki Sideri-Lampretsa, Julian McGinnis, Huaqi Qiu, Magdalini Paschali, Walter Simson, Daniel Rueckert",
        "Time": "Day 1 \u2014 14:30-15:15",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.2 - O.01",
        "Chairs": "Veronika Cheplygina & Francesco Ciompi"
    },
    {
        "number": 107,
        "UID": "S-107",
        "forum": "https://openreview.net/forum?id=kIRMpUufhm",
        "Track": "Short Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "BiF\u00b3-Net: A Full BiFormer Full-scale Fusion Network for Accurate Gastrointestinal Images Segmentation",
        "abstract": "UNet-like segmentation models have been widely explored for the computer-aided segmentation and diagnosis of gastrointestinal (GI) tract diseases. However, the UNet architecture encounters two primary challenges: limited receptive fields due to conventional convolution operations, and a semantic gap arising from simplistic skip connections. In this paper, we introduce BiF\u00b3-Net, a novel model that integrates BiFormer blocks throughout the encoder and decoder, along with a full-scale BiFormer Fusion Bridge (BFB) module, aimed at addressing the aforementioned limitations. Meanwhile, we propose the Dense Inception Classifier (DIC) module to mitigate the over-segmentation problem in non-organ images. Extensive experiments demonstrate the effectiveness and adaptability of the proposed model.",
        "authors": "Yunze Wang, Silin Chen, xi long, Yi.Tian, ye.huang, Tianyang Wang, Jingxin Liu",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - S.03"
    },
    {
        "number": 138,
        "UID": "S-138",
        "forum": "https://openreview.net/forum?id=dUYMXTHlUf",
        "Track": "Short Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Advancing Pharyngeal Constrictor Muscle Auto-Segmentation: A Comparative Analysis",
        "abstract": "Accurate auto-segmentation of the pharyngeal constrictor muscle (PCM) is crucial for precise treatment planning in head and neck cancer. This study compares three deep-learning segmentation methods: (i) 2D U-Net, (ii) 3D UNet with data fingerprinting (nnUNet), and (iii) our proposed 2D statistical curve fitting method using transfer learning. Using a total of 168 planning CT images for training and validation, results indicate varying effectiveness, with the nnUNet exhibiting the lowest mean surface distance (2.73mm, inter-quartile range = 1.02mm), statistical curve fitting obtained a mean-surface distance of 8.25 mm with IQR of 6.33 mm. Though this was not as accurate as nnUNet, but was significantly superior than a conventional 2D UNet model.",
        "authors": "Anju Kaimal, Konstantinos Zormpas-Petridis, Justine Tyler, Christopher M. Nutting, Matthew Blackledge",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - S.08"
    },
    {
        "number": 151,
        "UID": "S-151",
        "forum": "https://openreview.net/forum?id=n6D25aqdV3",
        "Track": "Short Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Contrast-agnostic Spinal Cord Segmentation: A Comparative Study of ConvNets and Vision Transformers",
        "abstract": "The cross-sectional area (CSA) of the spinal cord (SC) computed from its segmentation is a relevant clinical biomarker for the diagnosis and monitoring of cord compression and atrophy. One key limitation of existing automatic methods is that their SC segmentations depend on the MRI contrast, resulting in different CSA across contrasts. Furthermore, these methods rely on CNNs, leaving a gap in the literature for exploring the performance of modern deep learning (DL) architectures. In this study, we extend our recent work \\cite{Bdard2023TowardsCS} by evaluating the contrast-agnostic SC segmentation capabilities of different classes of DL architectures, namely, ConvNeXt,  vision transformers (ViTs), and hierarchical ViTs. We compared 7 different DL models using the open-source \\textit{Spine Generic} Database of healthy participants ($\\text{n}=243$) consisting of 6 MRI contrasts per participant. Given a fixed dataset size, our results show that CNNs produce robust SC segmentations across contrasts, followed by ConvNeXt, and hierarchical ViTs. This suggests that: (i) inductive biases such as learning hierarchical feature reprensentations via pooling (common in CNNs) are crucial for good performance on SC segmentation, and (ii) hierarchical ViTs that incorporate several CNN-based priors can perform similarly to pure CNN-based models.",
        "authors": "Enamundram Naga Karthik, Sandrine Bedard, Jan Valosek, Sarath Chandar, Julien Cohen-Adad",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - S.07"
    },
    {
        "number": 152,
        "UID": "S-152",
        "forum": "https://openreview.net/forum?id=1GEz81GU3g",
        "Track": "Short Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Automatic classification of prostate MR series type using image content and metadata",
        "abstract": "With the wealth of medical image data, efficient curation is essential. Assigning the sequence type to magnetic resonance images is necessary for scientific studies and artificial intelligence-based analysis. However, incomplete or missing metadata prevents effective automation. We therefore propose a deep-learning method for classification of prostate cancer scanning sequences based on a combination of image data and DICOM metadata. We demonstrate superior results compared to metadata or image data alone, and make our code publicly available at https://github.com/deepakri201/DICOMScanClassification.",
        "authors": "Deepa Krishnaswamy, Balint Kovacs, Stefan Denner, Steve Pieper, David Clunie, Christopher P. Bridge, Tina Kapur, Klaus Maier-Hein, Andrey Fedorov",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - S.11"
    },
    {
        "number": 17,
        "UID": "S-17",
        "forum": "https://openreview.net/forum?id=rSiCuOSOct",
        "Track": "Short Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Enhancing Neurofibroma Segmentation in Whole-Body MRI: Leveraging an Anatomy-Informed Approach",
        "abstract": "This study presents an anatomy-informed segmentation approach for neurofibroma in fat-suppressed T2-weighted whole-body MRI (WB-MRI). By adapting TotalSegmentator for WB-MRI segmentation and employing dedicated Dynamic UNet models across four anatomical zones, we achieved improvements of 20\\% in terms of the Dice coefficient on a test set. The proposed method promises to streamline neurofibroma segmentation, emphasizing future integration into interactive workflows.",
        "authors": "Georgii Kolokolnikov, Marie-Lena Schmalhofer, Inka Ristow, Ren\u00e9 Werner",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - S.01"
    },
    {
        "number": 175,
        "UID": "S-175",
        "forum": "https://openreview.net/forum?id=bn5vgsB5Nj",
        "Track": "Short Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Automatic muscle segmentation on healthy abdominal MRI using nnUNet",
        "abstract": "Understanding the dynamics of the abdominal wall is essential in both physiology and\nsurgery. To study the mechanical functionality of the abdominal wall, segmentation of the\nabdominal muscles could be useful but is a manual, tedious and time-consuming process.\nIn this study, we assessed the capability of Deep Learning to automatically segment the\nabdominal muscles from the axial plane of a unique dynamic abdominal MRI (2D+t)\ndataset. The 2D slices were acquired while the subject performed various exercises. The\nState-of-the-Art segmentation model, nnUNet was trained on 5, 492 images from fifteen\nhealthy subjects and tested it on 1, 801 images from five different subjects. We assessed\nthe segmentation accuracy using DICE similarity coefficient, Hausdorff distance, as well as\nmotion of the abdominal muscles. The ground truth and nnUNet segmentation showed high\nconcordance, with a DICE over 0.87 for all exercises and muscles, and minimal differences in\nabdominal muscles motion. nnUNet effectively automates abdominal muscle segmentation,\noffering efficiency and new clinical applications in abdominal physiology.",
        "authors": "Victoria Joppin, Niamh Belton, Marc Adrien Hostin, Marc-Emmanuel Bellemare, Aonghus Lawlor, Kathleen M Curran, Thierry B\u00e8ge, Catherine Masson, David Bendahan",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - S.02"
    },
    {
        "number": 23,
        "UID": "S-23",
        "forum": "https://openreview.net/forum?id=p1ILDdYJj1",
        "Track": "Short Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Gradual Learning for One-Shot Segmentation of Slice Stacks",
        "abstract": "Diagnostic imaging modalities like magnetic resonance imaging (MRI) or computed tomography (CT) are crucial for medical and industrial inspection. However, labeled datasets are not always available for segmentation of rare cancer types or other defects. Therefore, a new training strategy named gradual learning is proposed for one-shot segmentation, thus requiring only one labeled example slice. A segmentation network trained on this input generates suitable pseudo labels in a local neighborhood, with the quality degrading with distance. These adjacent pseudo labels can be incorporated into the training process repeatedly, to process the unlabeled slices step-by-step. Experiments were conducted on MRI head scans for skull-stripping. A total of 30 models were trained using gradual learning, receiving one scan with one annotated slice each. On a separate test set ($n=30$ scans), the mean intersection over union (mIoU), averaged over all models, increased from 0.885 to 0.935 using gradual learning compared to training without it. When trained with the ground truth (GT) of the same slices instead the models achieved a 0.955 mIoU.",
        "authors": "Johann Christopher Engster, Nele Blum, Thorsten Buzug, Maik Stille",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - S.05"
    },
    {
        "number": 41,
        "UID": "S-41",
        "forum": "https://openreview.net/forum?id=KXmiNZYuBR",
        "Track": "Short Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Brain Artery Segmentation for Structural MRI",
        "abstract": "The visualization of brain arteries in neuroimaging scans is essential for evaluating neurological disorders effectively. In this paper, we propose a deep learning-based method for the segmentation of brain arteries in structural MR images (sMRI), where their delineation poses a significant challenge due to the lack of contrast. Our fully automated strategy leverages two modules: one for generating pseudo labels from angiographic MR images and another for pairing these labels with five distinct sMRI sequences. The process enables us to construct a large dataset of 2626 labelled images for 669 patients used to train our segmentation model. In our experiments, our model achieved an average Dice Similarity Coefficient (DSC) of 0.66 across all sMRI around the central Circle of Willis structure in a 5-fold cross validation. We outline our results for each evaluated sMRI sequence, out of which we identify PD with a DSC of 0.7 as the best alternative to angiographic images.",
        "authors": "Bertram Sabrowsky-Hirsch, Ahmed AlShenoudy, Stefan Thumfart, Michael Giretzlehner, Josef Scharinger",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - S.09"
    },
    {
        "number": 80,
        "UID": "S-80",
        "forum": "https://openreview.net/forum?id=nhoD86FVY8",
        "Track": "Short Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Exploiting Scale Invariance and Rotation Equivariance for Sparse and Dense Artery Orientation Estimation",
        "abstract": "We present SIRE, a modular estimator of local artery orientations that is Scale Invariant and Rotation Equivariant. These symmetry preservations are obtained by operating on *spherical* image patches at multiple scales in parallel, and allow generalisation to arteries of unseen sizes and orientations. We embed SIRE into two different artery centerline tracking algorithms: a sparse, iterative tracker starting at a single seed point and a dense image filter serving as a cost function for connecting two bifurcation points. We show that SIRE can be used to obtain centerlines of arteries of various sizes and tortuosities by including datasets containing abdominal aortic aneurysms, coronary arteries and intracranial arteries.",
        "authors": "Dieuwertje Alblas, Iris Vos, Julian Suk, Christoph Brune, Kak Khee Yeung, Birgitta Velthuis, Hugo Kuijf, Ynte Ruigrok, Jelmer M. Wolterink",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - S.06"
    },
    {
        "number": 82,
        "UID": "S-82",
        "forum": "https://openreview.net/forum?id=wGEDqSex3q",
        "Track": "Short Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Revisiting Anomaly Localization Metrics",
        "abstract": "An assumption-free, disease-agnostic pathology detector and segmentor could often be seen as one of the holy grails of medical image analysis.  \nBuilding on this promise, un-/weakly supervised anomaly localization approaches, which aim to model normal/healthy samples using data and then detect anything deviant from this (i.e., anything abnormal), have gained popularity.\nHowever, being an upcoming area in between image segmentation and out-of-distribution detection, most approaches have adapted their evaluation setup and metrics from either field and thus might have missed peculiarities inherent to the anomaly localization task. \nHere, we revisit the anomaly localization setup, discuss and analyse the properties of the often used metrics, show alternative metrics inspired from instance segmentation and compare the metrics across multiple setting and algorithms. \nOverall, we argue that the choice of the metric is use-case dependent, however, the Soft Instance IoU shows significant promise going forward.",
        "authors": "David Zimmerer, Klaus Maier-Hein",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - S.10"
    },
    {
        "number": 98,
        "UID": "S-98",
        "forum": "https://openreview.net/forum?id=MZUSthHoVO",
        "Track": "Short Paper",
        "Session": "Poster 1.1",
        "Final Decision": "Poster",
        "title": "Unlocking Robust Segmentation Across All Age Groups via Continual Learning",
        "abstract": "Most deep learning models in medical imaging are trained on adult data with unclear per- formance on pediatric images. In this work, we aim to address this challenge in the context of automated anatomy segmentation in whole-body Computed Tomography (CT). We eval- uate the performance of CT organ segmentation algorithms trained on adult data when applied to pediatric CT volumes and identify substantial age-dependent underperformance. We subsequently propose and evaluate strategies, including data augmentation and con- tinual learning approaches, to achieve good segmentation accuracy across all age groups. Our best-performing model, trained using continual learning, achieves high segmentation accuracy on both adult and pediatric data (Dice scores of 0.90 and 0.84 respectively).",
        "authors": "Chih-Ying Liu, Jeya Maria Jose Valanarasu, Camila Gonzalez, Curtis Langlotz, Andrew Y. Ng, Sergios Gatidis",
        "Time": "Day 1 \u2014 11:00-12:30",
        "Poster time": "Day 1 \u2014 11:00-12:30",
        "Poster ID": "Poster 1.1 - S.04"
    },
    {
        "number": 111,
        "UID": "F-111",
        "forum": "https://openreview.net/forum?id=V9H32U1c87",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Predicting DNA Content Abnormalities in Barrett\u2019s Esophagus: A Weakly Supervised Learning Paradigm",
        "abstract": "Barrett\u2019s esophagus (BE) is the sole precursor to esophageal adenocarcinoma (EAC), and is an opportunity for developing biomarkers for cancer risk assessment. DNA content abnormalities, including aneuploidy, have been implicated in the progression to EAC in BE patients, but molecular assays require valuable tissue for its detection. We propose utilizing images from routine histology to detect ploidy status using deep learning.\nEmploying a weakly supervised deep learning approach, multi-instance learning (MIL), we trained a model to predict ploidy using hematoxylin and eosin-stained whole slide images of endoscopic biopsies and flow cytometry results. The study introduces a novel data augmentation method for MIL, sequentially altering features from original and augmented images during training loops. This method improved the average area under curve (AUC) from 0.43, 0.64 and 0.81 for ResNet50, DenseNet121 and REMEDIS foundation model, respectively (training without any augmentation), to 0.61, 0.87 and 0.91 with the proposed augmentation strategy.\nThe top-performing model, using REMEDIS foundation model as the backbone, achieved 0.93 AUC and 0.83 balanced accuracy to predict aneuploidy in the test cohort biopsies (n=279). Across all the patients (n=123), predicted aneuploidy status was correlated with progression to EAC (p=6.55e-06), similar to correlation with ploidy status based on flow cytometry results (p=2.84e-7). Supporting the findings, histologic nuclear features typically associated with dysplasia and DNA content abnormalities such as enlarged, hyperchromatic nuclei and loss of nuclear polarity, were seen in the samples called abnormal compared to the control diploid samples.\nIn conclusion, our deep learning model efficiently predicts aneuploidy, a mechanism that has been shown to underpin BE progression to EAC. This method, preserving precious biopsy tissues, complements routine histology, offering potential for identifying individuals at high risk of progression through molecular-based advancements.",
        "authors": "Caner Ercan, Xiaoxi Pan, Thomas G. Paulson, Matthew D. Stachler, Carlo C. Maley, William M. Grady, Yinyin Yuan",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.04"
    },
    {
        "number": 117,
        "UID": "F-117",
        "forum": "https://openreview.net/forum?id=gHCo43zcDm",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Auto-Generating Weak Labels for Real & Synthetic Data to Improve Label-Scarce Medical Image Segmentation",
        "abstract": "The high cost of creating pixel-by-pixel gold-standard labels, limited expert availability, and presence of diverse tasks make it challenging to generate segmentation labels to train deep learning models for medical imaging tasks. In this work, we present a new approach to overcome the hurdle of costly medical image labeling by leveraging foundation models like Segment Anything Model (SAM) and its medical alternate MedSAM. Our pipeline has the ability to generate *weak labels* for any unlabeled medical image and subsequently use it to augment label-scarce datasets. We perform this by leveraging a model trained on a few gold-standard labels and using it to intelligently prompt MedSAM for weak label generation. This automation eliminates the manual prompting step in MedSAM, creating a streamlined process for generating labels for both real and synthetic images, regardless of quantity. We conduct experiments on label-scarce settings for multiple tasks pertaining to modalities ranging from ultrasound, dermatology, and X-rays to demonstrate the usefulness of our pipeline. The code will be made public after review.",
        "authors": "Tanvi Deshpande, Eva Prakash, Elsie Gyang Ross, Curtis Langlotz, Andrew Y. Ng, Jeya Maria Jose Valanarasu",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.05"
    },
    {
        "number": 130,
        "UID": "F-130",
        "forum": "https://openreview.net/forum?id=H4KbJlAHuq",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Cell-DETR: Efficient cell detection and classification in WSIs with transformers",
        "abstract": "Understanding cell interactions and subpopulation distribution is crucial for pathologists to support their diagnoses. This cell information is traditionally extracted from segmentation methods, which poses significant computational challenges on processing Whole Slide Images (WSIs) due to their giga-size nature. Nonetheless, the clinically relevant tasks are nuclei detection and classification rather than segmentation. In this manuscript, we undertake a comprehensive exploration of the applicability of detection transformers for cell detection and classification (Cell-DETR). Not only do we demonstrate the effectiveness of the method by achieving state-of-the-art performance on well-established benchmarks, but we also develop a pipeline to tackle these tasks on WSIs at scale to enable the development of downstream applications. We show its efficiency and feasibility by reporting a x3.4 faster inference time on a dataset featuring large WSIs. By addressing the challenges associated with large-scale cell detection, our work contributes valuable insights that paves the way for the development of scalable diagnosis pipelines based on cell-level information.",
        "authors": "Oscar Pina, Eduard Dorca, Veronica Vilaplana",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.06"
    },
    {
        "number": 132,
        "UID": "F-132",
        "forum": "https://openreview.net/forum?id=c1AfNZoSyQ",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Lupus Nephritis Subtype Classification with only Slide Level Labels",
        "abstract": "Lupus Nephritis classification has historically relied on labor-intensive and meticulous glomerular-level labeling of renal structures in whole slide images (WSIs). However, this approach presents a formidable challenge due to its tedious and resource-intensive nature, limiting its scalability and practicality in clinical settings. In response to this challenge, our work introduces a novel methodology that utilizes only slide-level labels, eliminating the need for granular glomerular-level labeling. A comprehensive multi-stained lupus nephritis digital histopathology WSI dataset was created from the Indian population, which is the largest of its kind. LupusNet, a deep learning MIL-based model, was developed for the sub- type classification of LN. The results underscore its effectiveness, achieving an AUC score of 91.0%, an F1-score of 77.3%, and an accuracy of 81.1% on our dataset in distinguishing membranous and diffused classes of LN.",
        "authors": "Amit Sharma, Ekansh Chauhan, Megha S Uppin, Liza Rajasekhar, C.V. Jawahar, P K Vinod",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.07"
    },
    {
        "number": 184,
        "UID": "F-184",
        "forum": "https://openreview.net/forum?id=RabL0WcGHo",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Advancing Multiplex Immunofluorescence Imaging Cell Detection using Semi-Supervised Learning with Pseudo-Labeling",
        "abstract": "Accurate cell detection in multiplex immunofluorescence (mIF) is crucial for quantifying and analyzing the spatial distribution of complex cellular patterns within the tumor microenvironment. Despite its importance, cell detection in mIF is challenging, primarily due to difficulties obtaining comprehensive annotations. To address the challenge of limited and unevenly distributed annotations, we introduced a streamlined semi-supervised approach that effectively leveraged partially pathologist-annotated single-cell data in multiplexed images across different cancer types. We assessed three leading object detection models, Faster R-CNN, YOLOv5s, and YOLOv8s, with partially annotated data, selecting YOLOv8s for optimal performance. This model was subsequently used to generate pseudo labels, which enriched our dataset by adding more detected labels than the original partially annotated data, thus increasing its generalization and the comprehensiveness of cell detection. By fine-tuning the detector on the original dataset and the generated pseudo labels, we tested the refined model on five distinct cancer types using fully annotated data by pathologists. Our model achieved an average precision of 90.42%, recall of 85.09%, and an F1 Score of 84.75%, underscoring our semi-supervised model's robustness and effectiveness. This study contributes to analyzing multiplexed images from different cancer types at cellular resolution by introducing sophisticated object detection methodologies and setting a novel approach to effectively navigate the constraints of limited annotated data with semi-supervised learning.",
        "authors": "Yasin Shokrollahi, Karina Pinao Gonzales, Maria Esther Salvatierra, Simon P. Castillo, Tanishq Gautam, Pingjun Chen, B. Leticia Rodriguez, Sara Ranjbar, Patient Mosaic Team, Luisa Solis Soto, Yinyin Yuan, Xiaoxi Pan",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.08"
    },
    {
        "number": 205,
        "UID": "F-205",
        "forum": "https://openreview.net/forum?id=vh01Nd5PCl",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "A Patch-based Student-Teacher Pyramid Matching Approach to Anomaly Detection in 3D Magnetic Resonance Imaging",
        "abstract": "Anomaly detection on 3D magnet resonance images (MRI) is of high medical relevance in the context of detecting lesions associated with different diseases. Yet, reliable anomaly detection in MRI images involves major challenges, specifically taking into account information in 3D, and the need to localize relatively small and subtle abnormalities within the context of whole organ MRIs. In this paper, a top-down approach, which uses student-teacher feature pyramid matching (STFPM) for detecting anomalies at image and voxel level, is applied to 3D brain MRI inputs. The combination of a 3D patch based self-supervised pre-training and axial-coronal-sagittal (ACS) convolutions pushes the performance above that of f-AnoGAN (bottom-up). The evaluation is based on a tumor dataset.",
        "authors": "Johannes Schwarz, Lena Will, J\u00f6rg Wellmer, Axel Mosig",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.09"
    },
    {
        "number": 222,
        "UID": "F-222",
        "forum": "https://openreview.net/forum?id=q9OpRfSjSw",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Unsupervised Deep Learning Method for Bias Correction",
        "abstract": "In this paper, a new method for automatic MR image inhomogeneity correction is proposed. This method, based on deep learning, uses unsupervised learning to estimate the bias corrected images minimizing a cost function based on the entropy of the corrupted image, the derivative of the estimated bias field and corrected image statistics. The proposed method has been compared with the state-of-the-art method N4 providing improved results.",
        "authors": "Maria Perez-Caballero, Sergio Morell-Ortega, Marina Ruiz Perez, Pierrick Coupe, Jose V Manjon",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.10"
    },
    {
        "number": 263,
        "UID": "F-263",
        "forum": "https://openreview.net/forum?id=0qf8B7KTKR",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Weakly supervised deep learning model with size constraint for prostate cancer detection in multiparametric MRI and generalization to unseen domains",
        "abstract": "Fully supervised deep models have shown promising performance for many medical segmentation tasks. Still, the deployment of these tools in clinics is limited by the very time-consuming collection of manually expert-annotated data. Moreover, most of the state-of-the-art models have been trained and validated on moderately homogeneous datasets. It is known that deep learning methods are often greatly degraded by domain or label shifts and are yet to be built in such a way as to be robust to unseen data or label distributions. In the clinical setting, this problematic is particularly relevant as the deployment institutions may have different scanners or acquisition protocols than those from which the data has been collected to train the model. In this work, we propose to address these two challenges on the detection of clinically significant prostate cancer (csPCa) from bi-parametric MRI. We evaluate the method proposed by (Kervadec et al., 2018) , which introduces a size constaint loss to produce fine semantic cancer lesions segmentations from weak circle scribbles annotations. Performance of the model is based on two public (PI-CAI and Prostate158) and one private databases. First, we show that the model achieves on-par performance with strong fully supervised baseline models, both on in-distribution validation data and unseen test images. Second, we observe a performance decrease for both fully supervised and weakly supervised models when tested on unseen data domains. This confirms the crucial need for efficient domain adaptation methods if deep learning models are aimed to be deployed in a clinical environment. Finally, we show that ensemble predictions from multiple trainings increase generalization performance.",
        "authors": "Robin Trombetta, Olivier Rouvi\u00e8re, Carole Lartizien",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.11"
    },
    {
        "number": 275,
        "UID": "F-275",
        "forum": "https://openreview.net/forum?id=CCvRaXpMH7",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Automated ranking of chest x-ray radiological finding severity in a binary label setting",
        "abstract": "Machine learning has demonstrated the ability to match or exceed human performance in detecting a range of abnormalities in chest x-rays. However, current models largely operate within a binary classification paradigm with findings either present or absent using fixed decision thresholds, whereas many clinical findings can be more usefully described on a scale of severity which a skilled radiologist will incorporate into a more nuanced report. This limitation is due, in part, to the difficulty and expense of manually annotating fine-grained labels for training and test images versus the relative ease of automatically extracting binary labels from the associated free text reports using NLP algorithms. In this paper we examine the ability of models trained with only binary training data to give useful abnormality severity information from their raw outputs. We assess performance on a ranking task using manually ranked test sets for each of five findings: cardiomegaly, consolidation, paratracheal hilar changes, pleural effusion and subcutaneous emphysema. We find the raw model output agrees with human-assessed severity ranking with Spearman's rank coefficients between 0.563 - 0.848. Using patient age as an additional radiological finding with full ground truth ranking available, we go on to compare a binary classifier output against a fully supervised RankNet model, quantifying the reduction in training data required in the fully supervised setting for equivalent performance. We show that model performance is improved using a semi-supervised approach supplementing a smaller set of fully supervised images with a larger set of binary labelled images.",
        "authors": "Matthew Macpherson, Keerthini Muthuswamy, Ashik Amlani, Vicky Goh, Giovanni Montana",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.12"
    },
    {
        "number": 314,
        "UID": "F-314",
        "forum": "https://openreview.net/forum?id=4uqpqIoQVA",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Ano-swinMAE: Unsupervised Anomaly Detection in Brain MRI using swin Transformer based Masked Auto Encoder",
        "abstract": "The advanced deep learning-based Autoencoding techniques have enabled the introduction of efficient Unsupervised Anomaly Detection (UAD) approaches. Several autoencoder-based approaches have been used to solve UAD tasks. However, most of these approaches do not have any constraints to ensure the removal of pathological features while restoring the healthy regions in the pseudo-healthy image reconstruction. To remove the occurrence of pathological features, we propose to utilize an Autoencoder which deploys a masking strategy to reconstruct images. Additionally, the masked regions need to be meaningfully inpainted to enforce global and local consistency in the generated images which makes transformer-based masked autoencoder a potential approach. Although the transformer models can incorporate global contextual information, they are often computationally expensive and dependent on a large amount of data for training. Hence we propose to employ a Swin transformer-based Masked Autoencoder (MAE) for anomaly detection (Ano-swinMAE) in brain MRI. Our proposed method Ano-swinMAE is trained on a  healthy cohort by masking a certain percentage of information from the input images. While inferring, a pathological image is given to the model and different segments of the brain MRI slice are sequentially masked and their corresponding generation is accumulated to create a map indicating potential locations of pathologies. We have quantitatively and qualitatively validated the performance increment of our method on the following publicly available datasets: BraTS (Glioma), MSLUB (Multiple Sclerosis) and White Matter Hyperintensities (WMH). We have also empirically evaluated the generalisation capability of the method in a cross modality data setup.",
        "authors": "Kumari Rashmi, Ayantika Das, NagaGayathri Matcha, Keerthi Ram, Mohanasankar Sivaprakasam",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.13"
    },
    {
        "number": 319,
        "UID": "F-319",
        "forum": "https://openreview.net/forum?id=QfYXJUmIit",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Learned morphological features guide cell type assignment of deconvolved spatial transcriptomics",
        "abstract": "Spatial transcriptomics enables to study the relationship between gene expression and tissue organization. Despite many recent advancements, existing sequencing-based methods have a spatial resolution that limits identification of individual cells.  To address this, several cell type deconvolution methods have been proposed to integrate spatial gene expression with single-cell and single-nucleus RNA sequencing, producing per spot cell typing. However, these methods often overlook the contribution of morphology, which means cell identities are randomly assigned to the nuclei within a spot. In this paper, we introduce MHAST, a morphology-guided hierarchical permutation-based framework which efficiently reassigns cell types in spatial transcriptomics. We validate our method on simulated data, synthetic data, and a use case on the broadly used Tangram cell type deconvolution method with Visium data. We show that deconvolution-based cell typing using morphological tissue features from self-supervised deep learning lead to a more accurate annotation of the cells.",
        "authors": "Eduard Chelebian, Christophe Avenel, Julio Leon, Chung-Chau Hon, Carolina Wahlby",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.14"
    },
    {
        "number": 64,
        "UID": "F-64",
        "forum": "https://openreview.net/forum?id=SHVeWY9fJ1",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Dense Self-Supervised Learning for Medical Image Segmentation",
        "abstract": "Deep learning has revolutionized medical image segmentation, but it relies heavily on high-quality annotations. The time, cost and expertise required to label images at the pixel-level for each new task has slowed down widespread adoption of the paradigm. We propose Pix2Rep, a self-supervised learning (SSL) approach for few-shot segmentation, that reduces the manual annotation burden by learning powerful pixel-level representations directly from unlabeled images. Pix2Rep is a novel pixel-level loss and pre-training paradigm for contrastive SSL on whole images. It is applied to generic encoder-decoder deep learning backbones (e.g., U-Net). Whereas most SSL methods enforce invariance of the learned image-level representations under intensity and spatial image augmentations, Pix2Rep enforces equivariance of the pixel-level representations. We demonstrate the framework on a task of cardiac MRI segmentation. Results show improved performance compared to existing semi- and self-supervised approaches; and a 5-fold reduction in the annotation burden for equivalent performance versus a fully supervised U-Net baseline. This includes a 30% (resp. 31%) DICE improvement for one-shot segmentation under linear-probing (resp. fine-tuning). Finally, we also integrate the novel Pix2Rep concept with the Barlow Twins non-contrastive SSL, which leads to even better segmentation performance.",
        "authors": "Maxime Seince, Lo\u00efc Le Folgoc, Luiz Facury De Souza, Elsa Angelini",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.01"
    },
    {
        "number": 98,
        "UID": "F-98",
        "forum": "https://openreview.net/forum?id=YN4aoXelxg",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Laparoflow-SSL: Image Analysis From a Tiny Dataset Through Self-Supervised Transformers Leveraging Unlabeled Surgical Video",
        "abstract": "During minimally invasive surgery, surgeons monitor their actions and the relevant tissue through a camera. This provides an ideal environment for artificial intelligence (AI) assisted surgery. For the development of such AI components, the need for expert annotations remains a key bottleneck. In this paper, we study the application of self-supervised learning (SSL) on surgical data. In a self-supervised setting, a representation backbone is trained on information that is inherently present in the data. There is no need for annotations, leaving the backbone free to train on all recordings, not just labeled ones. We leveraged optical flow for weighting pairs in a view-contrastive self-supervised learning loss. Constructed as an Info Noise-Contrastive Estimation (InfoNCE) loss, it contrasted the pixel representations of two differently, photometrically and geometrically transformed views. The importance of each contrasted pixel pair is determined by computing the difference between the optical flows of the respective pixels. In this way, the optical flow guided the representations of pixels that move together to similar vectors. We tested the usefulness of the representation vectors by training simple networks for semantic segmentation or robotic instrument key point detection. These networks showed competitive performance, even when using over 92% fewer annotated samples than other works. For semantic segmentation, we used as little as 99.73% fewer samples for training, originating from the m2caiSeg dataset, and remained competitive even when testing on the unseen cholecSeg8k dataset.",
        "authors": "Karel Moens, Jonas De Vylder, Matthew B. Blaschko, Tinne Tuytelaars",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.02"
    },
    {
        "number": 99,
        "UID": "F-99",
        "forum": "https://openreview.net/forum?id=1GxyidfQzc",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "NcIEMIL: Rethinking Decoupled Multiple Instance Learning Framework for Histopathological Slide Classification",
        "abstract": "On account of superiority in annotation efficiency, multiple instance learning (MIL) has proved to be a promising framework for the whole slide image (WSI) classification in pathological diagnosis. However, current methods employ fully- or semi-decoupled frameworks to address the trade-off between billions of pixels and limited computational resources. This exacerbates the information bottleneck, leading to instance representations in a high-rank space that contains semantic redundancy compared to the potential low-rank category space of instances. Additionally, most negative instances are also independent of the positive properties of the bag. To address this, we introduce a weakly annotation-supervised filtering network, aiming to restore the low-rank nature of the slide-level representations. We then design a parallel aggregation structure that utilizes spatial attention mechanisms to model inter-correlation between instances and simultaneously assigns corresponding weights to channel dimensions to alleviate the redundant information introduced by feature extraction. Extensive experiments on the private gastrointestinal chemotaxis dataset and CAMELYON16 breast dataset show that our proposed framework is capable of handling both binary and multivariate classification problems and outperforms state-of-the-art MIL-based methods. The code is available at: https://github.com/polyethylene16/NcIEMIL.",
        "authors": "Sun Qiehe, Doukou Jiang, Jiawen Li, Renao Yan, Yonghong He, Tian Guan, Zhiqiang Cheng",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - F.03"
    },
    {
        "number": 129,
        "UID": "F-129",
        "forum": "https://openreview.net/forum?id=sfjgmuvLS7",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Network conditioning for synergistic learning on partial annotations",
        "abstract": "The robustness and accuracy of multi-organ segmentation networks is limited by the scarcity of labels. A common strategy to alleviate the annotation burden is to use partially labelled datasets, where each image can be annotated for a subset of all organs of interest. Unfortunately, this approach causes inconsistencies in the background class since it can now include target organs. Moreover, we consider the even more relaxed setting of region-based segmentation, where voxels can be labelled for super-regions, thus causing further inconsistencies across annotations. Here we propose CoNeMOS (Conditional Network for Multi-Organ Segmentation), a framework that leverages a label-conditioned network for synergistic learning on partially labelled region-based segmentations. Conditioning is achieved by combining convolutions with expressive Feature-wise Linear Modulation (FiLM) layers, whose parameters are controlled by an auxiliary network. In contrast to other conditioning methods, FiLM layers are stable to train and add negligible computation overhead, which enables us to condition the entire network. As a result, the network can learn where it needs to extract shared or label-specific features, instead of imposing it with the architecture (e.g., with different segmentation heads). By encouraging flexible synergies across labels, our method obtains state-of-the-art results for the segmentation of challenging low-resolution fetal MRI data. Our code is available at https://github.com/BBillot/CoNeMOS.",
        "authors": "Benjamin Billot, Neel Dey, Esra Abaci Turk, Ellen Grant, Polina Golland",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - O.05"
    },
    {
        "number": 129,
        "UID": "O-129",
        "forum": "https://openreview.net/forum?id=sfjgmuvLS7",
        "Track": "Full Paper",
        "Session": "Oral 1.3 - Semi/Weakly-Supervised Learning",
        "Final Decision": "Oral",
        "title": "Network conditioning for synergistic learning on partial annotations",
        "abstract": "The robustness and accuracy of multi-organ segmentation networks is limited by the scarcity of labels. A common strategy to alleviate the annotation burden is to use partially labelled datasets, where each image can be annotated for a subset of all organs of interest. Unfortunately, this approach causes inconsistencies in the background class since it can now include target organs. Moreover, we consider the even more relaxed setting of region-based segmentation, where voxels can be labelled for super-regions, thus causing further inconsistencies across annotations. Here we propose CoNeMOS (Conditional Network for Multi-Organ Segmentation), a framework that leverages a label-conditioned network for synergistic learning on partially labelled region-based segmentations. Conditioning is achieved by combining convolutions with expressive Feature-wise Linear Modulation (FiLM) layers, whose parameters are controlled by an auxiliary network. In contrast to other conditioning methods, FiLM layers are stable to train and add negligible computation overhead, which enables us to condition the entire network. As a result, the network can learn where it needs to extract shared or label-specific features, instead of imposing it with the architecture (e.g., with different segmentation heads). By encouraging flexible synergies across labels, our method obtains state-of-the-art results for the segmentation of challenging low-resolution fetal MRI data. Our code is available at https://github.com/BBillot/CoNeMOS.",
        "authors": "Benjamin Billot, Neel Dey, Esra Abaci Turk, Ellen Grant, Polina Golland",
        "Time": "Day 1 \u2014 16:30-17:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - O.05",
        "Chairs": "Dimitris Samaras & Mickael Sdika"
    },
    {
        "number": 22,
        "UID": "F-22",
        "forum": "https://openreview.net/forum?id=7iW2nuL2lS",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "From Barlow Twins to Triplet Training: Differentiating Dementia with Limited Data",
        "abstract": "Differential diagnosis of dementia is challenging due to overlapping symptoms, with structural magnetic resonance imaging (MRI) being the primary method for diagnosis. Despite the clinical value of computer-aided differential diagnosis, research has been limited, mainly due to the absence of public datasets that contain diverse types of dementia. This leaves researchers with small in-house datasets that are insufficient for training deep neural networks (DNNs). Self-supervised learning shows promise for utilizing unlabeled MRI scans in training, but small batch sizes for volumetric brain scans make its application challenging. To address these issues, we propose Triplet Training for differential diagnosis with limited target data. It consists of three key stages: (i) self-supervised pre-training on unlabeled data with Barlow Twins, (ii) self-distillation on task-related data, and (iii) fine-tuning on the target dataset. Our approach significantly outperforms traditional training strategies, achieving a balanced accuracy of 75.6%. We further provide insights into the training process by visualizing changes in the latent space after each step. Finally, we validate the robustness of Triplet Training in terms of its individual components in a comprehensive ablation study. Our code is available at https://github.com/ai-med/TripletTraining.",
        "authors": "Yitong Li, Tom Nuno Wolf, Sebastian P\u00f6lsterl, Igor Yakushev, Dennis M. Hedderich, Christian Wachinger",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - O.04"
    },
    {
        "number": 22,
        "UID": "O-22",
        "forum": "https://openreview.net/forum?id=7iW2nuL2lS",
        "Track": "Full Paper",
        "Session": "Oral 1.3 - Semi/Weakly-Supervised Learning",
        "Final Decision": "Oral",
        "title": "From Barlow Twins to Triplet Training: Differentiating Dementia with Limited Data",
        "abstract": "Differential diagnosis of dementia is challenging due to overlapping symptoms, with structural magnetic resonance imaging (MRI) being the primary method for diagnosis. Despite the clinical value of computer-aided differential diagnosis, research has been limited, mainly due to the absence of public datasets that contain diverse types of dementia. This leaves researchers with small in-house datasets that are insufficient for training deep neural networks (DNNs). Self-supervised learning shows promise for utilizing unlabeled MRI scans in training, but small batch sizes for volumetric brain scans make its application challenging. To address these issues, we propose Triplet Training for differential diagnosis with limited target data. It consists of three key stages: (i) self-supervised pre-training on unlabeled data with Barlow Twins, (ii) self-distillation on task-related data, and (iii) fine-tuning on the target dataset. Our approach significantly outperforms traditional training strategies, achieving a balanced accuracy of 75.6%. We further provide insights into the training process by visualizing changes in the latent space after each step. Finally, we validate the robustness of Triplet Training in terms of its individual components in a comprehensive ablation study. Our code is available at https://github.com/ai-med/TripletTraining.",
        "authors": "Yitong Li, Tom Nuno Wolf, Sebastian P\u00f6lsterl, Igor Yakushev, Dennis M. Hedderich, Christian Wachinger",
        "Time": "Day 1 \u2014 16:30-17:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - O.04",
        "Chairs": "Dimitris Samaras & Mickael Sdika"
    },
    {
        "number": 225,
        "UID": "F-225",
        "forum": "https://openreview.net/forum?id=iWfUcg4FrD",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Combining Reconstruction-based Unsupervised Anomaly Detection with Supervised Segmentation for Brain MRIs",
        "abstract": "In contrast to supervised deep learning approaches, unsupervised anomaly detection (UAD) methods can be trained with healthy data only and do not require pixel-level annotations, enabling the identification of unseen pathologies. While this is promising for clinical screening tasks, reconstruction-based UAD methods fall short in segmentation accuracy compared to supervised models. Therefore, self-supervised UAD approaches have been proposed to improve segmentation accuracy. Typically, synthetic anomalies are used to train a segmentation network in a supervised fashion. However, this approach does not effectively generalize to real pathologies. We propose a framework combining reconstruction-based and self-supervised UAD methods to improve both segmentation performance for known anomalies and generalization to unknown pathologies. The framework includes an unsupervised diffusion model trained on healthy data to produce pseudo-healthy reconstructions and a supervised Unet trained to delineate anomalies from deviations between input- reconstruction pairs. Besides the effective use of synthetic training data, this framework allows for weakly-supervised training with small annotated data sets, generalizing to unseen pathologies. Our results show that with our approach, utilizing annotated data sets during training can substantially improve the segmentation performance for in-domain data while maintaining the generalizability of reconstruction-based approaches to pathologies unseen during training.",
        "authors": "Finn Behrendt, Debayan Bhattacharya, Lennart Maack, Julia Kr\u00fcger, Roland Opfer, Alexander Schlaefer",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - O.07"
    },
    {
        "number": 225,
        "UID": "O-225",
        "forum": "https://openreview.net/forum?id=iWfUcg4FrD",
        "Track": "Full Paper",
        "Session": "Oral 1.3 - Semi/Weakly-Supervised Learning",
        "Final Decision": "Oral",
        "title": "Combining Reconstruction-based Unsupervised Anomaly Detection with Supervised Segmentation for Brain MRIs",
        "abstract": "In contrast to supervised deep learning approaches, unsupervised anomaly detection (UAD) methods can be trained with healthy data only and do not require pixel-level annotations, enabling the identification of unseen pathologies. While this is promising for clinical screening tasks, reconstruction-based UAD methods fall short in segmentation accuracy compared to supervised models. Therefore, self-supervised UAD approaches have been proposed to improve segmentation accuracy. Typically, synthetic anomalies are used to train a segmentation network in a supervised fashion. However, this approach does not effectively generalize to real pathologies. We propose a framework combining reconstruction-based and self-supervised UAD methods to improve both segmentation performance for known anomalies and generalization to unknown pathologies. The framework includes an unsupervised diffusion model trained on healthy data to produce pseudo-healthy reconstructions and a supervised Unet trained to delineate anomalies from deviations between input- reconstruction pairs. Besides the effective use of synthetic training data, this framework allows for weakly-supervised training with small annotated data sets, generalizing to unseen pathologies. Our results show that with our approach, utilizing annotated data sets during training can substantially improve the segmentation performance for in-domain data while maintaining the generalizability of reconstruction-based approaches to pathologies unseen during training.",
        "authors": "Finn Behrendt, Debayan Bhattacharya, Lennart Maack, Julia Kr\u00fcger, Roland Opfer, Alexander Schlaefer",
        "Time": "Day 1 \u2014 16:30-17:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - O.07",
        "Chairs": "Dimitris Samaras & Mickael Sdika"
    },
    {
        "number": 298,
        "UID": "F-298",
        "forum": "https://openreview.net/forum?id=U3vfFn9WQ7",
        "Track": "Full Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "IHCScoreGAN: An unsupervised generative adversarial network for end-to-end ki67 scoring for clinical breast cancer diagnosis",
        "abstract": "Ki67 is a biomarker whose activity is routinely measured and scored by pathologists through  immunohistochemistry (IHC) staining, which informs clinicians of patient prognosis and guides treatment. Currently, most clinical laboratories rely on a tedious, inconsistent manual scoring process to quantify the percentage of Ki67-positive cells. While many works have shown promise for Ki67 quantification using computational approaches, the current state-of-the-art methods have limited real-world feasibility: they either require large datasets of meticulous cell-level ground truth labels to train, or they provide pre-trained weights that may not generalize well to in-house data. To overcome these challenges, we propose IHCScoreGAN, the first unsupervised deep learning framework for end-to-end Ki67 scoring without the need for any ground truth labels. IHCScoreGAN only requires IHC image samples and unpaired synthetic data, yet it learns to generate colored cell segmentation masks while simultaneously predicting cell center point and biomarker expressions for Ki67 scoring, made possible through our novel dual-branch generator structure. We validated our framework on a large cohort of 2,136 clinically signed-out cases, yielding an accuracy of 0.97 and an F1-score of 0.95 and demonstrating substantially better performance than a pre-trained state-of-the-art supervised model. By removing ground truth requirements, our unsupervised technique constitutes an important step towards easily-trained Ki67 scoring solutions which can train on out-of-domain data in an unsupervised manner.",
        "authors": "Carl Molnar, Thomas E. Tavolara, Christopher A. Garcia, David S. McClintock, Mark D. Zarella, Wenchao Han",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - O.06"
    },
    {
        "number": 298,
        "UID": "O-298",
        "forum": "https://openreview.net/forum?id=U3vfFn9WQ7",
        "Track": "Full Paper",
        "Session": "Oral 1.3 - Semi/Weakly-Supervised Learning",
        "Final Decision": "Oral",
        "title": "IHCScoreGAN: An unsupervised generative adversarial network for end-to-end ki67 scoring for clinical breast cancer diagnosis",
        "abstract": "Ki67 is a biomarker whose activity is routinely measured and scored by pathologists through  immunohistochemistry (IHC) staining, which informs clinicians of patient prognosis and guides treatment. Currently, most clinical laboratories rely on a tedious, inconsistent manual scoring process to quantify the percentage of Ki67-positive cells. While many works have shown promise for Ki67 quantification using computational approaches, the current state-of-the-art methods have limited real-world feasibility: they either require large datasets of meticulous cell-level ground truth labels to train, or they provide pre-trained weights that may not generalize well to in-house data. To overcome these challenges, we propose IHCScoreGAN, the first unsupervised deep learning framework for end-to-end Ki67 scoring without the need for any ground truth labels. IHCScoreGAN only requires IHC image samples and unpaired synthetic data, yet it learns to generate colored cell segmentation masks while simultaneously predicting cell center point and biomarker expressions for Ki67 scoring, made possible through our novel dual-branch generator structure. We validated our framework on a large cohort of 2,136 clinically signed-out cases, yielding an accuracy of 0.97 and an F1-score of 0.95 and demonstrating substantially better performance than a pre-trained state-of-the-art supervised model. By removing ground truth requirements, our unsupervised technique constitutes an important step towards easily-trained Ki67 scoring solutions which can train on out-of-domain data in an unsupervised manner.",
        "authors": "Carl Molnar, Thomas E. Tavolara, Christopher A. Garcia, David S. McClintock, Mark D. Zarella, Wenchao Han",
        "Time": "Day 1 \u2014 16:30-17:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - O.06",
        "Chairs": "Dimitris Samaras & Mickael Sdika"
    },
    {
        "number": 10,
        "UID": "S-10",
        "forum": "https://openreview.net/forum?id=dJWeNm1ACh",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Advancing ASL Kidney Image Registration: A Tailored Pipeline with Groupwise VoxelMorph",
        "abstract": "Arterial spin labeling (ASL) provides a non-invasive assessment of renal blood flow, but it faces difficulties due to motion artifacts and the effects of blood inflow. This work introduces GVox, a deep learning-based motion correction (MoCo) framework tailored for ASL imaging. GVox extends VoxelMorph, incorporating cortical signal enhancement as metric to optimize and groupwise inference as main contribution. Proposed GVox demonstrates superior performance compared to the baseline Elastix, with significantly improved image similarity and computational efficiency.",
        "authors": "Anne Oyarzun-Dome\u00f1o, Izaskun Cia, Rebeca Echeverria-Chasco, Mar\u00eda A. Fern\u00e1ndez-Seara, Paloma L. Martin-Moreno, Nuria Garcia-Fernandez, Gorka Bastarrika, Javier Navallas, Arantxa Villanueva",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.03"
    },
    {
        "number": 102,
        "UID": "S-102",
        "forum": "https://openreview.net/forum?id=Jyw2pHKeFk",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Utilizing Weak-to-Strong Consistency for Semi-Supervised Glomeruli Segmentation",
        "abstract": "Accurate segmentation of glomerulus instances attains high clinical significance in the automated analysis of renal biopsies to aid in diagnosing and monitoring kidney disease. Analyzing real-world histopathology images often encompasses inter-observer variability and requires a labor-intensive process of data annotation. Therefore, conventional supervised learning approaches generally achieve sub-optimal performance when applied to external datasets. Considering these challenges, we present a semi-supervised learning approach for glomeruli segmentation based on the weak-to-strong consistency framework validated on multiple real-world datasets. Our experimental results on 3 independent datasets indicate superior performance of our approach as compared with existing supervised baseline models such as U-Net and SegFormer.",
        "authors": "Irina Zhang, Jim Denholm, Azam Hamidinekoo, Oskar \u00c5lund, Christopher Bagnall, Joana Pal\u00e9s Huix, Michal Sulikowski, Ortensia Vito, Arthur Lewis, Robert Unwin, Magnus Soderberg, Nikolay Burlutskiy, Talha Qaiser",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.13"
    },
    {
        "number": 108,
        "UID": "S-108",
        "forum": "https://openreview.net/forum?id=Ow20DosS2K",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "SDDA-MAE: Self-distillation enhanced Dual Attention Masked Autoencoder for Small-scale Medical Image Datasets",
        "abstract": "Masked Autoencoder (MAE) has shown promise as a self-supervised learning method in natural images. However, its application in medical imaging is limited by data scarcity. To alleviate this challenge, we propose SDDA-MAE, a method for direct pre-training and fine-tuning on targeted datasets without the requirement of self-supervised pre-training on an extra large dataset. The Dual Attention Transformer (DAT) serves as the backbone for enhanced spatial and channel-wise image representation. During the pre-training stage, we employ Self-distillation (SD) to transfer knowledge from the decoder, containing global information, to the encoder, which holds local information, improving weight initialization for downstream tasks. Experimental results demonstrate our method outperforms numerous self-supervised and supervised state-of-the-art (SOTA) methods in tasks like medical image segmentation and classification, even without pre-training on larger upstream datasets.",
        "authors": "Yunze Wang, Silin Chen, Tianyang Wang, Jingxin Liu",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.18"
    },
    {
        "number": 111,
        "UID": "S-111",
        "forum": "https://openreview.net/forum?id=NGAMehiw5U",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Self-supervised contrastive learning unveils cortical folding pattern linked to prematurity",
        "abstract": "Brain folding patterns have been reported to carry clinically relevant information. The brain folds mainly during the last trimester of pregnancy, and the process might be durably disturbed by preterm birth. Yet little is known about preterm-specific patterns. In this work, we train a self-supervised model (SimCLR) on the UKBioBank cohort (21070 adults) to represent the right superior temporal sulcus (STS) region and apply it to sulci images of 374 babies from the dHCP database, containing preterms and full-terms, and acquired at 40 weeks post-menstrual age. We find a lower variability in the preterm embeddings, supported by the identification of a knob pattern, missing in the extremely preterm population.",
        "authors": "Julien Laval, Denis Rivi\u00e8re, Aymeric Gaudin, Vincent Frouin, Jessica Dubois, Andrea Gondova, Jean-Francois Mangin, Jo\u00ebl Chavas",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.11"
    },
    {
        "number": 117,
        "UID": "S-117",
        "forum": "https://openreview.net/forum?id=7SMswKSKIX",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Rapid Personalization of PDE-Based Tumor Growth using a Differentiable Forward Model",
        "abstract": "Partial differential equation (PDE) based brain tumor growth models have the potential to personalize glioma therapy. However, calibrating these models to individual patients is computationally expensive using traditional optimization techniques. In this work, we propose an approach leveraging the differentiability of deep learning (DL) based PDE forward solvers to efficiently calibrate the tumor models. Through gradient-based optimization with respect to the input tumor parameters, we iteratively minimize the loss between the predicted and actual tumor distribution in the patient's MRI scans. We evaluate our method on a cohort of nine glioma patients and demonstrate a dramatic reduction in the time to solve the inverse problem from hours, using typically employed evolutionary sampling or Monte Carlo methods, to minutes while achieving comparable modeling results.",
        "authors": "Jonas Weidner, Ivan Ezhov, Michal Balcerak, bjoern menze, Benedikt Wiestler",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.15"
    },
    {
        "number": 142,
        "UID": "S-142",
        "forum": "https://openreview.net/forum?id=fj5lCxYpCs",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Pre-training of U-Net Encoder for Improved Keypoint Detection in Transmitral Doppler Imaging",
        "abstract": "Self-supervised learning enables models to extract meaningful representations from unlabelled data. These representations can then be effectively transferred to supervised learning tasks, often requiring less labelled data compared to traditional approaches. The BT-Unet method leverages the strengths of U-Net for medical image segmentation tasks and is specifically designed to facilitate Barlow twins pre-training of backbone networks, such as ResNet50. However, accurate keypoint detection in medical images remains a challenge. This study investigates the potential of pre-training these backbones with unlabelled, domain-specific medical imagery using the Barlow Twins method to enhance keypoint detection performance in BT-Unet. We hypothesise that pre-training with domain-specific data will lead to more accurate and robust detection of Doppler peak velocities in Mitral Inflow ultrasound images compared to models trained without pre-training.",
        "authors": "Abas Abdi, Jevgeni Jevsikov, Eman I Alajrami, Isreal Ufumaka, Patricia Fernandes, Nasim DadashiSerej, Darrel P Francis, Massoud Zolgharni",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.17"
    },
    {
        "number": 155,
        "UID": "S-155",
        "forum": "https://openreview.net/forum?id=sMiSQP8zmr",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Does Multimodality Help in Deep Learning-Based Structural Heart Disease Detection?",
        "abstract": "Structural heart disease (SHD) is typically diagnosed using transthoracic echocardiograms (TTEs), a modality underutilized in the United States. We investigate what combination of common clinical modalities in electrocardiograms (ECGs), posteroanterior view chest X-rays, and structured electronic health record (EHR) data can detect SHD labels generated with an TTE unseen by the model. Our experiments show that ECG-based models in both unimodal and multimodal settings performed best and that the inclusion of additional modalities with a late-fusion approach can give a marginal performance improvement.",
        "authors": "Young Sang Choi, Shalmali Joshi, Linyuan Jing, Pierre Elias",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.08"
    },
    {
        "number": 162,
        "UID": "S-162",
        "forum": "https://openreview.net/forum?id=tvPboxOKBc",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Genomics-Embedded Histopathology Whole Slide Image Encoding for Data-efficient Survival Prediction",
        "abstract": "Extensive works have shown that fusing histopathology images with genomics features can significantly improve the performance of survival prediction. However, the current methods still require both image and genomics data during the inference phase. In this work, we proposed the Genomics-Embedded WSI Encoding (GEE) framework where a proxy branch is built to guide the WSI encoder to extract genomics-related features from image modality. It makes the model achieve comparable inference accuracy solely based on image modality input when compared to the SOTA multi-modal-based survival prediction methods.",
        "authors": "Kun Wu, Zhiguo Jiang, Xinyu Zhu, Jun Shi, Yushan Zheng",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.01"
    },
    {
        "number": 176,
        "UID": "S-176",
        "forum": "https://openreview.net/forum?id=j8v7qc5bof",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "A Label-Free and Data-Free Training Strategy for Vasculature Segmentation in serial sectioning OCT Data",
        "abstract": "Serial sectioning Optical Coherence Tomography (sOCT) is a high-throughput, label free microscopic imaging technique that is becoming increasingly popular to study post-mortem neurovasculature. Quantitative analysis of the vasculature requires highly accurate segmentation; however, sOCT has low signal-to-noise-ratio and displays a wide range of contrasts and artifacts that depend on acquisition parameters. Furthermore, labeled data is scarce and extremely time consuming to generate. Here, we leverage synthetic datasets of vessels to train a deep learning segmentation model. We construct the vessels with semi-realistic splines that simulate the vascular geometry and compare our model with realistic vascular labels generated by constrained constructive optimization. Both approaches yield similar Dice scores, although with very different false positive and false negative rates. This method addresses the complexity inherent in OCT images and paves the way for more accurate and efficient analysis of neurovascular structures.",
        "authors": "Etienne Chollet, Ya\u00ebl Balbastre, Caroline Magnain, Bruce Fischl, Hui Wang",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.20"
    },
    {
        "number": 19,
        "UID": "S-19",
        "forum": "https://openreview.net/forum?id=1qbQ3wPLK9",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Multi-View Attention Network to Improve Breast Cancer Detection",
        "abstract": "Breast cancer is the most prevalent cancer in women, and mammography is an effective imaging modality for detecting it in its early stages. However, identifying tumors in mammograms is challenging, and many AI algorithms have been proposed to assist radiologists in detecting them. This study focuses on demonstrating the potential of a multi-view attention network for breast cancer detection by investigating the change in the detection performance depending on the types of attention (no, single-view, or multi-view attention), image resolution (low or high), and backbone network (ResNet50 or HRNet). The experiment results showed that the detection performance of a high-resolution, multi-view attention network with an HRNet backbone was better than the other networks with different configurations, suggesting that multi-view attention has benefits in detecting masses on mammograms.",
        "authors": "Wen Tai, PinJui Huang, Dongmyung Shin",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.04"
    },
    {
        "number": 35,
        "UID": "S-35",
        "forum": "https://openreview.net/forum?id=GIhTiyAYy6",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Cell-painting phenotyping in filamentous fungi: deep learning for image-based drug discovery",
        "abstract": "The advancement of Cell Painting, a relatively new morphological profiling assay, requires adequate and effective image analysis methods suitable for high-content data. That is why, developing deep learning methods for biological image analysis is extremely relevant. In this paper, we present feature extraction results on Cell Painting images of phytopathogenic fungi using deep learning techniques. We give an overview of the current method, which is based on a supervised convolutional neural network, and present the prospects and advantages of leveraging a self-supervised model based on vision transformers in the given problem setting.",
        "authors": "Polina Simkina, Bruno Leggio, Gerard Sanroma-Guell, Yoann Huet",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.16"
    },
    {
        "number": 53,
        "UID": "S-53",
        "forum": "https://openreview.net/forum?id=GYSPQ8tXLe",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Weakly supervised lung cancer detection on label-free intraoperative microscopy with higher harmonic generation",
        "abstract": "Higher harmonic generation microscopy (HHGM) enables label-free on-site imaging of fresh tissue, potentially allowing a new means of pathology assessment for disease diagnosis.\nWe investigate the potential of using self-supervised learning (SSL) in combination with weakly-supervised, attention-based, clustering constrained multiple instance learning (CLAM) to detect lung cancer in HHGM images.\nFirst, we tailor encoders to HHGM-specific data domain via both SimCLR and DINO SSL. \nSecond, we train a CLAM classifier with and without an SSL feature extractor on 100 HHGM images acquired during bronchoscopy procedures.\nWe show that SSL pre-training with random initialization and CLAM are beneficial to intraoperatively detect lung cancer in HHGM images.",
        "authors": "Siem de Jong, Marie Louise Groot, Roel L. J. Verhoeven, Erik H. F. M. van der Heijden, Francesco Ciompi",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.09"
    },
    {
        "number": 54,
        "UID": "S-54",
        "forum": "https://openreview.net/forum?id=U4uiovUtim",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Scaling Temporal and Volumetric Datasets for Tumor Localization with Weak Annotations",
        "abstract": "Creating large-scale, well-annotated datasets is vital for training AI algorithms in tumor detection. However, with limited resources, it is challenging to determine the best type of annotations when annotating massive amounts of unlabeled data. To address this issue, we focus on polyps in colonoscopy videos and pancreatic tumors in abdominal CT scans, both requiring extensive pixel-wise annotation due to the high dimensional nature of the data. In this paper, we develop a new annotation strategy, termed Drag&Drop, which simplifies the annotation process to drag and drop, proving more efficient for temporal and volumetric imaging. Furthermore, we introduce a novel weakly supervised learning method based on the watershed algorithm to leverage Drag&Drop annotations. Experimental results show that, with limited resources, allocating weak annotations from diverse patients enhances model robustness more effectively than per-pixel ones on a limited set of images. In summary, this research proposes an efficient annotation strategy that is useful for creating large-scale datasets for screening tumors in various medical modalities.",
        "authors": "Yu-Cheng Chou, Bowen Li, Deng-Ping Fan, Alan Yuille, Zongwei Zhou",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.12"
    },
    {
        "number": 59,
        "UID": "S-59",
        "forum": "https://openreview.net/forum?id=SbrRyCrc4v",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "RegSegNet: A Joint Registration Segmentation Network for Automatic Liver Segmentation from Non-contrast 3D SPECT-CT Images",
        "abstract": "3D SPECT-CT images play a vital role in the treatment process for liver cancer. However, in many cases, the CT scan taken alongside SPECT is non-contrast, making the liver segmentation task a tough challenge for both AI models and radiologists. Previous methods often faced trade-offs between accuracy and runtime. This study introduces RegSegNet, a deep learning model that utilizes image registration to effectively guide the liver segmentation in non-contrast SPECT-CT images. The proposed method is trained and evaluated on a dataset consisting of 60 liver cancer patients. Experimental results show that RegSegNet significantly outperforms baseline methods in terms of runtime while maintaining comparable accuracy.",
        "authors": "Xuan Loc Pham, Manh Ha Luu, Hong Son Mai, Ngoc Ha Le, Bram van Ginneken, Alessa Hering",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.05"
    },
    {
        "number": 6,
        "UID": "S-6",
        "forum": "https://openreview.net/forum?id=atsESsgxPa",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Fusing Echocardiography Images and Medical Records for Continuous Patient Stratification",
        "abstract": "Deep learning now enables automatic and robust extraction of cardiac function descriptors from echocardiographic sequences, such as ejection fraction or strain. These descriptors provide fine-grained information that physicians consider, in conjunction with global variables from the clinical record, to assess patients' condition. Drawing on novel transformer models applied to tabular data (e.g. variables from electronic health records), we propose a method that considers descriptors extracted from medical records and echocardiograms to learn a representation of hypertension, a difficult-to-characterize and highly prevalent cardiovascular pathology. Our method first embeds each descriptor separately using modality-specific approaches. These embeddings are fed as tokens to a transformer encoder, which combines them into a unified representation of the patient to predict a clinical rating. This task is formulated as an ordinal classification to enforce a pathological continuum in the representation space. We observe trends along this continuum for a cohort of 239 hypertensive patients to describe the gradual effects of hypertension on cardiac function descriptors. Our analysis shows that i) pretrained weights from a foundation model allow to reach good performance (83% accuracy) even with limited data ($<$ 200 training samples), ii) trends across the population are reproducible between trainings, and iii) for descriptors known to interact with hypertension, patterns are consistent with prior physiological knowledge.",
        "authors": "Nathan Painchaud, Pierre-Yves Courand, Pierre-marc Jodoin, Nicolas Duchateau, Olivier Bernard",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.02"
    },
    {
        "number": 68,
        "UID": "S-68",
        "forum": "https://openreview.net/forum?id=687qKBVmEN",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Myoblast Mutation Classification via Microgroove-Induced Nuclear Deformations",
        "abstract": "Microgroove substrates induce 3D nuclear deformations in various adherent cell types. In this study, we explore the capacity of a CNN classifier to identify myoblast mutations through subtle differences in nuclear deformations on 2D fluorescence microscopy images. A large set of experimental images from immunostained nuclei screened on microgroove platforms is exploited. Leveraging ResNet-50 in a weakly-supervised setting, we present preliminary results to accurately classify healthy myoblasts from laminopathy-associated mutations. We achieved F1 scores of 0.99 and 0.94 at whole-image and patch levels evaluations. These results demonstrate the potential for microgroove screening as a functional diagnostic device of diseases characterized by aberrant nuclear deformations.",
        "authors": "Xingjian Zhang, Claire Leclech, Bettina Roellinger, Catherine Coirault, Elsa Angelini, Abdul I. Barakat",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.14"
    },
    {
        "number": 70,
        "UID": "S-70",
        "forum": "https://openreview.net/forum?id=6ZCChFrGWD",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Classification of Medical Text in Small and Imbalanced Datasets in a Non-English Language",
        "abstract": "Natural language processing (NLP) in the medical domain can underperform in real-case applications involving small datasets in a non-English language with few labeled samples and imbalanced classes. We evaluated a range of state-of-the-art NLP models on datasets\nrepresenting this situation and found that current approaches are not sufficiently accurate to allow for fully automated classification, but can potentially be used to filter and reduce the amount of manual labeling.",
        "authors": "Vincent Beliveau, Helene Kaas, Martin Prener, Claes Ladefoged, Desmond Elliott, Gitte M. Knudsen, Lars H. Pinborg, Melanie Ganz",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.10"
    },
    {
        "number": 74,
        "UID": "S-74",
        "forum": "https://openreview.net/forum?id=zZL7voUMbb",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Efficient Repairing of Disconnected Pulmonary Tree Structures via Point-based Implicit Fields",
        "abstract": "Segmentation of pulmonary tree structures is critical for diagnosing and planning treatment for lung diseases. However, existing deep learning models often yield inaccurate segmentations, resulting in disconnected vessel predictions. To overcome this challenge, we propose an efficient framework for reconstructing pulmonary trees. Initially, we represent disconnected pulmonary tree structures as sparse surface point clouds. Next, we utilize a point cloud network to extract features and predict the disconnected segments. Finally, we employ an implicit neural network to infer the occupancy of arbitrary points, thereby facilitating efficient reconstruction. We validate the effectiveness of our approach on real data from 799 subjects; the code and data will be publicly available.",
        "authors": "Ziqiao Weng, Jiancheng Yang, Dongnan Liu, Weidong Cai",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.06"
    },
    {
        "number": 84,
        "UID": "S-84",
        "forum": "https://openreview.net/forum?id=yxX1D556ic",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Self-Supervision Revives Simple Multiple Instance Classification Methods in Pathology",
        "abstract": "Multiple Instance Learning (MIL) is the current solution for classifying whole slide pathology images (WSI). MIL divides WSIs into patches, treating each slide as a bag of instances with a global label. There are two main MIL approaches: instance-based and embedding-based. The former classifies patches independently and aggregates scores for bag label prediction, while the latter performs bag classification after aggregating patch embeddings. Even if instance-based methods are more interpretable, embedding-based MILs have been preferred in the past, due to their robustness to poor feature extractors. In parallel, many works started to use self-supervised learning (SSL) for training better encoders. However, despite the use of SSL feature extractors, many works continue to endorse the superiority of embedding-based MILs. Here, we show that with a good SSL feature extractor, simple instance-based MILs, with very few parameters, obtain similar or better performance than complex, state-of-the-art embedding-based MIL methods.",
        "authors": "Ali Mammadov, Loic Le Folgoc, Julien Adam, Anne Buronfosse, Gilles Hayem, Guillaume HOCQUET, Pietro Gori",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.19"
    },
    {
        "number": 88,
        "UID": "S-88",
        "forum": "https://openreview.net/forum?id=qyhlarjiqs",
        "Track": "Short Paper",
        "Session": "Poster 1.2",
        "Final Decision": "Poster",
        "title": "Evidence Regularization for Multimodal Deep Evidential Regression",
        "abstract": "Uncertainty estimation is crucial in cost-sensitive areas, especially in the medical field, where multimodal information is common and effective. Existing studies have found the zero-confidence issue in unimodal settings, while the analysis in multimodal scenarios is lacking. In this work, we introduce the confidence paradox, where unimodal uncertainty is high but decreases after fusion, and present evidence regularization to tackle this issue. Initial results on the cubic and CT slice datasets show reduced root mean squared errors and improved detection of out-of-distribution samples, improving predictive reliability and training stability.",
        "authors": "Zhimin Shao, Weibei DOU",
        "Time": "Day 1 \u2014 15:30-16:30",
        "Poster time": "Day 1 \u2014 15:30-16:30",
        "Poster ID": "Poster 1.2 - S.07"
    },
    {
        "number": 15,
        "UID": "O-15",
        "forum": "https://openreview.net/forum?id=LVRhXa0q5r",
        "Track": "Full Paper",
        "Session": "Oral 2.1 - Clinical Translation & Domain Adaption",
        "Final Decision": "Oral",
        "title": "Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed Opportunity",
        "abstract": "Foundation models have significantly advanced medical image analysis through the pre-train fine-tune paradigm. Among various fine-tuning algorithms, Parameter-Efficient Fine-Tuning (PEFT) is increasingly utilized for knowledge transfer across diverse tasks, including vision-language and text-to-image generation. However, its application in medical image analysis is relatively unexplored due to the lack of a structured benchmark for evaluating PEFT methods. This study fills this gap by evaluating 17 distinct PEFT algorithms across convolutional and transformer-based networks on image classification and text-to-image generation tasks using six medical datasets of varying size, modality, and complexity. Through a battery of over 700 controlled experiments, our findings demonstrate PEFT's effectiveness, particularly in low data regimes common in medical imaging, with performance gains of up to 22% in discriminative and generative tasks. These recommendations can assist the community in incorporating PEFT into their workflows and facilitate fair comparisons of future PEFT methods, ensuring alignment with advancements in other areas of machine learning and AI.",
        "authors": "Raman Dutt, Linus Ericsson, Pedro Sanchez, Sotirios A. Tsaftaris, Timothy Hospedales",
        "Time": "Day 2 \u2014 9:00-10:15",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.04",
        "Chairs": "Philipp Berens & Mattias Heinrich"
    },
    {
        "number": 15,
        "UID": "F-15",
        "forum": "https://openreview.net/forum?id=LVRhXa0q5r",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed Opportunity",
        "abstract": "Foundation models have significantly advanced medical image analysis through the pre-train fine-tune paradigm. Among various fine-tuning algorithms, Parameter-Efficient Fine-Tuning (PEFT) is increasingly utilized for knowledge transfer across diverse tasks, including vision-language and text-to-image generation. However, its application in medical image analysis is relatively unexplored due to the lack of a structured benchmark for evaluating PEFT methods. This study fills this gap by evaluating 17 distinct PEFT algorithms across convolutional and transformer-based networks on image classification and text-to-image generation tasks using six medical datasets of varying size, modality, and complexity. Through a battery of over 700 controlled experiments, our findings demonstrate PEFT's effectiveness, particularly in low data regimes common in medical imaging, with performance gains of up to 22% in discriminative and generative tasks. These recommendations can assist the community in incorporating PEFT into their workflows and facilitate fair comparisons of future PEFT methods, ensuring alignment with advancements in other areas of machine learning and AI.",
        "authors": "Raman Dutt, Linus Ericsson, Pedro Sanchez, Sotirios A. Tsaftaris, Timothy Hospedales",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.04"
    },
    {
        "number": 228,
        "UID": "O-228",
        "forum": "https://openreview.net/forum?id=YFfOvLf2T1",
        "Track": "Full Paper",
        "Session": "Oral 2.1 - Clinical Translation & Domain Adaption",
        "Final Decision": "Oral",
        "title": "Detecting Brain Anomalies in Clinical Routine with the $\\beta$-VAE: Feasibility Study on Age-Related White Matter Hyperintensities",
        "abstract": "This experimental study assesses the ability of variational autoencoders (VAEs) to perform anomaly detection in clinical routine, in particular the detection of age-related white matter lesions in brain MRIs acquired at different hospitals and gathered in a clinical data warehouse (CDW). We pre-trained a state-of-the-art $\\beta$-VAE on a healthy cohort of over 10,000 FLAIR MR images from the UK Biobank to learn the distribution of healthy brains. The model was then fine-tuned on a cohort of nearly 700 healthy FLAIR images coming from a CDW. We first ensured the good performance of our pre-trained model compared with the state-of-the-art using a widely used public dataset (MSSEG). We then validated it on our target task, age-related WMH detection, on ADNI3 and on a curated clinical dataset from a single-site neuroradiology department, for which we had manually delineated lesion masks. Next, we applied the fine-tuned $\\beta$-VAE for anomaly detection in a CDW characterised by an exceptional heterogeneity in terms of hospitals, scanners and image quality. We found a correlation between the Fazekas scores extracted from the radiology reports and the volumes of the lesions detected by our model, providing a first insight into the performance of VAEs in a clinical setting. We also observed that our model was robust to image quality, which strongly varies in the CDW. However, despite these encouraging results, such approach is not ready for an application in clinical routine yet due to occasional failures in detecting certain lesions, primarily attributed to the poor quality of the images reconstructed by the VAE.",
        "authors": "Sophie Loizillon, Yannick Jacob, Maire Aur\u00e9lien, Didier Dormont, Olivier Colliot, Ninon Burgos, APPRIMAGE Study Group",
        "Time": "Day 2 \u2014 9:00-10:15",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.02",
        "Chairs": "Philipp Berens & Mattias Heinrich"
    },
    {
        "number": 228,
        "UID": "F-228",
        "forum": "https://openreview.net/forum?id=YFfOvLf2T1",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Detecting Brain Anomalies in Clinical Routine with the $\\beta$-VAE: Feasibility Study on Age-Related White Matter Hyperintensities",
        "abstract": "This experimental study assesses the ability of variational autoencoders (VAEs) to perform anomaly detection in clinical routine, in particular the detection of age-related white matter lesions in brain MRIs acquired at different hospitals and gathered in a clinical data warehouse (CDW). We pre-trained a state-of-the-art $\\beta$-VAE on a healthy cohort of over 10,000 FLAIR MR images from the UK Biobank to learn the distribution of healthy brains. The model was then fine-tuned on a cohort of nearly 700 healthy FLAIR images coming from a CDW. We first ensured the good performance of our pre-trained model compared with the state-of-the-art using a widely used public dataset (MSSEG). We then validated it on our target task, age-related WMH detection, on ADNI3 and on a curated clinical dataset from a single-site neuroradiology department, for which we had manually delineated lesion masks. Next, we applied the fine-tuned $\\beta$-VAE for anomaly detection in a CDW characterised by an exceptional heterogeneity in terms of hospitals, scanners and image quality. We found a correlation between the Fazekas scores extracted from the radiology reports and the volumes of the lesions detected by our model, providing a first insight into the performance of VAEs in a clinical setting. We also observed that our model was robust to image quality, which strongly varies in the CDW. However, despite these encouraging results, such approach is not ready for an application in clinical routine yet due to occasional failures in detecting certain lesions, primarily attributed to the poor quality of the images reconstructed by the VAE.",
        "authors": "Sophie Loizillon, Yannick Jacob, Maire Aur\u00e9lien, Didier Dormont, Olivier Colliot, Ninon Burgos, APPRIMAGE Study Group",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.02"
    },
    {
        "number": 300,
        "UID": "O-300",
        "forum": "https://openreview.net/forum?id=Y15taNvfFN",
        "Track": "Full Paper",
        "Session": "Oral 2.1 - Clinical Translation & Domain Adaption",
        "Final Decision": "Oral",
        "title": "Beyond Structured Attributes: Image-Based Predictive Trends for Chest X-Ray Classification",
        "abstract": "A commonly emphasized challenge in medical AI is the drop in performance when testing on data from institutions other than those used for training. However, even if models trained on distinct datasets perform similarly well overall, they may still exhibit other systematic differences. Here, we study these potential dataset-centric prediction variations using two popular chest x-ray datasets, CheXpert (CXP) and MIMIC-CXR (MMC). While CXP-trained models generally perform better on CXP than MMC test data and vice versa, this performance decrease is not uniform across individual images. We find that image-level variations in predictions are not random but can be inferred well above chance, even for pathologies where the overall performance gap is small, suggesting that there are systematic tendencies of models trained on different datasets. Furthermore, these \"predictive tendencies\" are not solely explained by image statistics or attributes like radiographic position or patient sex, but rather are pathology-specific and related to higher-order image characteristics. Our findings stress the complexity of AI robustness and generalization, highlighting the need for a nuanced approach that especially considers the diversity of pathology presentation.",
        "authors": "Katharina V Hoebel, Jesseba Fernando, William Lotter",
        "Time": "Day 2 \u2014 9:00-10:15",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.03",
        "Chairs": "Philipp Berens & Mattias Heinrich"
    },
    {
        "number": 300,
        "UID": "F-300",
        "forum": "https://openreview.net/forum?id=Y15taNvfFN",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Beyond Structured Attributes: Image-Based Predictive Trends for Chest X-Ray Classification",
        "abstract": "A commonly emphasized challenge in medical AI is the drop in performance when testing on data from institutions other than those used for training. However, even if models trained on distinct datasets perform similarly well overall, they may still exhibit other systematic differences. Here, we study these potential dataset-centric prediction variations using two popular chest x-ray datasets, CheXpert (CXP) and MIMIC-CXR (MMC). While CXP-trained models generally perform better on CXP than MMC test data and vice versa, this performance decrease is not uniform across individual images. We find that image-level variations in predictions are not random but can be inferred well above chance, even for pathologies where the overall performance gap is small, suggesting that there are systematic tendencies of models trained on different datasets. Furthermore, these \"predictive tendencies\" are not solely explained by image statistics or attributes like radiographic position or patient sex, but rather are pathology-specific and related to higher-order image characteristics. Our findings stress the complexity of AI robustness and generalization, highlighting the need for a nuanced approach that especially considers the diversity of pathology presentation.",
        "authors": "Katharina V Hoebel, Jesseba Fernando, William Lotter",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.03"
    },
    {
        "number": 45,
        "UID": "O-45",
        "forum": "https://openreview.net/forum?id=SlMMyPqQTm",
        "Track": "Full Paper",
        "Session": "Oral 2.1 - Clinical Translation & Domain Adaption",
        "Final Decision": "Oral",
        "title": "Comparing the Performance of Radiation Oncologists versus a Deep Learning Dose Predictor to Estimate Dosimetric Impact of Segmentation Variations for Radiotherapy",
        "abstract": "Current evaluation methods for quality control of manual/automated tumor and organs-at- risk segmentation for radiotherapy are driven mostly by geometric correctness. It is however known that geometry-driven segmentation quality metrics cannot characterize potentially detrimental dosimetric effects of sub-optimal tumor segmentation. In this work, we build on prior studies proposing deep learning-based dose prediction models to extend its use for the task of contour quality evaluation of brain tumor treatment planning. Using a test set of 54 contour variants and their corresponding dose plans, we show that our model can be used to dosimetrically assess the quality of contours and can outperform clinical expert radiation oncologists while estimating sub-optimal situations. We compare results against three such experts and demonstrate improved accuracy in addition to time savings. Our code is available at https://github.com/ubern-mia/radonc-vs-dldp.",
        "authors": "Amith Jagannath Kamath, Zahira Mercado Auf der Maur, Robert Poel, Jonas Willmann, Ekin Ermis, Elena Riggenbach, Nicolaus Andratschke, Mauricio Reyes",
        "Time": "Day 2 \u2014 9:00-10:15",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.05",
        "Chairs": "Philipp Berens & Mattias Heinrich"
    },
    {
        "number": 45,
        "UID": "F-45",
        "forum": "https://openreview.net/forum?id=SlMMyPqQTm",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Comparing the Performance of Radiation Oncologists versus a Deep Learning Dose Predictor to Estimate Dosimetric Impact of Segmentation Variations for Radiotherapy",
        "abstract": "Current evaluation methods for quality control of manual/automated tumor and organs-at- risk segmentation for radiotherapy are driven mostly by geometric correctness. It is however known that geometry-driven segmentation quality metrics cannot characterize potentially detrimental dosimetric effects of sub-optimal tumor segmentation. In this work, we build on prior studies proposing deep learning-based dose prediction models to extend its use for the task of contour quality evaluation of brain tumor treatment planning. Using a test set of 54 contour variants and their corresponding dose plans, we show that our model can be used to dosimetrically assess the quality of contours and can outperform clinical expert radiation oncologists while estimating sub-optimal situations. We compare results against three such experts and demonstrate improved accuracy in addition to time savings. Our code is available at https://github.com/ubern-mia/radonc-vs-dldp.",
        "authors": "Amith Jagannath Kamath, Zahira Mercado Auf der Maur, Robert Poel, Jonas Willmann, Ekin Ermis, Elena Riggenbach, Nicolaus Andratschke, Mauricio Reyes",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.05"
    },
    {
        "number": 57,
        "UID": "O-57",
        "forum": "https://openreview.net/forum?id=X8Rh7gLtDu",
        "Track": "Full Paper",
        "Session": "Oral 2.1 - Clinical Translation & Domain Adaption",
        "Final Decision": "Oral",
        "title": "ThickV-Stain: Unprocessed Thick Tissues Virtual Staining for Rapid Intraoperative Histology",
        "abstract": "Virtual staining has shown great promise in realizing a rapid and low-cost clinical alternative for pathological examinations, eliminating the need for chemical reagents and laborious staining procedures. However, most of the previous studies mainly focus on thin slice samples, which still require tissue sectioning and are unsuitable for intraoperative use. In this paper, we propose a multi-scale model to virtually stain label-free and slide-free biological tissues, allowing hematoxylin- and eosin- (H&E) staining generation in less than a minute for an image with 100 million pixels. We name this ThickV-Stain model, specifically developed to virtually stain intricated and unprocessed thick tissues. We harness the ability of a multi-scale network to encourage the model to capture multiple-level micromorphological characteristics from low-resolution images. Experimental results highlight the advantages of our multi-scale method for virtual staining on unprocessed thick samples. We also show the effectiveness of ThickV-Stain on thin sections, showing generalizability to other clinical workflows. The proposed method enables us to obtain virtually stained images from unstained samples within minutes and can be seamlessly integrated with downstream pathological analysis tasks, providing an efficient alternative scheme for intraoperative assessment as well as general pathological examination.",
        "authors": "Lulin Shi, Xingzhong Hou, Ivy H. M. Wong, Simon C. K. Chan, Zhenghui Chen, Claudia T. K. Lo, Terence T. W. Wong",
        "Time": "Day 2 \u2014 9:00-10:15",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.01",
        "Chairs": "Philipp Berens & Mattias Heinrich"
    },
    {
        "number": 57,
        "UID": "F-57",
        "forum": "https://openreview.net/forum?id=X8Rh7gLtDu",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "ThickV-Stain: Unprocessed Thick Tissues Virtual Staining for Rapid Intraoperative Histology",
        "abstract": "Virtual staining has shown great promise in realizing a rapid and low-cost clinical alternative for pathological examinations, eliminating the need for chemical reagents and laborious staining procedures. However, most of the previous studies mainly focus on thin slice samples, which still require tissue sectioning and are unsuitable for intraoperative use. In this paper, we propose a multi-scale model to virtually stain label-free and slide-free biological tissues, allowing hematoxylin- and eosin- (H&E) staining generation in less than a minute for an image with 100 million pixels. We name this ThickV-Stain model, specifically developed to virtually stain intricated and unprocessed thick tissues. We harness the ability of a multi-scale network to encourage the model to capture multiple-level micromorphological characteristics from low-resolution images. Experimental results highlight the advantages of our multi-scale method for virtual staining on unprocessed thick samples. We also show the effectiveness of ThickV-Stain on thin sections, showing generalizability to other clinical workflows. The proposed method enables us to obtain virtually stained images from unstained samples within minutes and can be seamlessly integrated with downstream pathological analysis tasks, providing an efficient alternative scheme for intraoperative assessment as well as general pathological examination.",
        "authors": "Lulin Shi, Xingzhong Hou, Ivy H. M. Wong, Simon C. K. Chan, Zhenghui Chen, Claudia T. K. Lo, Terence T. W. Wong",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.01"
    },
    {
        "number": 58,
        "UID": "O-58",
        "forum": "https://openreview.net/forum?id=8vtxQoxPCC",
        "Track": "Full Paper",
        "Session": "Oral 2.1 - Clinical Translation & Domain Adaption",
        "Final Decision": "Oral",
        "title": "Disruptive Autoencoders: Leveraging Low-level features for 3D Medical Image Pre-training",
        "abstract": "Harnessing the power of pre-training on large-scale datasets like ImageNet forms a funda- mental building block for the progress of representation learning-driven solutions in com- puter vision. Medical images are inherently different from natural images as they are acquired in the form of many modalities (CT, MR, PET, Ultrasound etc.) and contain granulated information like tissue, lesion, organs etc. These characteristics of medical im- ages require special attention towards learning features representative of local context. In this work, we focus on designing an effective pre-training framework for 3D radiology im- ages. First, we propose a new masking strategy called local masking where the masking is performed across channel embeddings instead of tokens to improve the learning of local feature representations. We combine this with classical low-level perturbations like adding noise and downsampling to further enable low-level representation learning. To this end, we introduce Disruptive Autoencoders, a pre-training framework that attempts to re- construct the original image from disruptions created by a combination of local masking and low-level perturbations. We curate a large-scale dataset to enable pre-training of 3D medical radiology images (MRI and CT). The proposed pre-training framework is tested across multiple downstream tasks and achieves state-of-the-art performance. Notably, our proposed method tops the public test leaderboard of BTCV multi-organ segmentation chal- lenge. Our code can be found here: https://github.com/Project-MONAI/research-contributions/tree/main/DAE.",
        "authors": "Jeya Maria Jose Valanarasu, Yucheng Tang, Dong Yang, Ziyue Xu, Can Zhao, Wenqi Li, Vishal M. Patel, Bennett Allan Landman, Daguang Xu, Yufan He, Vishwesh Nath",
        "Time": "Day 2 \u2014 9:00-10:15",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.06",
        "Chairs": "Philipp Berens & Mattias Heinrich"
    },
    {
        "number": 58,
        "UID": "F-58",
        "forum": "https://openreview.net/forum?id=8vtxQoxPCC",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Disruptive Autoencoders: Leveraging Low-level features for 3D Medical Image Pre-training",
        "abstract": "Harnessing the power of pre-training on large-scale datasets like ImageNet forms a funda- mental building block for the progress of representation learning-driven solutions in com- puter vision. Medical images are inherently different from natural images as they are acquired in the form of many modalities (CT, MR, PET, Ultrasound etc.) and contain granulated information like tissue, lesion, organs etc. These characteristics of medical im- ages require special attention towards learning features representative of local context. In this work, we focus on designing an effective pre-training framework for 3D radiology im- ages. First, we propose a new masking strategy called local masking where the masking is performed across channel embeddings instead of tokens to improve the learning of local feature representations. We combine this with classical low-level perturbations like adding noise and downsampling to further enable low-level representation learning. To this end, we introduce Disruptive Autoencoders, a pre-training framework that attempts to re- construct the original image from disruptions created by a combination of local masking and low-level perturbations. We curate a large-scale dataset to enable pre-training of 3D medical radiology images (MRI and CT). The proposed pre-training framework is tested across multiple downstream tasks and achieves state-of-the-art performance. Notably, our proposed method tops the public test leaderboard of BTCV multi-organ segmentation chal- lenge. Our code can be found here: https://github.com/Project-MONAI/research-contributions/tree/main/DAE.",
        "authors": "Jeya Maria Jose Valanarasu, Yucheng Tang, Dong Yang, Ziyue Xu, Can Zhao, Wenqi Li, Vishal M. Patel, Bennett Allan Landman, Daguang Xu, Yufan He, Vishwesh Nath",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - O.06"
    },
    {
        "number": 109,
        "UID": "F-109",
        "forum": "https://openreview.net/forum?id=G9Te2IevNm",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Self-supervised pretraining in the wild imparts image acquisition robustness to medical image transformers: an application to lung cancer segmentation",
        "abstract": "Self-supervised learning (SSL) is an approach to pretrain models with unlabeled datasets and extract useful feature representations such that these models can be easily fine-tuned for various downstream tasks. \\textcolor{blue}{Self-pretraining applies SSL on curated task-specific datasets.} Increasing availability of public data repositories has now made it possible to utilize diverse and large task unrelated datasets to pretrain models in the \"wild\" using SSL. However, the benefit of such wild-pretraining over self-pretraining has not been studied in the context of medical image analysis. Hence, we analyzed transformers (Swin and ViT) and a convolutional neural network created using wild- and self-pretraining trained to segment lung tumors from 3D-computed tomography (CT) scans in terms of: (a) accuracy, (b) fine-tuning epoch efficiency, and (c) robustness to image acquisition differences (contrast versus non-contrast, slice thickness, and image reconstruction kernels). We also studied feature reuse using centered kernel alignment (CKA) with the Swin networks. Our analysis with two independent testing (public N = 139; internal N = 196) datasets showed that wild-pretrained Swin models significantly outperformed self-pretrained Swin for the various imaging acquisitions. Fine-tuning epoch efficiency was higher for both wild-pretrained Swin and ViT models compared to their self-pretrained counterparts. Feature reuse close to the final encoder layers was lower than in the early layers for wild-pretrained models irrespective of the pretext tasks used in SSL. Models and code will be made available through GitHub upon manuscript acceptance.",
        "authors": "Jue Jiang, Harini Veeraraghavan",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.17"
    },
    {
        "number": 113,
        "UID": "F-113",
        "forum": "https://openreview.net/forum?id=nqM0lZMevc",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "ASMR: Angular Support for Malfunctioning Client Resilience in Federated Learning",
        "abstract": "Federated Learning (FL) allows the training of deep neural networks in a distributed and\nprivacy-preserving manner. However, this concept suffers from malfunctioning updates\nsent by the attending clients that cause global model performance degradation. Reasons\nfor this malfunctioning might be technical issues, disadvantageous training data, or mali-\ncious attacks. Most of the current defense mechanisms are meant to require impractical\nprerequisites like knowledge about the number of malfunctioning updates, which makes\nthem unsuitable for real-world applications. To counteract these problems, we introduce\na novel method called ASMR, that dynamically excludes malfunctioning clients based on\ntheir angular distance. Our novel method does not require any hyperparameters or knowl-\nedge about the number of malfunctioning clients. Our experiments showcase the detection\ncapabilities of ASMR in an image classification task on a histopathological dataset, while\nalso presenting findings on the significance of dynamically adapting decision boundaries.",
        "authors": "Mirko Konstantin, Moritz Fuchs, Anirban Mukhopadhyay",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.06"
    },
    {
        "number": 120,
        "UID": "F-120",
        "forum": "https://openreview.net/forum?id=FbM7sDDAZ4",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Towards a Collective Medical Imaging AI: Enabling Continual Learning from Peers",
        "abstract": "Federated learning is an exciting area within machine learning that allows cross-silo training of large-scale machine learning models on disparate or similar tasks in a privacy-preserving manner. However, conventional federated learning frameworks require a synchronous training schedule and are incapable of lifelong learning. To that end, we propose an asynchronous decentralized federated lifelong learning (ADFLL) method that allows agents in the system to asynchronously and continually learn from their own previous experiences and others', thus overcoming the potential drawbacks of conventional federated learning. We evaluate the ADFLL framework in two experimental setups for deep reinforcement learning (DRL) based landmark localization across different imaging modalities, orientations, and sequences. The ADFLL was compared to central aggregation and conventional lifelong learning for upper-bound comparison and with a conventional DRL model for lower-bound comparison. Across all the experiments, ADFLL demonstrated excellent capability to collaboratively learn all tasks across all the agents compared to the baseline models in in-distribution and out-of-distribution test sets. In conclusion, we provide a flexible, efficient, and robust federated lifelong learning framework that can be deployed in real-world applications.",
        "authors": "Guangyao Zheng, Vladimir Braverman, Jeffrey Leal, Steven Rowe, Doris Leung, Michael A. Jacobs, Vishwa Sanjay Parekh",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.18"
    },
    {
        "number": 138,
        "UID": "F-138",
        "forum": "https://openreview.net/forum?id=00gWBAAbMI",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Improving CNN-Based Mitosis Detection through Rescanning Annotated Glass Slides and Atypical Mitosis Subtyping",
        "abstract": "The identification of mitotic figures (MFs) is a routine task in the histopathological assessment of tumor malignancy with known limitations for human observers. For a machine learning pipeline to robustly detect MFs, it must overcome a variety of conditions such as different scanners, staining protocols, tissue configurations, and organ types. \nIn order to develop a deep learning-based algorithm that can cope with these challenges, there are two obstacles that need to be overcome: obtaining a large-scale dataset of MF annotations spread across different domains of interest, including whole slide images (WSIs) exhaustively annotated for MFs, and using the annotated MFs in an efficient training process to extract the most relevant features for classification.\nOur work attempts to address both of these challenges and establishes an MF detection pipeline trained solely on animal data, yet competitive on the mixed human/animal MIDOG22 dataset, and, in particular, on human breast cancer.\nFirst, we propose a processing pipeline that allows us to strengthen the true scanner robustness of our dataset by physically rescanning the glass slides of annotated WSIs and registering MF positions. To enable the use of such rescans for training, we propose a novel learning paradigm tailored for labels that match partially, which allows to account for ambiguous MF positions in the rescans caused by spurious, suboptimal fine-focus on potential MFs by the scanner. Second, we demonstrate how a multi-task learning approach for MF subtypes, including the prediction of atypical mitotic figures (AMFs), can significantly enhance a model's ability to distinguish MFs from imposters. Our algorithm, using a standard object detection pipeline, performs very competitively with an average test set F1 value across five runs of 0.80 on the MIDOG22 training set. We also demonstrate its ability to stratify overall survival on the TCGA-BRCA dataset based on mitotic density, though it falls short of reaching significance in stratifying survival based on AMFs.",
        "authors": "Rutger RH Fick, Christof Bertram, Marc Aubreville",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.07"
    },
    {
        "number": 14,
        "UID": "F-14",
        "forum": "https://openreview.net/forum?id=lemBHfGNZD",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "REINDIR: Repeated Embedding Infusion for Neural Deformable Image Registration",
        "abstract": "The use of implicit neural representations (INRs) has been explored for medical image registration in a number of recent works. Using these representations has several advantages over both classic optimization-based methods and deep learning-based methods, but it is hindered by long optimization times during inference. To address this issue, we propose REINDIR: Repeated Embedding Infusion for Neural Deformable Image Registration. REINDIR is a meta-learning framework that uses a combination of an image encoder and template representations, which are infused with image embeddings to specialize them for a pair of test images. This specialization results in a better initialization for the subsequent optimization process. By broadcasting the encodings to fill our modulation weight matrices, we greatly reduce the required size of the encoder compared to approaches that predict the complete weight matrices directly. Additionally, our method retains the flexibility to infuse arbitrarily large encodings. The presented approach greatly improves the efficiency of deformable registration with INRs when applied to (near-)IID data, while remaining robust to severe domain shifts from the distribution the method is trained on.",
        "authors": "Louis van Harten, Rudolf Leonardus Mirjam Van Herten, Ivana Isgum",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.15"
    },
    {
        "number": 150,
        "UID": "F-150",
        "forum": "https://openreview.net/forum?id=YUMVjxdIqn",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "A Comprehensive Benchmark of Supervised and Self-supervised Pre-training on Multi-view Chest X-ray Classification",
        "abstract": "Chest X-ray analysis in medical imaging has largely focused on single-view methods. However, recent advancements have led to the development of multi-view approaches that harness the potential of multiple views for the same patient. Although these methods have shown improvements, it is especially difficult to collect large multi-view labeled datasets owing to the prohibitive annotation costs and acquisition times. Hence, it is crucial to address the multi-view setting in the low data regime. Pre-training is a critical component to ensure efficient performance in this low data regime, as evidenced by its improvements in natural and medical imaging. However, in the multi-view setup, such pre-training strategies have received relatively little attention and ImageNet initialization remains largely the norm. We bridge this research gap by conducting an extensive benchmarking study illustrating the efficacy of 10 strong supervised and self-supervised models pre-trained on both natural and medical images for multi-view chest X-ray classification. We further examine the performance in the low data regime by training these methods on 1%, 10%, and 100% fractions of the training set. Moreover, our best models yield significant improvements compared to existing state-of-the-art multi-view approaches, outperforming them by as much as 9.9%, 8.8% and 1.6% on the 1%, 10%, and 100% data fractions respectively. We hope this benchmark will spur the development of stronger multi-view medical imaging models, similar to the role of such benchmarks in other computer vision and medical imaging domains. As open science, we make our code publicly available to aid in the development of stronger multi-view models.",
        "authors": "Muhammad Muneeb Afzal, Muhammad Osama Khan, Yi Fang",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.08"
    },
    {
        "number": 16,
        "UID": "F-16",
        "forum": "https://openreview.net/forum?id=vu4LsiSpf7",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Unsupervised Domain Adaptation of Brain MRI Skull Stripping Trained on Adult Data to Newborns: Combining Synthetic Data with Domain Invariant Features",
        "abstract": "Skull-stripping constitutes a crucial initial step in neuroimaging analysis, and supervised deep-learning models have demonstrated considerable success in automating this task. However, a notable challenge is the limited availability of publicly accessible newborn brain MRI datasets. Furthermore, these datasets frequently use diverse post-processing techniques to improve image quality, which may not be consistently feasible in all clinical settings. Additionally, manual segmentation of newborn brain MR images is labor-intensive and demands specialized expertise, rendering it inefficient. While various adult brain MRI datasets with skull-stripping masks are publicly available, applying supervised models trained on these datasets directly to newborns poses a challenge due to domain shift. We propose a methodology that combines domain adversarial models to learn domain-invariant features between newborn and adult data, along with the integration of synthetic data generated using a Gaussian Mixture Model (GMM) as well as data augmentation procedures. The GMM method facilitates the creation of synthetic brain MR images, ensuring a diverse and representative input from multiple domains within our source dataset during model training. The data augmentation procedures were tailored to make the adult MRI data distribution closer to the newborn data distribution. Our results yielded an overall Dice coefficient of 0.9308 \u00b1 0.0297 (mean\u00b1 std), outperforming all compared unsupervised domain adaptation models and surpassing some supervised techniques previously trained on newborn data. This project's code and trained models' weights are publicly available at https://github.com/abbasomidi77/GMM-Enhanced-DAUnet",
        "authors": "Abbas Omidi, Amirmohammad Shamaei, Anouk Verschuu, Regan King, Lara Leijser, Roberto Souza",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.02"
    },
    {
        "number": 161,
        "UID": "F-161",
        "forum": "https://openreview.net/forum?id=uoRbMNoZ7w",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "VariViT: A Vision Transformer for Variable Image Sizes",
        "abstract": "Vision Transformers (ViTs) have emerged as the state-of-the-art architecture in representation learning, leveraging self-attention mechanisms to excel in various tasks. ViTs split images into fixed-size patches, constraining them to a predefined size and necessitating pre-processing steps like resizing, padding, or cropping. This poses challenges in medical imaging, particularly with irregularly shaped structures like tumors. A fixed bounding box crop size produces input images with highly variable foreground-to-background ratios. Resizing medical images can degrade information and introduce artifacts, impacting diagnosis. Hence, tailoring variable-sized crops to regions of interest can enhance feature representation capabilities. Moreover, large images are computationally expensive, and smaller sizes risk information loss, presenting a computation-accuracy tradeoff. We propose VariViT, an improved ViT model crafted to handle variable image sizes while maintaining a consistent patch size. VariViT employs a novel positional embedding resizing scheme for a variable number of patches. We also implement a new batching strategy within VariViT to reduce computational complexity, resulting in faster training and inference times. In our evaluations on two 3D brain MRI datasets, VariViT surpasses vanilla ViTs and ResNet in glioma genotype prediction and brain tumor classification. It achieves F1-scores of 75.5% and 76.3%, respectively, learning more discriminative features. Our proposed batching strategy reduces computation time by up to 30% compared to conventional architectures. These findings underscore the efficacy of VariViT in image representation learning.",
        "authors": "Aswathi Varma, Suprosanna Shit, Chinmay Prabhakar, Daniel Scholz, Hongwei Bran Li, bjoern menze, Daniel Rueckert, Benedikt Wiestler",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.09"
    },
    {
        "number": 168,
        "UID": "F-168",
        "forum": "https://openreview.net/forum?id=FmCscsj7Ey",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Erase to Enhance: Data-Efficient Machine Unlearning in MRI Reconstruction",
        "abstract": "Machine unlearning is a promising paradigm for removing unwanted data samples from a trained model, towards ensuring compliance with privacy regulations and limiting harmful biases. Although unlearning has been shown in, e.g., classification and recommendation systems, its potential in medical image-to-image translation, specifically in image reconstruction, has not been thoroughly investigated. This paper shows that machine unlearning is possible in MRI tasks and has the potential to benefit for bias removal. We set up a protocol to study how much shared knowledge exists between datasets of different organs, allowing us to effectively quantify the effect of unlearning. Our study reveals that combining training data can lead to hallucinations and reduced image quality in the reconstructed data. We use unlearning to remove hallucinations as a proxy exemplar of undesired data removal. Indeed, we show that machine unlearning is possible without full retraining. Furthermore, our observations indicate that maintaining high performance is feasible even when using only a subset of retain data. We have made our code publicly accessible.",
        "authors": "Yuyang Xue, Jingshuai Liu, Steven McDonagh, Sotirios A. Tsaftaris",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.19"
    },
    {
        "number": 17,
        "UID": "F-17",
        "forum": "https://openreview.net/forum?id=1vUli5JSrr",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "DDA: Dimensionality Driven Augmentation Search for Contrastive Learning in Laparoscopic Surgery",
        "abstract": "Self-supervised learning (SSL) has the potential for effective representation learning in medical imaging, but the choice of data augmentation is critical and domain-specific. It remains uncertain if general augmentation policies suit surgical applications. In this work, we automate the search for suitable augmentation policies through a new method called Dimensionality Driven Augmentation Search (DDA). DDA leverages the local dimensionality of deep representations as a proxy target, and differentiably searches for suitable data augmentation policies in contrastive learning. We demonstrate the effectiveness and efficiency of DDA in navigating a large search space and successfully identifying an appropriate data augmentation policy for laparoscopic surgery. We systematically evaluate DDA  across three laparoscopic image classification and segmentation tasks, where it significantly improves over existing baselines. Furthermore, DDA's optimised set of augmentations provides insight into domain-specific dependencies when applying contrastive learning in medical applications. For example, while hue is an effective augmentation for natural images, it is not advantageous for laparoscopic images.",
        "authors": "Yuning Zhou, Henry Badgery, Matthew Read, James Bailey, Catherine Davey",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.03"
    },
    {
        "number": 175,
        "UID": "F-175",
        "forum": "https://openreview.net/forum?id=Wcb6Wynz3e",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Semi-supervised learning with Noisy Students improves domain generalization in optic disc and cup segmentation in uncropped fundus images",
        "abstract": "Automated optic disc (OD) and cup (OC) segmentation in fundus images has been widely explored for computer-aided diagnosis of glaucoma. However, existing models usually suffer from drops in performance when applied on images significantly different than those used for training.\nSeveral domain generalization strategies have been introduced to mitigate this issue, although they are trained and evaluated using images manually cropped around the optic nerve head. This operation eliminates most sources of domain variation, therefore overestimating their actual ability to cope with new, unseen patterns. \nIn this paper, we analyze the most recent and accurate methods for domain generalization in OD/OC segmentation by applying them on uncropped fundus pictures, observing notorious degradations in their performance when trained and evaluated under this setting.\nTo overcome their drawbacks, we also introduce a simple semi-supervised learning approach for domain generalization based on the Noisy Student framework.\nUsing a Teacher model trained on a combination of domains, we pseudo-labeled a dataset of 18.000 originally unlabeled images that are then used for training a Student model.\nThis semi-supervised setting allowed the Student network to capture additional sources of variability while retaining the original cues and patterns used by the Teacher through the weak annotations.\nOur results on eight different public datasets show improvements in every unseen domain over all alternative methods, and are available in https://github.com/eugeniaMoris/Noisy_student_ODOC_MIDL_2024.",
        "authors": "Eugenia Moris, Ignacio Larrabide, Jos\u00e9 Ignacio Orlando",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.10"
    },
    {
        "number": 178,
        "UID": "F-178",
        "forum": "https://openreview.net/forum?id=FVUeDnd2uS",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Multi-Objective Learning for Deformable Image Registration",
        "abstract": "Deformable image registration (DIR) involves optimization of multiple conflicting objectives, however, not many existing DIR algorithms are multi-objective (MO). Further, while there has been progress in the design of deep learning algorithms for DIR, there is no work in the direction of MO DIR using deep learning. In this paper, we fill this gap by combining a recently proposed approach for MO training of neural networks with a well-known deep neural network for DIR and create a deep learning based MO DIR approach. We evaluate the proposed approach for DIR of pelvic magnetic resonance imaging (MRI) scans. We experimentally demonstrate that the proposed MO DIR approach -- providing multiple DIR outputs for each patient that each correspond to a different trade-off between the objectives -- has additional desirable properties from a clinical use point-of-view as compared to providing a single DIR output. The experiments also show that the proposed MO DIR approach provides a better spread of DIR outputs across the entire trade-off front than simply training multiple neural networks with weights for each objective sampled from a grid of possible values.",
        "authors": "Monika Grewal, Henrike Westerveld, Peter Bosman, Tanja Alderliesten",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.11"
    },
    {
        "number": 209,
        "UID": "F-209",
        "forum": "https://openreview.net/forum?id=shuwpLaOJP",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Video-CT MAE: Self-supervised Video-CT Domain Adaptation for Vertebral Fracture Diagnosis",
        "abstract": "Early and accurate diagnosis of vertebral body anomalies is crucial for effectively treating spinal disorders, but the manual interpretation of CT scans can be time-consuming and error-prone. While deep learning has shown promise in automating vertebral fracture detection, improving the interpretability of existing methods is crucial for building trust and ensuring reliable clinical application. Vision Transformers (ViTs) offer inherent interpretability through attention visualizations but are limited in their application to 3D medical images due to reliance on 2D image pretraining. To address this challenge, we propose a novel approach combining the benefits of transfer learning from video-pretrained models and domain adaptation with self-supervised pretraining on a task-specific but unlabeled dataset. Compared to naive transfer learning from Video MAE, our method shows improved downstream task performance by 8.3 in F1 and a training speedup of factor 2. This closes the gap between videos and medical images, allowing a ViT to learn relevant anatomical features while adapting to the task domain. We demonstrate that our framework enables ViTs to effectively detect vertebral fractures in a low data regime, outperforming CNN-based state-of-the-art methods while providing inherent interpretability. Our task adaptation approach and dataset not only improve the performance of our proposed method but also enhance existing self-supervised pretraining approaches, highlighting the benefits of task-specific self-supervised pretraining for domain adaptation. The code is publicly available.",
        "authors": "Lukas Buess, Marijn F. Stollenga, David Schinz, Benedikt Wiestler, Jan Kirschke, Andreas Maier, Nassir Navab, Matthias Keicher",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.14"
    },
    {
        "number": 238,
        "UID": "F-238",
        "forum": "https://openreview.net/forum?id=Zg0mfl10o2",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "FedFDD: Federated Learning with Frequency Domain Decomposition for Low-Dose CT Denoising",
        "abstract": "Low-dose computed tomography (LDCT) enables imaging with minimal radiation exposure but typically results in noisy outputs. Deep learning algorithms have been emerging as popular tools for denoising LDCT images, where they typically rely on large data sets requiring data from multiple centers. However, LDCT images collected from different centers (clients) can present significant data heterogeneity, and the sharing of them between clients is also constrained by privacy regulations. In this work, we propose a personalized federated learning (FL) approach for enhancing model generalization across different organ images from multiple local clients while preserving data privacy. Empirically, we find that earlier FL methods tend to underperform single-set models on non-IID LDCT data due to the presence of data heterogeneity characterized by varying frequency patterns. To address this, we introduce a Federated Learning with Frequency Domain Decomposition (FedFDD) approach, which decomposes images into different frequency components and then updates high-frequency signals in an FL setting while preserving local low-frequency characteristics. Specifically, we leverage an adaptive frequency mask with discrete cosine transformation for the frequency domain decomposition. The proposed algorithm is evaluated on LDCT datasets of different organs and our experimental results show that FedFDD can surpass state-of-the-art FL methods as well as both localized and centralized models, especially on challenging LDCT denoising cases. \nOur code is available at https://github.com/xuhang2019/FedFDD.",
        "authors": "Xuhang Chen, Zeju Li, Zikun Xu, kaijie Xu, Cheng Ouyang, Chen Qin",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.20"
    },
    {
        "number": 313,
        "UID": "F-313",
        "forum": "https://openreview.net/forum?id=iefmK5E2N6",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Train Once, Deploy Anywhere: Edge-Guided Single-source Domain Generalization for Medical Image Segmentation",
        "abstract": "In medical image analysis, unsupervised domain adaptation models require retraining when receiving samples from a new data distribution, and multi-source domain generalization methods might be infeasible when there is only a single source domain. These will pose formidable obstacles to model deployment. To this end, we take the \"Train Once, Deploy Anywhere\" as our objective and consider a challenging but practical problem: Single-source Domain Generalization (SDG). Meanwhile, we note that (i) the medical image segmentation applications where generalization errors often come from imprecise predictions at the ambiguous boundary of anatomies and (ii) the edge of the image is domain-invariant, which can reduce the domain shift between the source and target domain in all network layers. Specifically, we borrow the prior knowledge from Digital Image Processing and take the edge of the image as input to enhance the model attention at the boundary of anatomies and improve the generalization performance on unknown target domains. Extensive experiments on three typical medical image segmentation datasets, which cover cross-sequence, cross-center, and cross-modality settings with various anatomical structures, demonstrate our method achieves superior generalization performance compared to the state-of-the-art SDG methods. The code is available at https://github.com/thinkdifferentor/EGSDG.",
        "authors": "JunJiang, Shi Gu",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.12"
    },
    {
        "number": 43,
        "UID": "F-43",
        "forum": "https://openreview.net/forum?id=5D71lHj9HZ",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Med-Tuning: A New Parameter-Efficient Tuning Framework for Medical Volumetric Segmentation",
        "abstract": "The \"pre-training then fine-tuning (FT)\" paradigm is widely adopted to boost the model performance of deep learning-based methods for medical volumetric segmentation. However, conventional full FT incurs high computational and memory costs. Thus, it is of increasing importance to fine-tune pre-trained models for medical volumetric segmentation tasks in a both effective and parameter-efficient manner. In this paper, we introduce a new framework named Med-Tuning to realize parameter-efficient tuning (PET) for medical volumetric segmentation task and an efficient plug-and-play module named Med-Adapter for task-specific feature extraction. With a small number of tuned parameters, our framework enhances the 2D baselines's precision on segmentation tasks, which are pre-trained on natural images. Extensive experiments on three benchmark datasets (CT and MRI modalities) show that our method achieves better results than previous PET methods on volumetric segmentation tasks. Compared to full FT, Med-Tuning reduces the fine-tuned model parameters by up to 4x, with even better segmentation performance. Our project webpage is at \\url{https://rubics-xuan.github.io/Med-Tuning/}.",
        "authors": "Jiachen Shen, Wenxuan Wang, Chen Chen, Jianbo Jiao, Jing Liu, Yan Zhang, Shanshan Song, Jiangyun Li",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.13"
    },
    {
        "number": 52,
        "UID": "F-52",
        "forum": "https://openreview.net/forum?id=MjoaKfhrzU",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Joint Motion Estimation with Geometric Deformation Correction for Fetal Echo Planar Images Via Deep Learning",
        "abstract": "In this paper, we introduce a novel end-to-end predictive model for efficient fetal motion correction using deep neural networks. Diverging from conventional methods that estimate fetal brain motions and geometric distortions separately, our approach introduces a newly developed joint learning framework that not only reliably estimates various degrees of rigid movements, but also effectively corrects local geometric distortions of fetal brain images. Specifically, we first develop a method to learn rigid motion through a closed-form update integrated into network training. Subsequently, we incorporate a diffeomorphic deformation estimation model to guide the motion correction network, particularly in regions where local distortions and deformations occur. To the best of our knowledge, our study is the first to simultaneously track fetal motion and address geometric deformations in fetal echo-planar images. We validated our model using real fetal functional magnetic resonance imaging data with simulated and real motions. Our method demonstrates significant practical value to measure, track, and correct fetal motion in fetal MRI.",
        "authors": "Jian Wang, Razieh Faghihpirayesh, Deniz Erdogmus, Ali Gholipour",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.04"
    },
    {
        "number": 7,
        "UID": "F-7",
        "forum": "https://openreview.net/forum?id=Ym30gCtKqN",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Target and task specific source-free domain adaptive image segmentation",
        "abstract": "Solving the domain shift problem during inference is essential in medical imaging as most deep-learning based solutions suffer from it. In practice, domain shifts are tackled by performing Unsupervised Domain Adaptation (UDA), where a model is adapted to an unlabeled target domain by leveraging the labelled source domain. In medical scenarios, the data comes with huge privacy concerns making it difficult to apply standard UDA techniques. Hence, a closer clinical setting is Source-Free UDA (SFUDA), where we have access to source trained model but not the source data during adaptation. Methods trying to solve SFUDA typically address the domain shift using pseudo-label based self-training techniques. However due to domain shift, these pseudo-labels are usually of high entropy and denoising them still does not make them perfect labels to supervise the model. Therefore, adapting the source model with noisy pseudo labels reduces its segmentation capability while addressing the domain shift. To this end, we propose a two-stage approach for source-free domain adaptive image segmentation: 1) Target-specific adaptation followed by 2) Task-specific adaptation. In the Stage-I, we focus on learning target-specific representation and generating high-quality pseudo labels by leveraging a proposed ensemble entropy minimization loss and selective voting strategy. In Stage II, we focus on improving segmentation performance by utilizing teacher-student self-training and augmentation-guided consistency loss, leveraging the pseudo labels obtained from Stage I. We evaluate our proposed method on both 2D fundus datasets and 3D MRI volumes across 7 different domain shifts where we achieve better performance than recent UDA and SF-UDA methods for medical image segmentation. Code is available at https://github.com/Vibashan/tt-sfuda.",
        "authors": "Vibashan VS, Jeya Maria Jose Valanarasu, Vishal M. Patel",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.01"
    },
    {
        "number": 79,
        "UID": "F-79",
        "forum": "https://openreview.net/forum?id=2Q3mTp6a6T",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "RADR: A Robust Domain-Adversarial-based Framework for Automated Diabetic Retinopathy Severity Classification",
        "abstract": "Diabetic retinopathy (DR), a potentially vision-threatening condition, necessitates accurate diagnosis and staging, which deep-learning models can facilitate. However, these models often struggle with robustness in clinical practice due to distribution shifts caused by variations in data acquisition protocols and hardware. We propose RADR, a novel deep-learning framework for DR severity classification, aimed at generalization across diverse datasets and clinic cameras. Our work builds upon existing research: we combine several ideas to perform extensive dataset curation, preprocessing, and enrichment with camera information. We then use a domain adversarial training regime, which encourages our model to extract features that are both task-relevant and invariant to domain shifts. We explore our framework in its various levels of complexity in combination with multiple data augmentations policies in an ablative fashion. Experimental results demonstrate the effectiveness of our proposed method, achieving competitive performance to multiple state-of-the-art models on three unseen external datasets.",
        "authors": "Sara M\u00ednguez Monedero, Fabian Westhaeusser, Ehsan Yaghoubi, Simone Frintrop, Marina Zimmermann",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.05"
    },
    {
        "number": 85,
        "UID": "F-85",
        "forum": "https://openreview.net/forum?id=8245ExLB4I",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "OFELIA: Optical Flow-based Electrode LocalIzAtion",
        "abstract": "Catheter ablation is one of the most common cardiac ablation procedures for atrial fibrillation, which is mainly based on catheters with electrodes collecting electrophysiology signals.\nCatheter electrode localization facilitates intraoperative catheter positioning, surgical planning, and other applications such as 3D model reconstruction.\nIn this paper, we propose a novel deep network for automatic electrode localization in an X-ray sequence, which integrates spatiotemporal features between adjacent frames, aided by optical flow maps.\nTo improve the utility and robustness of the proposed method, we first design a saturation-based optical flow dataset construction pipeline, then finetune the optical flow estimation to obtain more realistic and contrasting optical flow maps for electrode localization.\nThe extensive results on clinical-challenging test sequences reveal the effectiveness of our method, with a mean radial error (MRE) of 0.95 mm for radiofrequency catheters and an MRE of 0.71 mm for coronary sinus catheters, outperforming several state-of-the-art landmark detection methods.",
        "authors": "Xinyi Wang, Zikang Xu, Qingsong Yao, Yiyong Sun, S Kevin Zhou",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.16"
    },
    {
        "number": 106,
        "UID": "F-106",
        "forum": "https://openreview.net/forum?id=jeLDTCFltu",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "The Multiscale Surface Vision Transformer",
        "abstract": "Surface meshes are a favoured domain for representing structural and functional information on the human cortex, but their complex topology and geometry pose significant challenges for deep learning analysis. While Transformers have excelled as domain-agnostic architectures for sequence-to-sequence learning, the quadratic cost of the self-attention operation remains an obstacle for many dense prediction tasks. Inspired by some of the latest advances in hierarchical modelling with vision transformers, we introduce the Multiscale Surface Vision Transformer (MS-SiT) as a backbone architecture for surface deep learning. The self-attention mechanism is applied within local-mesh-windows to allow for high-resolution sampling of the underlying data, while a shifted-window strategy improves the sharing of information between windows. Neighbouring patches are successively merged, allowing the MS-SiT to learn hierarchical representations suitable for any prediction task. Results demonstrate that the MS-SiT outperforms existing surface deep learning methods for neonatal phenotyping prediction tasks using the Developing Human Connectome Project (dHCP) dataset. Furthermore, building the MS-SiT backbone into a U-shaped architecture for surface segmentation demonstrates competitive results on cortical parcellation using the UK Biobank (UKB) and manually-annotated MindBoggle datasets. Code and trained models are publicly available at https://github.com/metrics-lab/surface-vision-transformers.",
        "authors": "Simon Dahan, Logan Zane John Williams, Daniel Rueckert, Emma Claire Robinson",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.2 - O.02"
    },
    {
        "number": 106,
        "UID": "O-106",
        "forum": "https://openreview.net/forum?id=jeLDTCFltu",
        "Track": "Full Paper",
        "Session": "Oral 2.2 - Geometric Deep Learning & Federated Learning",
        "Final Decision": "Oral",
        "title": "The Multiscale Surface Vision Transformer",
        "abstract": "Surface meshes are a favoured domain for representing structural and functional information on the human cortex, but their complex topology and geometry pose significant challenges for deep learning analysis. While Transformers have excelled as domain-agnostic architectures for sequence-to-sequence learning, the quadratic cost of the self-attention operation remains an obstacle for many dense prediction tasks. Inspired by some of the latest advances in hierarchical modelling with vision transformers, we introduce the Multiscale Surface Vision Transformer (MS-SiT) as a backbone architecture for surface deep learning. The self-attention mechanism is applied within local-mesh-windows to allow for high-resolution sampling of the underlying data, while a shifted-window strategy improves the sharing of information between windows. Neighbouring patches are successively merged, allowing the MS-SiT to learn hierarchical representations suitable for any prediction task. Results demonstrate that the MS-SiT outperforms existing surface deep learning methods for neonatal phenotyping prediction tasks using the Developing Human Connectome Project (dHCP) dataset. Furthermore, building the MS-SiT backbone into a U-shaped architecture for surface segmentation demonstrates competitive results on cortical parcellation using the UK Biobank (UKB) and manually-annotated MindBoggle datasets. Code and trained models are publicly available at https://github.com/metrics-lab/surface-vision-transformers.",
        "authors": "Simon Dahan, Logan Zane John Williams, Daniel Rueckert, Emma Claire Robinson",
        "Time": "Day 2 \u2014 14:30-15:15",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.2 - O.02",
        "Chairs": "Loic Le Folgoc & Tolga Tasdizen"
    },
    {
        "number": 208,
        "UID": "F-208",
        "forum": "https://openreview.net/forum?id=x7EqWCyO5X",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Predicting Age-related Macular Degeneration Progression from Retinal Optical Coherence Tomography with Intra-Subject Temporal Consistency",
        "abstract": "The wide variability in the progression rates of Age-Related Macular Degeneration (AMD) and the absence of well-established clinical biomarkers make it difficult to predict an individual's risk of AMD progression from intermediate stage (iAMD) to late dry stage (dAMD) using Optical Coherence Tomography (OCT) scans.\nTo address this challenge, we propose to jointly train an AMD stage classifier to discriminate between iAMD and dAMD with a Neural-ODE that models the future trajectory of the disease progression in the learned embedding space. A temporal ordering is imposed such that the distance of a scan from the decision hyperplane of the AMD stage classifier is inversely related to its time-to-conversion. In addition, an intra-subject temporal consistency in the predicted conversion risk scores is ensured by incorporating a pair of longitudinal scans from the same eye during training. We evaluated our proposed method on a longitudinal dataset comprising 235 eyes (3,534 OCT scans) with 40 converters. The results demonstrate the effectiveness of our approach, achieving an average area under the ROC of 0.84 for predicting conversion within the next 6, 12, 18 and 24 months. Additionally, the Concordance Index of 0.78 surpasses the performance of several popular methods for survival analysis.",
        "authors": "Arunava Chakravarty, Taha Emre, Dmitrii Lachinov, Antoine Rivail, Ursula Schmidt-Erfurth, Hrvoje Bogunovi\u0107",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.2 - O.03"
    },
    {
        "number": 208,
        "UID": "O-208",
        "forum": "https://openreview.net/forum?id=x7EqWCyO5X",
        "Track": "Full Paper",
        "Session": "Oral 2.2 - Geometric Deep Learning & Federated Learning",
        "Final Decision": "Oral",
        "title": "Predicting Age-related Macular Degeneration Progression from Retinal Optical Coherence Tomography with Intra-Subject Temporal Consistency",
        "abstract": "The wide variability in the progression rates of Age-Related Macular Degeneration (AMD) and the absence of well-established clinical biomarkers make it difficult to predict an individual's risk of AMD progression from intermediate stage (iAMD) to late dry stage (dAMD) using Optical Coherence Tomography (OCT) scans.\nTo address this challenge, we propose to jointly train an AMD stage classifier to discriminate between iAMD and dAMD with a Neural-ODE that models the future trajectory of the disease progression in the learned embedding space. A temporal ordering is imposed such that the distance of a scan from the decision hyperplane of the AMD stage classifier is inversely related to its time-to-conversion. In addition, an intra-subject temporal consistency in the predicted conversion risk scores is ensured by incorporating a pair of longitudinal scans from the same eye during training. We evaluated our proposed method on a longitudinal dataset comprising 235 eyes (3,534 OCT scans) with 40 converters. The results demonstrate the effectiveness of our approach, achieving an average area under the ROC of 0.84 for predicting conversion within the next 6, 12, 18 and 24 months. Additionally, the Concordance Index of 0.78 surpasses the performance of several popular methods for survival analysis.",
        "authors": "Arunava Chakravarty, Taha Emre, Dmitrii Lachinov, Antoine Rivail, Ursula Schmidt-Erfurth, Hrvoje Bogunovi\u0107",
        "Time": "Day 2 \u2014 14:30-15:15",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.2 - O.03",
        "Chairs": "Loic Le Folgoc & Tolga Tasdizen"
    },
    {
        "number": 219,
        "UID": "F-219",
        "forum": "https://openreview.net/forum?id=RsP618PEFF",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "FluidRegNet: Longitudinal registration of retinal OCT images with new pathological fluids",
        "abstract": "Eye diseases such as the chronic central serous chorioretinopathy are characterized by fluid deposits that alter the retina and impair vision. These fluids occur at irregular intervals and may dissolve spontaneously or thanks to treatment. Accurately capturing this behavior within an image registration framework is challenging due to the resulting prominent tissue deformations and missing image correspondences between visits. This paper presents FluidRegNet, a convolutional neural network for the registration of successive optical coherence tomography images of the retina. The correspondence between time points is established by predicting the position of the origin of the fluids by creating a fluid seed in the form of sparse intensity offsets in the moving image and registering the fluid seed to the affected area in the follow-up image. We show that this leads to deformation fields that more accurately reflect the actual dynamics of retinal fluid growth compared to other image registration methods. In addition, the network outputs are used for unsupervised fluid segmentation.",
        "authors": "Julia Andresen, Jan Ehrhardt, Claus von der Burchard, Ayse Tatli, Johann Roider, Heinz Handels, Hristina Uzunova",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.2 - O.04"
    },
    {
        "number": 219,
        "UID": "O-219",
        "forum": "https://openreview.net/forum?id=RsP618PEFF",
        "Track": "Full Paper",
        "Session": "Oral 2.2 - Geometric Deep Learning & Federated Learning",
        "Final Decision": "Oral",
        "title": "FluidRegNet: Longitudinal registration of retinal OCT images with new pathological fluids",
        "abstract": "Eye diseases such as the chronic central serous chorioretinopathy are characterized by fluid deposits that alter the retina and impair vision. These fluids occur at irregular intervals and may dissolve spontaneously or thanks to treatment. Accurately capturing this behavior within an image registration framework is challenging due to the resulting prominent tissue deformations and missing image correspondences between visits. This paper presents FluidRegNet, a convolutional neural network for the registration of successive optical coherence tomography images of the retina. The correspondence between time points is established by predicting the position of the origin of the fluids by creating a fluid seed in the form of sparse intensity offsets in the moving image and registering the fluid seed to the affected area in the follow-up image. We show that this leads to deformation fields that more accurately reflect the actual dynamics of retinal fluid growth compared to other image registration methods. In addition, the network outputs are used for unsupervised fluid segmentation.",
        "authors": "Julia Andresen, Jan Ehrhardt, Claus von der Burchard, Ayse Tatli, Johann Roider, Heinz Handels, Hristina Uzunova",
        "Time": "Day 2 \u2014 14:30-15:15",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.2 - O.04",
        "Chairs": "Loic Le Folgoc & Tolga Tasdizen"
    },
    {
        "number": 91,
        "UID": "F-91",
        "forum": "https://openreview.net/forum?id=lV1NJ8S55g",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Accelerating physics-informed neural fields for fast CT perfusion analysis in acute ischemic stroke",
        "abstract": "Spatio-temporal perfusion physics-informed neural networks were introduced as a new method (SPPINN) for CT perfusion (CTP) analysis in acute ischemic stroke. SPPINN leverages physics-informed learning and neural fields to perform a robust analysis of noisy CTP data. However, SPPINN faces limitations that hinder its application in practice, namely its implementation as a slice-based (2D) method, lengthy computation times, and the lack of infarct core segmentation. To address these challenges, we introduce a new approach to accelerate physics-informed neural fields for fast, volume-based (3D), CTP analysis including infarct core segmentation: ReSPPINN. To accommodate 3D data while simultaneously reducing computation times, we integrate efficient coordinate encodings. Furthermore, to ensure even faster model convergence, we use a meta-learning strategy. In addition, we also segment the infarct core. We employ acute MRI reference standard infarct core segmentations to evaluate ReSPPINN and we compare the performance with two commercial software packages. We show that meta-learning allows for full-volume perfusion map generation in 1.2 minutes without comprising quality, compared to over 40 minutes required by SPPINN. Moreover, ReSPPINN's infarct core segmentation outperforms commercial software.",
        "authors": "Lucas de Vries, Rudolf Leonardus Mirjam Van Herten, Jan W. Hoving, Ivana Isgum, Bart Emmer, Charles B. Majoie, Henk Marquering, Stratis Gavves",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.2 - O.01"
    },
    {
        "number": 91,
        "UID": "O-91",
        "forum": "https://openreview.net/forum?id=lV1NJ8S55g",
        "Track": "Full Paper",
        "Session": "Oral 2.2 - Geometric Deep Learning & Federated Learning",
        "Final Decision": "Oral",
        "title": "Accelerating physics-informed neural fields for fast CT perfusion analysis in acute ischemic stroke",
        "abstract": "Spatio-temporal perfusion physics-informed neural networks were introduced as a new method (SPPINN) for CT perfusion (CTP) analysis in acute ischemic stroke. SPPINN leverages physics-informed learning and neural fields to perform a robust analysis of noisy CTP data. However, SPPINN faces limitations that hinder its application in practice, namely its implementation as a slice-based (2D) method, lengthy computation times, and the lack of infarct core segmentation. To address these challenges, we introduce a new approach to accelerate physics-informed neural fields for fast, volume-based (3D), CTP analysis including infarct core segmentation: ReSPPINN. To accommodate 3D data while simultaneously reducing computation times, we integrate efficient coordinate encodings. Furthermore, to ensure even faster model convergence, we use a meta-learning strategy. In addition, we also segment the infarct core. We employ acute MRI reference standard infarct core segmentations to evaluate ReSPPINN and we compare the performance with two commercial software packages. We show that meta-learning allows for full-volume perfusion map generation in 1.2 minutes without comprising quality, compared to over 40 minutes required by SPPINN. Moreover, ReSPPINN's infarct core segmentation outperforms commercial software.",
        "authors": "Lucas de Vries, Rudolf Leonardus Mirjam Van Herten, Jan W. Hoving, Ivana Isgum, Bart Emmer, Charles B. Majoie, Henk Marquering, Stratis Gavves",
        "Time": "Day 2 \u2014 14:30-15:15",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.2 - O.01",
        "Chairs": "Loic Le Folgoc & Tolga Tasdizen"
    },
    {
        "number": 127,
        "UID": "S-127",
        "forum": "https://openreview.net/forum?id=yvtAAEQV7b",
        "Track": "Short Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Regional supervised learning of inhibitory control strength from cortical sulci",
        "abstract": "The human cortical brain is folded and is highly variable among individuals. \nWe ultimately want to quantify how cortical folding relates to clinically relevant parameters. Here, using supervised convolutional networks on the human connectome project (HCP) dataset, we learn to predict inhibitory control strength, using all the information from the cortical folds. As we want to focus on the shape of the folds (which are supposed to remain identical throughout adult life), we don't use the full MRI images but the cortical skeletons, which are negative casts of the brain. As we expect putative folding patterns to be local, we scatter the supervised learning over 24 sulcal bilateral regions on the two hemispheres and apply an ensemble method to each region.  We found the strongest significant correlations between inhibitory control and cortical sulci in the frontal marginal, the central sulcal, and the cingulate regions.",
        "authors": "Jo\u00ebl Chavas, Aymeric Gaudin, Denis Rivi\u00e8re, Jean-Francois Mangin",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - S.02"
    },
    {
        "number": 137,
        "UID": "S-137",
        "forum": "https://openreview.net/forum?id=IS1D6APJyz",
        "Track": "Short Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Improving Multi-Center Generalizability of GAN-Based Fat Suppression using Federated Learning",
        "abstract": "Generative Adversarial Network (GAN)-based synthesis of fat suppressed (FS) MRIs from non-FS proton density sequences has the potential to accelerate acquisition of knee MRIs. However, GANs trained on single-site data have poor generalizability to external data. We show that federated learning can improve multi-center generalizability of GANs for synthesizing FS MRIs, while facilitating privacy-preserving multi-institutional collaborations.",
        "authors": "Pranav Kulkarni, Adway Kanhere, Harshita Kukreja, Vivian Zhang, Paul Yi, Vishwa Sanjay Parekh",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - S.08"
    },
    {
        "number": 164,
        "UID": "S-164",
        "forum": "https://openreview.net/forum?id=oGnucxuiAA",
        "Track": "Short Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Expert-annotated datasets for deep-learning based classification methods for morphologic Leukaemia diagnostics",
        "abstract": "In recent years, image analysis and classification methods have become increasingly popular for segmentation and classification tasks in pathology imaging, specifically for the evaluation of tissues sections and single cells.\nHematological cytomorphology, with single cell-classification at its base and lacking some of the complexities of tissue architecture, has been at the forefront of this development. Recent additions of large-scale datasets to the public domain open up new possibilities for developing and benchmarking algorithms for morphologic diagnostics, prognostication and subgrouping.",
        "authors": "Christian Matek, Carsten Marr",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - S.07"
    },
    {
        "number": 168,
        "UID": "S-168",
        "forum": "https://openreview.net/forum?id=JyZKi38kv2",
        "Track": "Short Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Improving Glioma Segmentation in Low-Resolution Domains with Transfer Learning",
        "abstract": "Training accurate tumor segmentation models only using data from the BraTS Sub-Saharan\nAfrica (SSA) Glioma dataset is difficult due to the low quantity and resolution of the images.\nHowever, it is possible to improve model performance through the use of transfer learning\nmethods which leverage insights gained from larger datasets, such as the BraTS23 Adult\nGlioma dataset. Here, we evaluate the performance of various transfer learning approaches\non the task of improving tumor segmentation Dice and Hausdorff Distance (95%) scores on\nthe BraTS SSA dataset. The transfer learning approaches assessed here include: Domain\nAdversarial Neural Networks, Fine Tuning (with and without freezing layer weights), and\ntraining with a combined dataset of low- and high-resolution images.",
        "authors": "Juampablo E Heras Rivera, Harshitha Rebala, Tianyi Ren, Abhishek Sharma, Mehmet Kurt",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - S.04"
    },
    {
        "number": 173,
        "UID": "S-173",
        "forum": "https://openreview.net/forum?id=AIPQBCXesT",
        "Track": "Short Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "MedIMeta: An easy-to-use meta-dataset for medical imaging applications",
        "abstract": "Scarcity of large, diverse, and well-annotated datasets remains a challenge in medical image analysis. Medicalimages vary in format, size, and other parameters and therefore require extensive preprocessing and standardization for usage in machine learning. Addressing these challenges, we introduce the Medical Imaging Meta-Dataset (MedIMeta), a novel multi-domain, multi-task meta-dataset. MedIMeta contains 19 medical imaging datasets spanning 10 different domains and encompassing 54 distinct medical tasks, all of which are standardised to the same format and readily usable in PyTorch or other ML frameworks.",
        "authors": "Stefano Woerner, Arthur Jaques, Christian F. Baumgartner",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - S.01"
    },
    {
        "number": 179,
        "UID": "S-179",
        "forum": "https://openreview.net/forum?id=rzti8Per8e",
        "Track": "Short Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Statistical Evaluation of Competing Models in Brain MRI Analysis",
        "abstract": "Current state-of-the-art models in medical image analysis are often evaluated using small, single train/test split datasets, which may not provide a reliable indication of model improvement. This practice, combined with the increasing complexity and computational cost of newer architectures, exacerbates reproducibility issues, particularly in brain MRI imaging. This paper introduces a robust statistical framework to assess model comparisons in medical imaging. We correct common misapplications of the t-test, which assumes data normality, and promote the use of Two One-Sided Tests for demonstrating non-inferiority in model optimization scenarios. Through this approach, we aim to enhance the rigor of model performance evaluations and address the challenges of dataset size and test validity in medical image analysis.",
        "authors": "Ekaterina Kondrateva, Aleksandr Yugay",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - S.03"
    },
    {
        "number": 22,
        "UID": "S-22",
        "forum": "https://openreview.net/forum?id=nIIXQGPZJc",
        "Track": "Short Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Automatic Facial Landmark Detection for Neurosurgical Mixed Reality Applications in MRI and CT scans using Deep Learning",
        "abstract": "Mixed reality (MxR) has the potential to revolutionize the way neurosurgical interventions are performed. However, the use of MxR in the operating room (OR) introduces new challenges, such as the registration of preoperative images to the patient. This paper presents a deep learning method for automatic detection of facial landmarks in Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) scans, which can be used for image-to-patient registration in MxR applications. The method achieves a mean error of 4.02(\u00b12.65) mm in 3.55(\u00b11.53) seconds on a CPU. Apart from the nasion, no statistically significant differences were found between the performance of the method between the CT and MRI scans.",
        "authors": "Mathijs de Boer, Lambertus W. Bartels, Pierre A.J.T. Robe, Tristan P.C. van Doormaal",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - S.09"
    },
    {
        "number": 56,
        "UID": "S-56",
        "forum": "https://openreview.net/forum?id=jh1KZ4cyWA",
        "Track": "Short Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Automatic real-time prostate detection in transabdominal ultrasound images",
        "abstract": "Prostate cancer is the second most common form of cancer in men, thus easily accessible early diagnostic\ntools are of vital importance. Transabdominal ultrasound is an inexpensive, non-invasive, and accessible\nimaging modality. However, generated images are challenging to interpret and require expert clinicians\nto be assessed. Therefore, we propose a DL model for real-time automatic detection of the prostate\nfor guidance to inexpert operators. Results show that the proposed model has similar performance\nto state-of-the-art models, achieving mean average precision at 0.50 of 0.95 and queries per second of\n993, indicating possible clinical application and possible improvement with more extensive pretraining\nstrategies.",
        "authors": "Tiziano Natali, Mark Wijkhuizen, Liza Kurucz, Matteo Fusaglia, Pim J. van Leeuwen, Theo J.M. Ruers, Behdad Dashtbozorg",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - S.06"
    },
    {
        "number": 69,
        "UID": "S-69",
        "forum": "https://openreview.net/forum?id=5R2cnoAOiR",
        "Track": "Short Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Results of the CROWN challenge on automated assessment of circle of Willis morphology",
        "abstract": "Automated assessment of circle of Willis (CoW) morphology may aid the identification of imaging risk factors associated with intracranial aneurysm (IA) development. However, comparative studies that explore optimal methodological approaches are currently lacking.\nTo systematically compare the performance of automated methods to a clinical reference standard, we initiated a scientific challenge associated with MICCAI 2023. This challenge comprised two tasks: automated classification of CoW anatomical variants and automated\nprediction of CoW artery diameters and bifurcation angles. The challenge dataset comprised 300 TOF-MRA images for training and 300 for testing, all manually annotated. Method were evaluated using balanced accuracy, mean absolute error, and Pearson correlation coefficient metrics. This short paper will present the first results and evaluation of the challenge. The challenge remains open for future submissions, serving as a benchmark for evaluating methods aimed at assessing CoW morphology.",
        "authors": "Iris Vos, Ynte Ruigrok, Hugo Kuijf",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - S.05"
    },
    {
        "number": 103,
        "UID": "F-103",
        "forum": "https://openreview.net/forum?id=Ko7BwrNuzG",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Deep blind arterial input function: signal correction in perfusion cardiac magnetic resonance",
        "abstract": "Objectives: The non-linear relationship between gadolinium concentration and the signal in perfusion cardiac magnetic resonance (CMR) poses a significant challenge for accurate quantification of pharmacokinetic parameters. This phenomenon primarily impacts the arterial input function (AIF), causing it to appear saturated in comparison to the temporal concentration profile. This study aims to leverage a blind deconvolution strategy through a deep-learning approach to address the saturation in the AIF.\n\nMethods: We propose the utilization of a convolutional neural network (CNN) architecture with the saturated AIF and a set of myocardial tissue signals as inputs, generating the corrected AIF as the output. To train the network, a dataset comprising over 3\u00d710^6 simulated AIFs with associated signals from five simulated tissues response for each instance was employed. To assess the effectiveness of the approach, the trained network was evaluated using a dual-saturation sequence to compare the corrected AIF with the unsaturated version. The clinical dataset encompassed scans from 43 patients.\n\nResults: The mean square error (MSE) for the testing subset of the simulated database was 0.69% of the peak. In the in vivo dataset, the coefficient of determination R2 was 0.26 and 0.86 for the saturated and corrected AIF, respectively, in comparison to the unsaturated AIF.\n\nConclusion: The proposed network successfully corrects the acquisition-induced effects on the AIF. Moreover, the extensive simulated database, featuring diverse acquisition parameters, facilitates the robust generalization of the network's application.",
        "authors": "habib rebbah, Magalie Viallon, Pierre Croisille, Timoth\u00e9 Boutelier",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.07"
    },
    {
        "number": 108,
        "UID": "F-108",
        "forum": "https://openreview.net/forum?id=bVunbe4hoV",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "HARP: Unsupervised Histopathology Artifact Restoration",
        "abstract": "Histopathological analysis, vital for medical diagnostics, is often challenged by artifacts in\nsample preparation and imaging, such as staining inconsistencies and physical obstructions.\nAddressing this, our work introduces a novel, fully unsupervised histopathological artifact\nrestoration pipeline (HARP). HARP integrates artifact detection, localization, and restoration\ninto one pipeline. The first step to make artifact restoration applicable is an analysis\nof anomaly detection algorithms. Then, HARP leverages the power of unsupervised segmentation\ntechniques to propose localizations for potential artifacts, for which we select the\nbest localization based on our novel inpainting denoising diffusion model. Finally, HARP\nemploys an inpainting model for artifact restoration while conditioning it on the artifact localizations.\nWe evaluate the artifact detection quality along with the image reconstruction\nquality, surpassing the state-of-the-art artifact restoration. Furthermore, we demonstrate\nthat HARP improves the robustness and reliability of downstream models and show that\npathologists can not tell the difference between clean images and images restored through\nHARP. This demonstrates that HARP significantly improves image quality and diagnostic\nreliability, enhancing histopathological examination accuracy for AI systems.",
        "authors": "Moritz Fuchs, Ssharvien Kumar R Sivakumar, Mirko Sch\u00f6ber, Niklas Woltering, Marie-Lisa Eich, Leonille Schweizer, Anirban Mukhopadhyay",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.08"
    },
    {
        "number": 116,
        "UID": "F-116",
        "forum": "https://openreview.net/forum?id=LLoSHPorlM",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Conditional Generation of 3D Brain Tumor Regions via VQGAN and Temporal-Agnostic Masked Transformer",
        "abstract": "Neuroradiology studies often suffer from lack of sufficient data to properly train deep learning models. Generative Adversarial Networks (GANs) can mitigate this problem by generating synthetic images to augment training datasets. However, GANs sometimes are unstable and struggle to produce high-resolution, realistic, and diverse images. An alternative solution is Diffusion Probabilistic Models, but these models require extensive computational resources. Additionally, most of the existing generation models are designed to generate the entire image volumes, rather than the regions of interest (ROIs) such as the tumor region. Research on brain tumor classification using magnetic resonance imaging (MRIs) has shown that it is easier to classify the ROIs compared to the entire image volumes. To this end, we present a class-conditioned ROI generation framework that combines a conditional vector-quantization GAN and a class-conditioned masked Transformer to generate high-resolution and diverse 3D brain tumor ROIs. We also propose a temporal-agnostic masking strategy to effectively learn relationships between semantic tokens in the latent space. Our experiments demonstrate that the proposed method can generate high-quality 3D MRIs of brain tumor regions for both low- and high-grade glioma (LGG/HGG) in the BraTS 2019 dataset. Using the generated data, our approach demonstrates superior performance compared to several baselines in a downstream task of brain tumor type classification. Our proposed method has the potential to facilitate accurate diagnosis of rare brain tumors using MRI-based machine learning models.",
        "authors": "Meng Zhou, Farzad Khalvati",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.09"
    },
    {
        "number": 133,
        "UID": "F-133",
        "forum": "https://openreview.net/forum?id=Ywu8wb2wuH",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Modeling the acquisition shift between axial and sagittal MRI for diffusion superresolution to enable axial spine segmentation",
        "abstract": "Spine MRIs are usually acquired in highly anisotropic 2D axial or sagittal slices. Vertebra structures are not fully resolved in these images, and multi-image superresolution by aligning scans to pair them is difficult due to partial volume effects and inter-vertebral movement during acquisition. Hence, we propose an unpaired inpainting superresolution algorithm that extrapolates the missing spine structures. We generate synthetic training pairs by multiple degradation functions that model the data shift and acquisition errors between sagittal slices and sagittal views of axial images. Our method employs modeling of the k-space point spread function and the interslice gap. Further, we imitate different MR acquisition challenges like histogram shifts, bias fields, interlace movement artifacts, Gaussian noise, and blur. This enables the training of diffusion-based superresolution models on scaling factors larger than 6$\\times$ without real paired data. The low z-resolution in axial images prevents existing approaches from separating individual vertebrae instances. By applying this superresolution model to the z-dimension, we can generate images that allow a pre-trained segmentation model to distinguish between vertebrae and enable automatic segmentation and processing of axial images. We experimentally benchmark our method and show that diffusion-based superresolution outperforms state-of-the-art super-resolution models.",
        "authors": "Robert Graf, Hendrik M\u00f6ller, Julian McGinnis, Sebastian R\u00fchling, Maren Weihrauch, Matan Atad, Suprosanna Shit, bjoern menze, Mark M\u00fchlau, Johannes C. Paetzold, Daniel Rueckert, Jan Kirschke",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.10"
    },
    {
        "number": 197,
        "UID": "F-197",
        "forum": "https://openreview.net/forum?id=orE18Wdgbj",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Multi-scale Stochastic Generation of Labelled Microscopy Images for Neuron Segmentation",
        "abstract": "We introduce a novel method leveraging conditional generative adversarial networks (cGANs) to generate diverse, high-resolution microscopy images for neuron tracing model training. This approach addresses the challenge of limited annotated data availability, a significant obstacle in automating neuron dendrite tracing. Our technique utilizes a multi-scale cascade process to generate synthetic images from single neuron tractograms, accurately replicating the complex characteristics of real microscopy images, encompassing imaging artifacts and background structures. In experiments, our method generates diverse images that mimic the characteristics of two distinct neuron microscopy datasets, which were successfully used as training data in the segmentation task of real neuron images.",
        "authors": "Meghane Decroocq, Binbin XU, Katherine L Thompson-Peer, Adrian Moore, Henrik Skibbe",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.11"
    },
    {
        "number": 294,
        "UID": "F-294",
        "forum": "https://openreview.net/forum?id=mx63yXRHzY",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Hyperparameter-Free Medical Image Synthesis for Sharing Data and Improving Site-Specific Segmentation",
        "abstract": "Sharing synthetic medical images is a promising alternative to sharing real images that can improve patient privacy and data security. To get good results, existing methods for medical image synthesis must be manually adjusted when they are applied to unseen data. To remove this manual burden, we introduce a Hyperparameter-Free distributed learning method for automatic medical image Synthesis, Sharing, and Segmentation called HyFree-S3. For three diverse segmentation settings (pelvic MRIs, lung X-rays, polyp photos), the use of HyFree-S3 results in improved performance over training only with site-specific data (in the majority of cases). The hyperparameter-free nature of the method should make data synthesis and sharing easier, potentially leading to an increase in the quantity of available data and consequently the quality of the models trained that may ultimately be applied in the clinic. Our code is available at https://github.com/AwesomeLemon/HyFree-S3",
        "authors": "Alexander Chebykin, Peter Bosman, Tanja Alderliesten",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.12"
    },
    {
        "number": 37,
        "UID": "F-37",
        "forum": "https://openreview.net/forum?id=IB12ECdJ3a",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Heterogeneous Medical Data Integration with Multi-Source StyleGAN",
        "abstract": "Conditional deep generative models have emerged as powerful tools for generating realistic images enabling fine-grained control over latent factors. \nIn the medical domain, data scarcity and the need to integrate information from diverse sources present challenges for existing generative models, often resulting in low-quality image generation and poor controllability. \nTo address these two issues, we propose Multi-Source StyleGAN (MSSG). MSSG learns jointly from multiple heterogeneous data sources with different available covariates and can generate new images controlling all covariates together, thereby overcoming both data scarcity and heterogeneity.\nWe validate our method on semi-synthetic data of hand-written digit images with varying morphological features and in controlled multi-source simulations on retinal fundus images and brain magnetic resonance images. Finally, we apply MSSG in a real-world setting of brain MRI from different sources. Our proposed algorithm offers a promising direction for unbiased data generation from disparate sources. For the reproducibility of our experimental results, we provide [detailed code implementation](https://github.com/weslai/msstylegans).",
        "authors": "Wei-Cheng Lai, Matthias Kirchler, Hadya Yassin, Jana Fehr, Alexander Rakowski, Hampus Olsson, Ludger Starke, Jason M. Millward, Sonia Waiczies, Christoph Lippert",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.01"
    },
    {
        "number": 42,
        "UID": "F-42",
        "forum": "https://openreview.net/forum?id=2iAsI3SZ8C",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Deformation-aware GAN for Medical Image Synthesis with Substantially Misaligned Pairs",
        "abstract": "Medical image synthesis generates additional imaging modalities that are costly, invasive or harmful to acquire, which helps to facilitate the clinical workflow. When training pairs are substantially misaligned (e.g., lung MRI-CT pairs with respiratory motion), accurate image synthesis remains a critical challenge. Recent works explored the directional registration module to adjust misalignment in generative adversarial networks (GANs); however, substantial misalignment will lead to 1) suboptimal data mapping caused by correspondence ambiguity, and 2) degraded image fidelity caused by morphology influence on discriminators. To address the challenges, we propose a novel Deformation-aware GAN (DA-GAN) to dynamically correct the misalignment during the image synthesis based on multi-objective inverse consistency. Specifically, in the generative process, three levels of inverse consistency cohesively optimise symmetric registration and image generation for improved correspondence. In the adversarial process, to further improve image fidelity under misalignment, we design deformation-aware discriminators to disentangle the mismatched spatial morphology from the judgement of image fidelity. Experimental results show that DA-GAN achieved superior performance on a public dataset with simulated misalignments and a real-world lung MRI-CT dataset with respiratory motion misalignment. The results indicate the potential for a wide range of medical image synthesis tasks such as radiotherapy planning.",
        "authors": "Bowen Xin, Tony Young, Claire Wainwright, Tamara Blake, Leo Lebrat, Thomas Gaass, Thomas Benkert, Alto Stemmer, David Coman, Jason Dowling",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.02"
    },
    {
        "number": 48,
        "UID": "F-48",
        "forum": "https://openreview.net/forum?id=Lk5e05rXMI",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Anomaly-focused Single Image Super-resolution with Artifact Removal for Chest X-rays using Distribution-aware Diffusion Model",
        "abstract": "Single image super-resolution (SISR) is a crucial task in the field of medical imaging. It transforms low-resolution images into high-resolution counterparts. Performing SISR on chest x-ray images enhances image quality, aiding better diagnosis. However, artifacts may be present in the images. We propose an anomaly-guided SISR process utilizing the denoising mechanism of the diffusion model to iteratively remove noise and restore the original image. We train the model to learn the data distribution, enabling it to eliminate artifacts within the images. Additionally, we ensure reconstruction of the disease regions by prioritizing their reconstruction. Our research experiment over the publicly available dataset and find that the existing SISR methods are unable to learn and remove these artificially added artifacts. On the other hand, our proposed model not only prioritizes superior image reconstruction but also remove the artifacts. Our method is found to outperform the existing methods. The code is publicly available at https://github.com/Datta-IITJ/MIDL_code.git.",
        "authors": "Dattatreyo Roy, Angshuman Paul",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.03"
    },
    {
        "number": 50,
        "UID": "F-50",
        "forum": "https://openreview.net/forum?id=VHfh2J8MQ6",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Resolution and Field of View Invariant Generative Modelling with Latent Diffusion Models",
        "abstract": "Large dataset requirements for deep learning methods can pose a challenge in the medical field, where datasets tend to be relatively small. Synthetic data can provide a suitable solution to this problem, when complemented with real data. However current generative methods normally require all data to be of the same resolution and, ideally, aligned to an atlas. This not only creates more stringent restrictions on the training data but also limits what data can be used for inference. To overcome this our work proposes a latent diffusion model that is able to control sample geometries by varying their resolution, field of view, and orientation. We demonstrate this work on whole body CT data, using a spatial conditioning mechanism. We showcase how our model provides samples as good as an ordinary latent diffusion model trained fully on whole body single resolution data. This is in addition to the benefit of further control over resolution, field of view, orientation, and even the emergent behaviour of super-resolution. We found that our model could create realistic images across the varying tasks showcasing the potential of this application.",
        "authors": "Ashay Patel, Mark S Graham, Vicky Goh, Sebastien Ourselin, M. Jorge Cardoso",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.04"
    },
    {
        "number": 73,
        "UID": "F-73",
        "forum": "https://openreview.net/forum?id=sUbcVVy8IU",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "GazeDiff: A radiologist visual attention guided diffusion model for zero-shot disease classification",
        "abstract": "We present GazeDiff, a novel architecture that leverages radiologists' eye gaze patterns as controls to text-to-image diffusion models for zero-shot classification. Eye-gaze patterns provide important cues during the visual exploration process; existing diffusion-based models do not harness the valuable insights derived from these patterns during image interpretation. GazeDiff utilizes a novel expert visual attention-conditioned diffusion model to generate robust medical images. This model offers more than just image generation capabilities; the density estimates derived from the gaze-guided diffusion model can effectively improve zero-shot classification performance. We show the zero-shot classification efficacy of GazeDiff on four publicly available datasets for two common pulmonary disease types, namely pneumonia, and tuberculosis.",
        "authors": "Moinak Bhattacharya, Prateek Prasanna",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.05"
    },
    {
        "number": 95,
        "UID": "F-95",
        "forum": "https://openreview.net/forum?id=PhU50PjrtQ",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Style Randomization Improves the Robustness of Breast Density Estimation in MR Images",
        "abstract": "Breast density, a crucial risk factor for future breast cancer development, is defined by\nthe ratio of fat to fibro-glandular tissue (FGT) in the breast. Accurate breast and FGT\nsegmentation is essential for robust density estimation. Previous research on FGT segmen-\ntation in MRI has highlighted the significance of training on both images with and without\nfat suppression to enhance network\u2019s robustness. In this study, we propose a novel data\naugmentation technique to further exploit the multi-modal training setup motivated by the\nresearch in style randomization. We demonstrate that the network trained with the pro-\nposed augmentation is resilient to variations in fat content, showcasing improved robustness\ncompared to solely training with multi-modal data. Our method effectively improves FGT\nsegmentation, thereby enhancing the overall reliability of breast density estimation",
        "authors": "Goksenin Yuksel, Koen Eppenhof, Jaap Kroes, Marcel Worring",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - F.06"
    },
    {
        "number": 12,
        "UID": "F-12",
        "forum": "https://openreview.net/forum?id=9pBGVsHdzL",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "IST-editing: Infinite spatial transcriptomic editing in a generated gigapixel mouse pup",
        "abstract": "Advanced spatial transcriptomics (ST) techniques provide comprehensive insights into complex organisms across multiple scales, while simultaneously posing challenges in biomedical image analysis. The spatial co-profiling of biological tissues by gigapixel whole slide images (WSI) and gene expression arrays motivates the development of innovative and efficient algorithmic approaches. Using Generative Adversarial Nets (GAN), we introduce **I**nfinite **S**patial **T**ranscriptomic **e**diting (IST-editing) and establish gene expression-guided editing in a generated gigapixel mouse pup. Trained with patch-wise high-plex gene expression (input) and matched image data (output), IST-editing enables the seamless synthesis of arbitrarily large bioimages at inference, *e.g.*, with a $106496 \\times 53248$ resolution. After feeding edited gene expression values to the trained model, we simulate cell-, tissue- and animal-level morphological transitions in the generated mouse pup. Lastly, we discuss and evaluate editing effects on interpretable morphological features. The code and generated WSIs are publicly accessible via https://github.com/CTPLab/IST-editing.",
        "authors": "Jiqing Wu, Ingrid Berg, Viktor Koelzer",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - O.05"
    },
    {
        "number": 12,
        "UID": "O-12",
        "forum": "https://openreview.net/forum?id=9pBGVsHdzL",
        "Track": "Full Paper",
        "Session": "Oral 2.3 - Synthesis",
        "Final Decision": "Oral",
        "title": "IST-editing: Infinite spatial transcriptomic editing in a generated gigapixel mouse pup",
        "abstract": "Advanced spatial transcriptomics (ST) techniques provide comprehensive insights into complex organisms across multiple scales, while simultaneously posing challenges in biomedical image analysis. The spatial co-profiling of biological tissues by gigapixel whole slide images (WSI) and gene expression arrays motivates the development of innovative and efficient algorithmic approaches. Using Generative Adversarial Nets (GAN), we introduce **I**nfinite **S**patial **T**ranscriptomic **e**diting (IST-editing) and establish gene expression-guided editing in a generated gigapixel mouse pup. Trained with patch-wise high-plex gene expression (input) and matched image data (output), IST-editing enables the seamless synthesis of arbitrarily large bioimages at inference, *e.g.*, with a $106496 \\times 53248$ resolution. After feeding edited gene expression values to the trained model, we simulate cell-, tissue- and animal-level morphological transitions in the generated mouse pup. Lastly, we discuss and evaluate editing effects on interpretable morphological features. The code and generated WSIs are publicly accessible via https://github.com/CTPLab/IST-editing.",
        "authors": "Jiqing Wu, Ingrid Berg, Viktor Koelzer",
        "Time": "Day 2 \u2014 16:30-17:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - O.05",
        "Chairs": "Jonas Richiardi & Tolga Tasdizen"
    },
    {
        "number": 137,
        "UID": "F-137",
        "forum": "https://openreview.net/forum?id=2IwCwawQH4",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Generating Cerebral Vessel Trees of Acute Ischemic Stroke Patients using Conditional Set-Diffusion",
        "abstract": "The advancements in computational modeling and simulations have facilitated the emergence of in-silico clinical trials (ISCTs). ISCTs are valuable in developing and evaluating novel treatments targeting acute ischemic stroke (AIS), a prominent contributor to both mortality and disability rates. However, obtaining large populations of accurate anatomical structures that are required as input to ISCTs is labor-intensive and time-consuming. In this work, we propose and evaluate diffusion-based generative modeling and set transformers to generate a population of synthetic intracranial vessel tree centerlines with associated radii and vessel types. We condition our model on the presence of an occlusion in the middle cerebral artery, a frequently occurring occlusion location in AIS patients. Our analysis of generated synthetic populations shows that our model accurately produces diverse and realistic cerebral vessel trees that represent the geometric characteristics of the real population.",
        "authors": "Thijs P. Kuipers, Praneeta R. Konduri, Henk Marquering, Erik J Bekkers",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - O.06"
    },
    {
        "number": 137,
        "UID": "O-137",
        "forum": "https://openreview.net/forum?id=2IwCwawQH4",
        "Track": "Full Paper",
        "Session": "Oral 2.3 - Synthesis",
        "Final Decision": "Oral",
        "title": "Generating Cerebral Vessel Trees of Acute Ischemic Stroke Patients using Conditional Set-Diffusion",
        "abstract": "The advancements in computational modeling and simulations have facilitated the emergence of in-silico clinical trials (ISCTs). ISCTs are valuable in developing and evaluating novel treatments targeting acute ischemic stroke (AIS), a prominent contributor to both mortality and disability rates. However, obtaining large populations of accurate anatomical structures that are required as input to ISCTs is labor-intensive and time-consuming. In this work, we propose and evaluate diffusion-based generative modeling and set transformers to generate a population of synthetic intracranial vessel tree centerlines with associated radii and vessel types. We condition our model on the presence of an occlusion in the middle cerebral artery, a frequently occurring occlusion location in AIS patients. Our analysis of generated synthetic populations shows that our model accurately produces diverse and realistic cerebral vessel trees that represent the geometric characteristics of the real population.",
        "authors": "Thijs P. Kuipers, Praneeta R. Konduri, Henk Marquering, Erik J Bekkers",
        "Time": "Day 2 \u2014 16:30-17:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - O.06",
        "Chairs": "Jonas Richiardi & Tolga Tasdizen"
    },
    {
        "number": 141,
        "UID": "F-141",
        "forum": "https://openreview.net/forum?id=h1YFt5erJ7",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Evaluating Age-Related Anatomical Consistency in Synthetic Brain MRI against Real-World Alzheimer's Disease Data.",
        "abstract": "This study examines the realism of medical images created with deep generative models, specifically their replication of aging and Alzheimer's disease (AD) related anatomical changes. Previous research focused on developing generative methods with limited attention to image fidelity. We aim to assess the resemblance of brain MRI generated by a StyleGAN3 model with causal controls to neurodegenerative changes. For a benchmark, we conducted a visual Turing test (VTT) to see if radiologists could distinguish between synthetic and real images. Then, we employed a U-Net-based model to segment hallmarks relevant to normal aging and (AD). Finally, we conducted statistical tests for our hypothesis that no significant differences existed between real and synthetic images. (VTT) results showed radiologists struggled to differentiate between image types, highlighting (VTT)'s limitations due to subjectivity and time constraints. We found slight hippocampus distribution differences ($\\textit{P}$ = 5.7e-2) and significant lateral ventricle discrepancies ($\\textit{P}$s $<$ 5.0e-2), indicating higher hippocampus realism and ventricle size inconsistencies. The model more effectively simulated changes in the hippocampus than in the lateral ventricles, where difficulties were encountered with certain subgroups. We conclude that the (VTT) alone is inadequate for a comprehensive quality evaluation, promoting a more objective approach. Future research could adapt our approach to evaluate other generated medical images intended for different downstream tasks. For reproducibility, we provide detailed code implementation$^1$.",
        "authors": "Hadya Yassin, Jana Fehr, Wei-Cheng Lai, Alina Krichevsky, Alexander Rakowski, Christoph Lippert",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - O.07"
    },
    {
        "number": 141,
        "UID": "O-141",
        "forum": "https://openreview.net/forum?id=h1YFt5erJ7",
        "Track": "Full Paper",
        "Session": "Oral 2.3 - Synthesis",
        "Final Decision": "Oral",
        "title": "Evaluating Age-Related Anatomical Consistency in Synthetic Brain MRI against Real-World Alzheimer's Disease Data.",
        "abstract": "This study examines the realism of medical images created with deep generative models, specifically their replication of aging and Alzheimer's disease (AD) related anatomical changes. Previous research focused on developing generative methods with limited attention to image fidelity. We aim to assess the resemblance of brain MRI generated by a StyleGAN3 model with causal controls to neurodegenerative changes. For a benchmark, we conducted a visual Turing test (VTT) to see if radiologists could distinguish between synthetic and real images. Then, we employed a U-Net-based model to segment hallmarks relevant to normal aging and (AD). Finally, we conducted statistical tests for our hypothesis that no significant differences existed between real and synthetic images. (VTT) results showed radiologists struggled to differentiate between image types, highlighting (VTT)'s limitations due to subjectivity and time constraints. We found slight hippocampus distribution differences ($\\textit{P}$ = 5.7e-2) and significant lateral ventricle discrepancies ($\\textit{P}$s $<$ 5.0e-2), indicating higher hippocampus realism and ventricle size inconsistencies. The model more effectively simulated changes in the hippocampus than in the lateral ventricles, where difficulties were encountered with certain subgroups. We conclude that the (VTT) alone is inadequate for a comprehensive quality evaluation, promoting a more objective approach. Future research could adapt our approach to evaluate other generated medical images intended for different downstream tasks. For reproducibility, we provide detailed code implementation$^1$.",
        "authors": "Hadya Yassin, Jana Fehr, Wei-Cheng Lai, Alina Krichevsky, Alexander Rakowski, Christoph Lippert",
        "Time": "Day 2 \u2014 16:30-17:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - O.07",
        "Chairs": "Jonas Richiardi & Tolga Tasdizen"
    },
    {
        "number": 94,
        "UID": "F-94",
        "forum": "https://openreview.net/forum?id=J0zEnfU3Ow",
        "Track": "Full Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Medical diffusion on a budget: Textual Inversion for medical image generation",
        "abstract": "Diffusion models for text-to-image generation, known for their efficiency, accessibility, and quality, have gained popularity. While inference with these systems on consumer-grade GPUs is increasingly feasible, training from scratch requires large captioned datasets and significant computational resources. In medical image generation, limited availability of large, publicly accessible datasets with text reports poses challenges due to legal and ethical concerns. This work shows that adapting pre-trained Stable Diffusion models to medical imaging modalities is achievable by training text embeddings using Textual Inversion.\nIn this study, we experimented with small medical datasets (100 samples each from three modalities) and trained within hours to generate diagnostically accurate images, as judged by an expert radiologist. Experiments with Textual Inversion training and inference parameters reveal the necessity of larger embeddings and more examples in the medical domain. Classification experiments show an increase in diagnostic accuracy (AUC) for detecting prostate cancer on MRI, from 0.78 to 0.80. Further experiments demonstrate embedding flexibility through disease interpolation, combining pathologies, and inpainting for precise disease appearance control. Notably, the trained embeddings are compact (less than 1 MB), enabling easy data sharing with reduced privacy concerns.",
        "authors": "Bram de Wilde, Anindo Saha, Maarten de Rooij, Henkjan Huisman, Geert Litjens",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - O.08"
    },
    {
        "number": 94,
        "UID": "O-94",
        "forum": "https://openreview.net/forum?id=J0zEnfU3Ow",
        "Track": "Full Paper",
        "Session": "Oral 2.3 - Synthesis",
        "Final Decision": "Oral",
        "title": "Medical diffusion on a budget: Textual Inversion for medical image generation",
        "abstract": "Diffusion models for text-to-image generation, known for their efficiency, accessibility, and quality, have gained popularity. While inference with these systems on consumer-grade GPUs is increasingly feasible, training from scratch requires large captioned datasets and significant computational resources. In medical image generation, limited availability of large, publicly accessible datasets with text reports poses challenges due to legal and ethical concerns. This work shows that adapting pre-trained Stable Diffusion models to medical imaging modalities is achievable by training text embeddings using Textual Inversion.\nIn this study, we experimented with small medical datasets (100 samples each from three modalities) and trained within hours to generate diagnostically accurate images, as judged by an expert radiologist. Experiments with Textual Inversion training and inference parameters reveal the necessity of larger embeddings and more examples in the medical domain. Classification experiments show an increase in diagnostic accuracy (AUC) for detecting prostate cancer on MRI, from 0.78 to 0.80. Further experiments demonstrate embedding flexibility through disease interpolation, combining pathologies, and inpainting for precise disease appearance control. Notably, the trained embeddings are compact (less than 1 MB), enabling easy data sharing with reduced privacy concerns.",
        "authors": "Bram de Wilde, Anindo Saha, Maarten de Rooij, Henkjan Huisman, Geert Litjens",
        "Time": "Day 2 \u2014 16:30-17:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - O.08",
        "Chairs": "Jonas Richiardi & Tolga Tasdizen"
    },
    {
        "number": 100,
        "UID": "S-100",
        "forum": "https://openreview.net/forum?id=lmAqqL5wL8",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "MRExtrap: Linear Prediction of Brain Aging in Autoencoder Latent Space of MRI Scans",
        "abstract": "Longitudinal generative modeling of high-resolution 3D Magnetic-Resonance-Imaging (MRI) scans can reveal disease progression patterns in neurological disorders such as Alzheimer's disease. We introduce a novel approach called MRExtrap for simulating aging in brain MRI volumes given previously observed MRIs, by performing linear regression in the latent space of an autoencoder. We show that well-trained convolutional autoencoders can yield latent representations that exhibit linearity with respect to the regional brain volumes when interpolated, decoded, and segmented. We exploit this structure by training a linear progression model in the latent space of the autoencoder to predict trajectories of latent representations based on the age of the subject. On the ADNI dataset, we show that predicted MRIs align closely with held-out longitudinal scans, enabling accurate modeling of age-related structural brain changes.",
        "authors": "Jaivardhan Kapoor, Jakob H. Macke, Christian F. Baumgartner",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.12"
    },
    {
        "number": 11,
        "UID": "S-11",
        "forum": "https://openreview.net/forum?id=qHrvdiwCmX",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "A Generalizable Deep Voxel-Guided Morphometry Algorithm for Change Detection in Multiple Sclerosis",
        "abstract": "We present a deep learning-based approach to generate Voxel-Guided Morphometry (VGM) maps from MRI scans, aiming to improve the detection and monitoring of Multiple Sclerosis (MS) progression. Leveraging a 3D U-Net architecture with attention mechanisms and optimized by histogram matching, our model excels in processing three diverse datasets. It demonstrates enhanced accuracy in identifying MS-related changes, outperforming the reference method in mean absolute error by an average of 0.4%. Additionally, visual analysis confirmed our method yields more precise and stable VGM maps across all datasets, compared to the reference. This work underscores the potential of deep learning in MS progression and treatment assessment.",
        "authors": "Anish Raj, Achim Gass, Philipp Eisele, Andreas Dabringhaus, Matthias Kraemer, Frank G. Zoellner",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.03"
    },
    {
        "number": 112,
        "UID": "S-112",
        "forum": "https://openreview.net/forum?id=QNMaLE9plo",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Zero-Shot Simultaneous Multislice Reconstruction Based on Pretrained Generative Image Priors",
        "abstract": "In this study, we combine the readout-concatenation framework with generative image pri\u0002ors to achieve simultaneous multislice imaging (SMS) reconstruction. The results show that generative image priors have better generalization than supervised deep learning methods\nsuch as VarNet, and it can be processed flexibly handle arbitrary slice aliasing patterns with in-plane acceleration.",
        "authors": "Kexin Yang, Shoujin Huang, Guanxiong Luo, Lifeng Mei, Jingyu Li",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.15"
    },
    {
        "number": 115,
        "UID": "S-115",
        "forum": "https://openreview.net/forum?id=ldvtPOCS0i",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Recent Updates of the M4Raw Dataset and Applications in Evaluating MRI Denoising Methods",
        "abstract": "This paper presents the new multi-channel k-space dataset M4Raw acquired using low-field MRI. The M4Raw dataset comprises brain data from 183 subjects, each with 18 axial slices and three contrasts: T1-weighted (T1w), T2-weighted (T2w), and fluid attenuated inversion recovery (FLAIR). Additionally, the paper provides a description of the recently released test subset, as well as various denoising methods applied to the M4Raw dataset, demonstrating its potential applications in image denoising. Multiple deep learning methods trained on the M4Raw dataset, including traditional denoising networks and those using transformer modules, have been employed, achieving high-quality denoising of low-field MRI images. The M4Raw dataset not only facilitates the development of data-driven methods for low-field MRI denoising but also serves as a benchmark dataset for comparing different methods.",
        "authors": "Yi Li, Mengye Lyu, Guanxiong Luo, Jingwei Guan, Jingyu Li",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.22"
    },
    {
        "number": 143,
        "UID": "S-143",
        "forum": "https://openreview.net/forum?id=m9pHODd2HS",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Anatomy-compliant medical image synthesis by latent diffusion models",
        "abstract": "Data scarcity presents a significant challenge for achieving optimal model performance in medical imaging, due to the limited availability of high-quality data. One potential solution to address this issue is to synthesize medical images using powerful generative models with conditioning prior. However, obtaining full anatomical annotations of all organs for anatomical conditioning is impractical, resulting in synthetic images with incoherent or hallucinated anatomy. In this paper, we propose an innovative medical image generation method based on state-of-the-art latent diffusion models (LDM). To tackle the anatomy compliance challenge, we leverage both the anatomical mask, which is specific to the organ of interest, and the edge information, which is general and easy to compute in the full field of view (FOV), as dual conditioning. Our method does not require extra annotations to achieve anatomy compliance. Our method was evaluated on the ACDC dataset and compared with GAN baselines. Results demonstrate that incorporating edge-based conditioning strongly complements image semantics, leading to high-quality, anatomy-compliant medical image generation.",
        "authors": "Nuno Miguel Ferreira Capit\u00e3o, Yidong Zhao, Yi Zhang, Nathan Geerts, Jo\u00e3o Viana Lopes, Qian Tao",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.13"
    },
    {
        "number": 145,
        "UID": "S-145",
        "forum": "https://openreview.net/forum?id=g9T6yLuMJR",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Semi-supervised Active Learning for Left Ventricle Segmentation in Echocardiography",
        "abstract": "Training deep learning models requires large labelled datasets, which are expensive and scarce in medical imaging. This study investigates semi-supervised active learning for left ventricle segmentation in echocardiography, aiming to reduce the need for extensive manual expert annotations. A novel technique for identifying reliable pseudo-labels is proposed. Results show a significant reduction in annotation efforts by up to 93%, achieving 99% of the maximum accuracy using only 7% of labelled data. The study contributes to efficient annotation strategies in medical image segmentation.",
        "authors": "Eman I Alajrami, Nasim DadashiSerej, Jevgeni Jevsikov, Patricia Fernandes, Abas Abdi, Isreal Ufumaka, Darrel P Francis, Massoud Zolgharni",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.08"
    },
    {
        "number": 146,
        "UID": "S-146",
        "forum": "https://openreview.net/forum?id=OHybm7vB2v",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Simulation of Magnetic Resonance guided Laser Interstitial Thermal Therapy Temperature Maps through Time-series based Deep Learning Methods: Usage of ConvLSTM",
        "abstract": "Magnetic resonance-guided laser interstitial thermal therapy (MRgLITT) is a minimally invasive therapy that leverages thermal ablation to treat drug-resistant focal epilepsy. Patient-specific heat sinks, such as blood vessels, complicate the planning of MRgLITT as it creates patient-level variability in how heat from the laser propagates, thus potentially undermining treatment efficacy. To simulate the MRgLITT temperature maps, we developed a deep learning framework which can predict the resulting spatio-temporal temperature maps of the monitoring system. We used a convolutional long short-term memory (ConvLSTM) network and evaluated the outcome using both quantitative vs. impact evaluation metrics. In impact evaluation, we binarized temperature images and pixels exceeding a temperature of 39$^{\\circ}$C  are identified as potentially indicating cell death. We then use a dice score and sensitivity metrics to evaluate the overlap between predicted and ground truth thermal dose margin. We demonstrate strong performance of our ConvLSTM framework, with a structural similarity index metric of 0.88, dice score of 0.85 and sensitivity of 0.77 which shows that predicted heat propagation was highly similar to the ground truth. Our findings can be used by neurosurgeons to improve the delivery of MRgLITT.",
        "authors": "Saba Sadatamin, Paola Driza, Gemma Postill, Steven Robbins, Richard Tyc, Rahul Krishnan, Lueder Alexander Kahrs, Adam C. Waspe, James M. Drake",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.17"
    },
    {
        "number": 148,
        "UID": "S-148",
        "forum": "https://openreview.net/forum?id=4rKfNtCwmh",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Can We Encode Intra- and Inter-Variability with Log Jacobian Maps Derived from Brain Morphological Deformations Using Pediatric MRI Scans?",
        "abstract": "Understanding individual variabilities in brain development is crucial for unraveling typical and atypical neurodevelopmental patterns. We propose a novel method using 3D CNN to characterize intra- and inter-individual variability based on log Jacobian maps derived from deformation fields. Inter pairs are chosen to match the distribution of intra pairs based on initial age, age interval (ia\\_r experiment) and also sex per pair (ias\\_r experiment). Training our model on log Jacobian maps, we explore two scenarios: one with overlaps between train and test sets derived from the same subjects, and the other with no overlaps, using 10-fold cross-validation. While both approaches achieved commendable results, the no overlap scenario showed slightly higher accuracy and F1 score. This research contributes to modeling neurodevelopmental trajectories for future deviation prediction.",
        "authors": "Andjela Dimitrijevic, Fanny D\u00e9geilh, Benjamin De Leener",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.05"
    },
    {
        "number": 16,
        "UID": "S-16",
        "forum": "https://openreview.net/forum?id=m7wYKrUjzV",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Model-based Cleaning of the QUILT-1M Pathology Dataset for Text-Conditional Image Synthesis",
        "abstract": "The QUILT-1M dataset is the first openly available dataset containing images harvested from various online sources. While it provides a huge data variety, the image quality and composition is highly heterogeneous, impacting its utility for text-conditional image synthesis. We propose an automatic pipeline that provides predictions of the most common impurities within the images, e.g., visibility of narrators, desktop environment and pathology software, or text within the image. Additionally, we propose to use semantic alignment filtering of the image-text pairs. Our findings demonstrate that by rigorously filtering the dataset, there is a substantial enhancement of image fidelity in text-to-image tasks.",
        "authors": "Marc Aubreville, Jonathan Ganz, Jonas Ammeling, Christopher Kaltenecker, Christof Bertram",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.10"
    },
    {
        "number": 166,
        "UID": "S-166",
        "forum": "https://openreview.net/forum?id=fZoD19wN20",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "DyABD: A Dataset and Technique for Synthetically Generating Dynamic Abdominal MRIs with Dual Class and Anatomically Conditioned Diffusion Models",
        "abstract": "An abdominal hernia is a protrusion of intestine or tissue in the abdominal wall and is known to cause debilitating pain. The recurrence rate of abdominal hernia varies from 30\\% to 80\\%, meaning it is paramount to improve our understanding of the mechanical functionality and physiology of the abdominal wall. This work proposes DyABD, a dataset of dynamic abdominal MRIs (2D+t) of hernia patients and a 3D dual class and anatomically conditioned Denoising Diffusion Probabilistic Model (DDPM) that can perform the unique task of synthesising hernia patients performing any of three exercises, breathing, coughing or a Valsalva maneuver, whilst also taking into account whether the patient is pre or post corrective abdominal surgery. DyABD requires a subject prior as input which consists of the first 2D slice of the dynamic MRI sequence and the associated abdominal muscle masks of the first 2D slice which ensures anatomical correctness is preserved during synthesis. This work is based on 121 dynamic MRI volumes which will be made available for sharing as part of the complete dataset of approximately $300$ volumes. The preliminary results of DyABD demonstrate its ability to model the mechanical functionality of the abdominal wall. Examples of generated volumes are made available at  https://github.com/niamhbelton/DyABD/.",
        "authors": "Niamh Belton, Victoria Joppin, Aonghus Lawlor, Kathleen M Curran, Catherine Masson, Thierry Bege, David Bendahan",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.18"
    },
    {
        "number": 21,
        "UID": "S-21",
        "forum": "https://openreview.net/forum?id=eJxNEEsjAb",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Anatomy-guided Latent Diffusion Model for Fine-grained Medical Image Synthetic Augmentation",
        "abstract": "Medical data typically requires expert annotation to produce a reliable quantitative organ analysis, which can be costly and time-consuming. Recently, several deep learning-based synthetic augmentations have been proposed to address the limitations. However, previous success of generative synthetic augmentation methods cannot be guaranteed without additional fine-tuning. To mitigate the dependency on this issue, we propose an anatomy-guided latent diffusion model, which can perform anatomical synthesis in a selectively latent blending manner. We evaluate the proposed approach using a mandibular canal segmentation dataset on panoramic dental radiographs. The segmentation performance was improved by a Dice similarity coefficient of 16.6\\% with our proposed synthetic augmentation.",
        "authors": "Sang-Heon Lim, Su Yang, Jiyong Han, SuJeong Kim, DaEl Kim, Yu-Ri Kim, Jun-Min Kim, Jo-Eun Kim, Won-Jin Yi",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.20"
    },
    {
        "number": 28,
        "UID": "S-28",
        "forum": "https://openreview.net/forum?id=8ONihf0Fh9",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Torch$T_1$: GPU-accelerated cardiac $T_1$ mapping with deep learning framework",
        "abstract": "Quantitative cardiac $T_1$ mapping by MRI is an essential non-invasive diagnostic tool for cardiomyopathies. Traditionally, deriving the quantitative $T_1$ maps of myocardial tissue involves solving non-linear parametric fitting problems per image voxel, which is slow with sequential CPU computation and requires analytical derivation of the Jacobian matrix per signal model. In this paper, we introduce a new paradigm of parametric fitting, termed ``Torch$T_1$\", which leverages the powerful parallelization of modern GPUs and well-established functionalities of auto-differentiation in the deep learning framework of PyTorch. Torch$T_1$ strictly adheres to the signal model and does not require any training. Our method was evaluated on a $T_1$ mapping dataset with both pre-contrast and post-contrast sequences, and benchmarked by conventional CPU-based fitting and recent end-to-end physics-informed neural network (PINN) mapping. Torch$T_1$ showed more accurate and reliable mapping quality compared with the pretrained PINN, with a 13-fold acceleration compared with the CPU baseline.",
        "authors": "Yi Zhang, Yidong Zhao, Yifeng Shao, Nuno Miguel Ferreira Capit\u00e3o, Fleur van den Bogert, Qian Tao",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.07"
    },
    {
        "number": 3,
        "UID": "S-3",
        "forum": "https://openreview.net/forum?id=Hgziz4Swpt",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico Data Generation",
        "abstract": "The creation of in-silico datasets can expand the utility of existing annotations to new domains with different staining patterns in computational pathology. As such, it has the potential to significantly lower the cost associated with building large and pixel precise datasets needed to train supervised deep learning models. We propose a novel approach for the generation of in-silico immunohistochemistry (IHC) images by disentangling morphology specific IHC stains into separate image channels in immunofluorescence (IF) images. The proposed approach qualitatively and quantitatively outperforms baseline methods as proven by training nucleus segmentation models on the created in-silico datasets.",
        "authors": "Dominik Winter, Nicolas Triltsch, Philipp Plewa, Marco Rosati, Thomas Padel, Ross Hill, Markus Schick, Nicolas Brieu",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.09"
    },
    {
        "number": 30,
        "UID": "S-30",
        "forum": "https://openreview.net/forum?id=mT6bRCpBbe",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Cortical Surface Diffusion Generative Models",
        "abstract": "Cortical surface analysis is gaining prominence as a sensitive tool for studying complex neuropsychiatric disorders. However, patterns of cortical organization are complex and highly variable across individuals, challenging classical approaches for analysis that rely on diffeomorphic image registration. This leads to an urgent need for better methods to model brain development and diverse variability inherent across different individuals.  Traditional vision diffusion models have shown effectiveness in generating high-resolution and realistic natural images, which makes them particularly suited to addressing cortical surface problems, where the features of interest are subtle and highly variable across individuals. In this work, we first proposed a novel diffusion model for the generation of cortical surface metrics, using modified surface vision transformers as the principal architecture. We validate our method in the developing Human Connectome Project (dHCP) with results suggesting that our model demonstrates excellent performance in capturing the intricate details of evolving cortical surfaces - generating high-quality realistic samples of cortical surfaces conditioned on postmenstrual age (PMA) at scan.",
        "authors": "Zhenshan Xie, Simon Dahan, Logan Zane John Williams, M. Jorge Cardoso, Emma C. Robinson",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.16"
    },
    {
        "number": 50,
        "UID": "S-50",
        "forum": "https://openreview.net/forum?id=6EaycEaPoh",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "FootCapture: Towards an AR-based System for 3D Foot Object Acquisition through Photogrammetry",
        "abstract": "The acquisition of accurate 3D models of feet is crucial in fields such as chronic foot wound monitoring, prosthetics design, and orthopedic surgery. However, obtaining precise models of patients' feet typically relies on manual measurements, which is both costly and prone to error. Addressing this need, we introduce FootCapture, a mobile application designed to facilitate the acquisition of precise photographic measurements. Our solution employs augmented reality to intuitively guide untrained users to capture comprehensive photographic data from the correct positions and angles, suitable to create a high-fidelity  3D model of the patient's foot using photogrammetry.\nTo validate our application's utility, we compared FootCapture with Apple's Guided Capture application in a user study with n=7 participants. The results showed FootCapture\u2019s intuitive use and high robustness marking it as a tool worth considering for medical personnel.",
        "authors": "Valentin Khan-Blouki, Franziska Seiz, Nicolas Walter, Alexander Jaus, Zdravko Marinov, Gijs Luijten, Jan Egger, Constantin Marc Seibold, Dirk Solte, Jens Kleesiek, Rainer Stiefelhagen",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.01"
    },
    {
        "number": 60,
        "UID": "S-60",
        "forum": "https://openreview.net/forum?id=orIWvvBK2v",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "cHeartFlow: Synthesizing cardiac MR images from sketches",
        "abstract": "Medical image synthesis is a highly promising approach to generate and augment medical data, which suffers from high acquisition costs and stringent privacy restrictions. However, current generation methods typically require detailed anatomical annotations and are limited in generating high-quality, anatomy-compliant images. To overcome the limitations, we present contrastive HeartFlow (cHeartFlow), a novel generative framework to synthesize cardiac magnetic resonance (CMR) images from simple sketches by training on contrastive pairs of images and sketches. cHeartFlow supports one-step synthesis and allows multi-step synthesis for a flexible trade-off between faithfulness and realism. We illustrate the effectiveness and generalizability of cHeartFlow through our experiments on different input sketches, compared with GAN-based and diffusion-based baselines.",
        "authors": "Xinrui Zu, Qian Tao",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.21"
    },
    {
        "number": 71,
        "UID": "S-71",
        "forum": "https://openreview.net/forum?id=PGtCMchmdy",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "REMIX: Resolution Enhancement through Mixture of Experts",
        "abstract": "We propose a novel method to enhance the resolution of magnetic resonance images (MRI) using deep learning. Our approach is based on realistic MRI data degradation using real affine registrations. We demonstrate the efficacy of a Mixture of Experts approach in handling diverse input resolutions commonly present in clinical settings. This is done by training seven networks, each one for a specific native volume resolution, allowing more effective handling of low-resolution images when compared to a unique model.",
        "authors": "Sergio Morell-Ortega, Marina Ruiz Perez, Pierrick Coupe, Jose V Manjon",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.14"
    },
    {
        "number": 77,
        "UID": "S-77",
        "forum": "https://openreview.net/forum?id=B1ywjzVoNa",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Differential contrast enhancement using conditional deep learning for Gadolinium dose reduction in brain MRI",
        "abstract": "In this work, we propose a novel deep learning (DL)-based method to reduce the dose of Gadolinium-based contrast agents administered in brain MRI examinations. In contrast to recent DL approaches, we explicitly focus on accurately predicting contrast enhancement signals and synthesizing realistic images, leveraging contrast signals from subtraction images of pre- and post-contrast T1-weighted image pairs. By training our model to only extract and enhance contrast signals, and by conditioning its layers on relevant physical parameters, we demonstrate its effectiveness across diverse datasets, including data at different dose levels from various scanners, field strengths, and contrast agents.",
        "authors": "Thomas Pinetz, Erich Kobler, Robert Haase, Julian A. Luetkens, Alexander Radbruch, Katerina Deike-Hofmann, Alexander Effland",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.04"
    },
    {
        "number": 78,
        "UID": "S-78",
        "forum": "https://openreview.net/forum?id=c6fyfv45un",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Automatic Myocardium Segmentation in Arterial Spin Labeling Perfusion MRI Using Uncertainty-Aware Mask R-CNN",
        "abstract": "Coronary artery disease (CAD) is a leading cause of cardiovascular morbidity and mortality worldwide. Assessing myocardial perfusion is  important to detect potential areas of ischemia in patients with suspected CAD. Arterial spin labeling (ASL) allows non-invasive quantification of myocardial perfusion using arterial blood as endogenous tracer. Segmentation of the left ventricular myocardium is critical in the post-processing for ASL images, but it is challenging due to low signal-to-noise ratio (SNR). This study introduces an automatic myocardium segmentation pipeline including uncertainty awareness, employing Mask R-CNN with dropout layers to capture model uncertainty. Our dataset consists of flow-sensitive alternating inversion recovery (FAIR) ASL images from 16 patients with suspected CAD. Our approach achieves robust segmentation results, with similarity coefficient of 75% and 0.3% misclassification rate. We obtain an 80% correlation with real perfusion values.",
        "authors": "Anne Oyarzun-Dome\u00f1o, Ver\u00f3nica Aramend\u00eda-Vidaurreta, Gorka Bastarrika, Ana Ezponda, Mar\u00eda A. Fern\u00e1ndez-Seara, Arantxa Villanueva",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.02"
    },
    {
        "number": 89,
        "UID": "S-89",
        "forum": "https://openreview.net/forum?id=dEm1rfuGbO",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "From Normal to Abnormal: Transforming Medical Images with Diffusion Models for Dataset Balancing",
        "abstract": "Digital colposcopy relies on the accurate identification of high-grade lesions in cervical images. This study explores the use of diffusion models to address class imbalance, a common challenge in medical datasets. We propose a method that synthetically generates high-grade lesion features within normal cervical images. This method was evaluated on datasets from Berlin and Cambodia, the latter having a significant scarcity of high-grade lesions. Our approach successfully balanced the dataset and improved diagnostic performance by 5\\%.",
        "authors": "Martin Paulikat, Lennart Nausch\u00fctte, Martin Simon Kalteis, Witali Aswolinskiy, Achim Schneider, Hermann Bussmann, Christian Aichm\u00fcller, Magnus von Knebel Doeberitz",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.19"
    },
    {
        "number": 9,
        "UID": "S-9",
        "forum": "https://openreview.net/forum?id=zCTqI36Xfy",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Physics in the Loop: Integrating Biomechanics-Derived Training Data into a Neural Ordinary Differential Equation-Based Deformable Registration Framework",
        "abstract": "Image registration of moving heart valves has been fundamentally challenging due to large leaflet deformations occurring over a short period of time with limited temporal image resolution. In this work, we propose integrating mechanics-derived data augmentation into deep learning-based registration frameworks to enhance the accuracy of the registration of heart valve motion. A finite element (FE) analysis is employed to generate additional physically realistic image frames of heart valves, which are then integrated with a neural ordinary differential equation-based deformable registration method to facilitate the registration process. We observe that augmenting the cardiac image sequence with FE-simulated frames better preserves the dynamic anatomy of the heart valves and outperforms traditional registration-based methods alone.",
        "authors": "Wensi Wu, Yifan Wu, Analise M. Sulentic, James Gee, Alison Marie Pouch, Matthew A. Jolley",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.06"
    },
    {
        "number": 90,
        "UID": "S-90",
        "forum": "https://openreview.net/forum?id=9FNmY4nUzX",
        "Track": "Short Paper",
        "Session": "Poster 2.2",
        "Final Decision": "Poster",
        "title": "Classification of multiparametric correlation MRI signals using deep neural networks",
        "abstract": "Correlation MRI is a promising microstructure imaging technique, but its reconstruction remains highly ill-conditioned. We propose to first classify correlation signals, and achieve high accuracy for classification into single- vs. multi-component. Further we establish a solid baseline for predicting the exact number of sub-compartments.",
        "authors": "Sebastian Endt, Tobias Lachermeier, Marion Menzel",
        "Time": "Day 2 \u2014 15:30-16:30",
        "Poster time": "Day 2 \u2014 15:30-16:30",
        "Poster ID": "Poster 2.2 - S.11"
    },
    {
        "number": 172,
        "UID": "O-172",
        "forum": "https://openreview.net/forum?id=9G7ZEYHLVJ",
        "Track": "Full Paper",
        "Session": "Oral 3.1 - Explainable AI & Uncertainty",
        "Final Decision": "Oral",
        "title": "Spatio-Temporal Encoding of Brain Dynamics with Surface Masked Autoencoders",
        "abstract": "The development of robust and generalisable models for encoding the spatio-temporal dynamics of human brain activity is crucial for advancing neuroscientific discoveries. However, significant individual variation in the organisation of the human cerebral cortex makes it difficult to identify population-level trends in these signals. Recently, Surface Vision Transformers (SiTs) have emerged as a promising approach for modelling cortical signals, yet they face some limitations in low-data scenarios due to the lack of inductive biases in their architecture. To address these challenges, this paper proposes the surface Masked AutoEncoder (sMAE) and video surface Masked AutoEncoder (vsMAE) - for multivariate and spatio-temporal pre-training of cortical signals over regular icosahedral grids. These models are trained to reconstruct cortical feature maps from masked versions of the input by learning strong latent representations of cortical structure and function. Such representations translate into better modelling of individual phenotypes and enhanced performance in downstream tasks. The proposed approach was evaluated on cortical phenotype regression using data from the young adult Human Connectome Project (HCP) and developing HCP (dHCP). Results show that (v)sMAE pre-trained models improve phenotyping prediction performance on multiple tasks by $\\ge 26\\%$, and offer faster convergence relative to models trained from scratch.  Finally, we show that pre-training vision transformers on large datasets, such as the UK Biobank (UKB), supports transfer learning to low-data regimes. Our code and pre-trained models are publicly available at https://github.com/metrics-lab/surface-masked-autoencoders.",
        "authors": "Simon Dahan, Logan Zane John Williams, Yourong Guo, Daniel Rueckert, Emma Claire Robinson",
        "Time": "Day 3 \u2014 9:00-10:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.01",
        "Chairs": "Benjamin Billot & Jonas Richiardi"
    },
    {
        "number": 172,
        "UID": "F-172",
        "forum": "https://openreview.net/forum?id=9G7ZEYHLVJ",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Spatio-Temporal Encoding of Brain Dynamics with Surface Masked Autoencoders",
        "abstract": "The development of robust and generalisable models for encoding the spatio-temporal dynamics of human brain activity is crucial for advancing neuroscientific discoveries. However, significant individual variation in the organisation of the human cerebral cortex makes it difficult to identify population-level trends in these signals. Recently, Surface Vision Transformers (SiTs) have emerged as a promising approach for modelling cortical signals, yet they face some limitations in low-data scenarios due to the lack of inductive biases in their architecture. To address these challenges, this paper proposes the surface Masked AutoEncoder (sMAE) and video surface Masked AutoEncoder (vsMAE) - for multivariate and spatio-temporal pre-training of cortical signals over regular icosahedral grids. These models are trained to reconstruct cortical feature maps from masked versions of the input by learning strong latent representations of cortical structure and function. Such representations translate into better modelling of individual phenotypes and enhanced performance in downstream tasks. The proposed approach was evaluated on cortical phenotype regression using data from the young adult Human Connectome Project (HCP) and developing HCP (dHCP). Results show that (v)sMAE pre-trained models improve phenotyping prediction performance on multiple tasks by $\\ge 26\\%$, and offer faster convergence relative to models trained from scratch.  Finally, we show that pre-training vision transformers on large datasets, such as the UK Biobank (UKB), supports transfer learning to low-data regimes. Our code and pre-trained models are publicly available at https://github.com/metrics-lab/surface-masked-autoencoders.",
        "authors": "Simon Dahan, Logan Zane John Williams, Yourong Guo, Daniel Rueckert, Emma Claire Robinson",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.01"
    },
    {
        "number": 173,
        "UID": "O-173",
        "forum": "https://openreview.net/forum?id=LpUNSwHp0O",
        "Track": "Full Paper",
        "Session": "Oral 3.1 - Explainable AI & Uncertainty",
        "Final Decision": "Oral",
        "title": "Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations",
        "abstract": "The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce undetectable underdiagnosis bias in DL models. Our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups show that adversarial bias attacks demonstrate high-selectivity for bias in the targeted group by degrading group model performance without impacting overall model performance. Furthermore, our results indicate that adversarial bias attacks result in biased DL models that propagate prediction bias even when evaluated with external datasets.",
        "authors": "Pranav Kulkarni, Andrew Chan, Nithya Navarathna, Skylar Chan, Paul Yi, Vishwa Sanjay Parekh",
        "Time": "Day 3 \u2014 9:00-10:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.03",
        "Chairs": "Benjamin Billot & Jonas Richiardi"
    },
    {
        "number": 173,
        "UID": "F-173",
        "forum": "https://openreview.net/forum?id=LpUNSwHp0O",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations",
        "abstract": "The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce undetectable underdiagnosis bias in DL models. Our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups show that adversarial bias attacks demonstrate high-selectivity for bias in the targeted group by degrading group model performance without impacting overall model performance. Furthermore, our results indicate that adversarial bias attacks result in biased DL models that propagate prediction bias even when evaluated with external datasets.",
        "authors": "Pranav Kulkarni, Andrew Chan, Nithya Navarathna, Skylar Chan, Paul Yi, Vishwa Sanjay Parekh",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.03"
    },
    {
        "number": 259,
        "UID": "O-259",
        "forum": "https://openreview.net/forum?id=Hb0U9lED4N",
        "Track": "Full Paper",
        "Session": "Oral 3.1 - Explainable AI & Uncertainty",
        "Final Decision": "Oral",
        "title": "An unexpected confounder: how brain shape can be used to classify MRI scans ?",
        "abstract": "Although deep learning has proved its effectiveness in the analysis of medical images, its great ability to extract complex features makes it susceptible to base its decision on spurious confounders present in the images. However, especially for medical applications, network decisions must be based on relevant elements. Numerous confounding factors have been identified in the case of brain scans such as gender, age, MRI sites or scanners, etc. Nevertheless, although skull stripping is a classic preprocessing step for brain scans, brain shape has never been considered as a possible confounder. In this work, we show that brain shape is used in the classification of brain MRI scans from different databases, even when it should not be considered as a clinically relevant factor. To this purpose, we introduce a rigorous two steps method to assess whether a factor is a confounder or not, and we apply it to identify the brain shape as a confounding variable in brain images classification. Lastly, we propose to use a deformable registration in the data preprocessing pipeline to align the brain contours of the images in the datasets, whereas standard pipelines often do nothing more than affine registration. Including this deformable registration step makes the classification free from the brain shape confounding effect.",
        "authors": "Valentine Wargnier Dauchelle, Thomas Grenier, Micha\u00ebl Sdika",
        "Time": "Day 3 \u2014 9:00-10:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.02",
        "Chairs": "Benjamin Billot & Jonas Richiardi"
    },
    {
        "number": 259,
        "UID": "F-259",
        "forum": "https://openreview.net/forum?id=Hb0U9lED4N",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "An unexpected confounder: how brain shape can be used to classify MRI scans ?",
        "abstract": "Although deep learning has proved its effectiveness in the analysis of medical images, its great ability to extract complex features makes it susceptible to base its decision on spurious confounders present in the images. However, especially for medical applications, network decisions must be based on relevant elements. Numerous confounding factors have been identified in the case of brain scans such as gender, age, MRI sites or scanners, etc. Nevertheless, although skull stripping is a classic preprocessing step for brain scans, brain shape has never been considered as a possible confounder. In this work, we show that brain shape is used in the classification of brain MRI scans from different databases, even when it should not be considered as a clinically relevant factor. To this purpose, we introduce a rigorous two steps method to assess whether a factor is a confounder or not, and we apply it to identify the brain shape as a confounding variable in brain images classification. Lastly, we propose to use a deformable registration in the data preprocessing pipeline to align the brain contours of the images in the datasets, whereas standard pipelines often do nothing more than affine registration. Including this deformable registration step makes the classification free from the brain shape confounding effect.",
        "authors": "Valentine Wargnier Dauchelle, Thomas Grenier, Micha\u00ebl Sdika",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.02"
    },
    {
        "number": 296,
        "UID": "O-296",
        "forum": "https://openreview.net/forum?id=M6CfJ5H7XH",
        "Track": "Full Paper",
        "Session": "Oral 3.1 - Explainable AI & Uncertainty",
        "Final Decision": "Oral",
        "title": "DeCoDEx: Confounder Detector Guidance for Improved Diffusion-based Counterfactual Explanations",
        "abstract": "Deep learning classifiers are prone to latching onto dominant confounders present in a dataset rather than on the causal markers associated with the target class, leading to poor generalization and biased predictions. Although explainability via counterfactual image generation has been successful at exposing the problem, bias mitigation strategies that permit accurate explainability in the presence of dominant and diverse artifacts remain unsolved. In this work, we propose the DeCoDEx framework and show how an external, pre-trained binary artifact detector can be leveraged during inference to guide a diffusion-based counterfactual image generator towards accurate explainability.  Experiments on the CheXpert dataset, using both synthetic artifacts and real visual artifacts (support devices), show that the proposed method successfully synthesizes the counterfactual images that change the causal pathology markers associated with Pleural Effusion while preserving or ignoring the visual artifacts. Augmentation of ERM and Group-DRO classifiers with the DeCoDEx generated images substantially improves the results across underrepresented groups that are out of distribution for each class. The code is made publicly available at https://github.com/NimaFathi/DeCoDEx.",
        "authors": "Nima Fathi, Amar Kumar, Brennan Nichyporuk, Mohammad Havaei, Tal Arbel",
        "Time": "Day 3 \u2014 9:00-10:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.04",
        "Chairs": "Benjamin Billot & Jonas Richiardi"
    },
    {
        "number": 296,
        "UID": "F-296",
        "forum": "https://openreview.net/forum?id=M6CfJ5H7XH",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "DeCoDEx: Confounder Detector Guidance for Improved Diffusion-based Counterfactual Explanations",
        "abstract": "Deep learning classifiers are prone to latching onto dominant confounders present in a dataset rather than on the causal markers associated with the target class, leading to poor generalization and biased predictions. Although explainability via counterfactual image generation has been successful at exposing the problem, bias mitigation strategies that permit accurate explainability in the presence of dominant and diverse artifacts remain unsolved. In this work, we propose the DeCoDEx framework and show how an external, pre-trained binary artifact detector can be leveraged during inference to guide a diffusion-based counterfactual image generator towards accurate explainability.  Experiments on the CheXpert dataset, using both synthetic artifacts and real visual artifacts (support devices), show that the proposed method successfully synthesizes the counterfactual images that change the causal pathology markers associated with Pleural Effusion while preserving or ignoring the visual artifacts. Augmentation of ERM and Group-DRO classifiers with the DeCoDEx generated images substantially improves the results across underrepresented groups that are out of distribution for each class. The code is made publicly available at https://github.com/NimaFathi/DeCoDEx.",
        "authors": "Nima Fathi, Amar Kumar, Brennan Nichyporuk, Mohammad Havaei, Tal Arbel",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.04"
    },
    {
        "number": 315,
        "UID": "O-315",
        "forum": "https://openreview.net/forum?id=EWbMmmQnVy",
        "Track": "Full Paper",
        "Session": "Oral 3.1 - Explainable AI & Uncertainty",
        "Final Decision": "Oral",
        "title": "Parameter-Efficient Generation of Natural Language Explanations for Chest X-ray Classification",
        "abstract": "The increased interest and importance of explaining neural networks' predictions, especially in the medical community, associated with the known unreliability of saliency maps, the most common explainability method, has sparked research into other types of explanations. Natural Language Explanations (NLEs) emerge as an alternative, with the advantage of being inherently understandable by humans and the standard way that radiologists explain their diagnoses. We extend upon previous work on NLE generation for multi-label chest X-ray diagnosis by replacing the traditional decoder-only NLE generator with an encoder-decoder architecture. This constitutes a first step towards Reinforcement Learning-free adversarial generation of NLEs when no (or few) ground-truth NLEs are available for training, since the generation is done in the continuous encoder latent space, instead of in the discrete decoder output space.\nHowever, in the current scenario, large amounts of annotated examples are still required, which are especially costly to obtain in the medical domain, given that they need to be provided by clinicians. Thus, we explore how the recent developments in Parameter-Efficient Fine-Tuning (PEFT) can be leveraged for this use-case. We compare different PEFT methods and find that integrating the visual information into the NLE generator layers instead of only at the input achieves the best results, even outperforming the fully fine-tuned encoder-decoder-based model, while only training 12\\% of the model parameters. Additionally, we empirically demonstrate the viability of supervising the NLE generation process on the encoder latent space, thus laying the foundation for RL-free adversarial training in low ground-truth NLE availability regimes. The code is publicly available at https://github.com/to_be_added.",
        "authors": "Isabel Rio-Torto, Jaime S Cardoso, Luis Filipe Teixeira",
        "Time": "Day 3 \u2014 9:00-10:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.05",
        "Chairs": "Benjamin Billot & Jonas Richiardi"
    },
    {
        "number": 315,
        "UID": "F-315",
        "forum": "https://openreview.net/forum?id=EWbMmmQnVy",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Parameter-Efficient Generation of Natural Language Explanations for Chest X-ray Classification",
        "abstract": "The increased interest and importance of explaining neural networks' predictions, especially in the medical community, associated with the known unreliability of saliency maps, the most common explainability method, has sparked research into other types of explanations. Natural Language Explanations (NLEs) emerge as an alternative, with the advantage of being inherently understandable by humans and the standard way that radiologists explain their diagnoses. We extend upon previous work on NLE generation for multi-label chest X-ray diagnosis by replacing the traditional decoder-only NLE generator with an encoder-decoder architecture. This constitutes a first step towards Reinforcement Learning-free adversarial generation of NLEs when no (or few) ground-truth NLEs are available for training, since the generation is done in the continuous encoder latent space, instead of in the discrete decoder output space.\nHowever, in the current scenario, large amounts of annotated examples are still required, which are especially costly to obtain in the medical domain, given that they need to be provided by clinicians. Thus, we explore how the recent developments in Parameter-Efficient Fine-Tuning (PEFT) can be leveraged for this use-case. We compare different PEFT methods and find that integrating the visual information into the NLE generator layers instead of only at the input achieves the best results, even outperforming the fully fine-tuned encoder-decoder-based model, while only training 12\\% of the model parameters. Additionally, we empirically demonstrate the viability of supervising the NLE generation process on the encoder latent space, thus laying the foundation for RL-free adversarial training in low ground-truth NLE availability regimes. The code is publicly available at https://github.com/to_be_added.",
        "authors": "Isabel Rio-Torto, Jaime S Cardoso, Luis Filipe Teixeira",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.05"
    },
    {
        "number": 325,
        "UID": "O-325",
        "forum": "https://openreview.net/forum?id=CTJ5ERXOYE",
        "Track": "Full Paper",
        "Session": "Oral 3.1 - Explainable AI & Uncertainty",
        "Final Decision": "Oral",
        "title": "Finite Volume Informed Graph Neural Network for Myocardial Perfusion Simulation",
        "abstract": "Medical imaging and numerical simulation of partial differential equations (PDEs) representing biophysical processes, have been combined in the past few decades to provide noninvasive diagnostic and treatment prediction tools for various diseases. Most approaches involve solving computationally expensive PDEs, which can hinder their effective deployment in clinical settings. To overcome this limitation, deep learning has emerged as a promising method to accelerate numerical solvers. One challenge persists however in the generalization abilities of these models, given the wide variety of patient morphologies. This study addresses this challenge by introducing a physics-informed graph neural network designed to solve Darcy equations for the simulation of myocardial perfusion. Leveraging a finite volume discretization of the equations as a \"physics-informed\" loss, our model was successfully trained and tested on a 3D synthetic dataset, namely meshes representing simplified myocardium shapes. Subsequent evaluation on a genuine myocardium mesh, extracted from patient Computed Tomography images, demonstrated promising results and generalized capabilities. Such a fast solver, within a differentiable learning framework will enable to tackle inverse problems based on $\\text{H}_2$O-PET perfusion imaging data.",
        "authors": "Raoul Sall\u00e9 de Chou, Matthew Sinclair, Sabrina Lynch, Nan Xiao, Laurent Najman, Irene Vignon-clementel, Hugues Talbot",
        "Time": "Day 3 \u2014 9:00-10:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.06",
        "Chairs": "Benjamin Billot & Jonas Richiardi"
    },
    {
        "number": 325,
        "UID": "F-325",
        "forum": "https://openreview.net/forum?id=CTJ5ERXOYE",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Finite Volume Informed Graph Neural Network for Myocardial Perfusion Simulation",
        "abstract": "Medical imaging and numerical simulation of partial differential equations (PDEs) representing biophysical processes, have been combined in the past few decades to provide noninvasive diagnostic and treatment prediction tools for various diseases. Most approaches involve solving computationally expensive PDEs, which can hinder their effective deployment in clinical settings. To overcome this limitation, deep learning has emerged as a promising method to accelerate numerical solvers. One challenge persists however in the generalization abilities of these models, given the wide variety of patient morphologies. This study addresses this challenge by introducing a physics-informed graph neural network designed to solve Darcy equations for the simulation of myocardial perfusion. Leveraging a finite volume discretization of the equations as a \"physics-informed\" loss, our model was successfully trained and tested on a 3D synthetic dataset, namely meshes representing simplified myocardium shapes. Subsequent evaluation on a genuine myocardium mesh, extracted from patient Computed Tomography images, demonstrated promising results and generalized capabilities. Such a fast solver, within a differentiable learning framework will enable to tackle inverse problems based on $\\text{H}_2$O-PET perfusion imaging data.",
        "authors": "Raoul Sall\u00e9 de Chou, Matthew Sinclair, Sabrina Lynch, Nan Xiao, Laurent Najman, Irene Vignon-clementel, Hugues Talbot",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - O.06"
    },
    {
        "number": 147,
        "UID": "F-147",
        "forum": "https://openreview.net/forum?id=5Oiqw76ube",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Imbalance-aware loss functions improve medical image classification",
        "abstract": "Deep learning models offer unprecedented opportunities for diagnosis, prognosis, and treatment planning.\n    However, conventional deep learning pipelines often encounter challenges in learning unbiased classifiers within imbalanced data settings, frequently exhibiting bias towards minority classes.\n    In this study, we aim to improve medical image classification by effectively addressing class imbalance.\n    To this end, we employ differentiable loss functions derived from classification metrics commonly used in imbalanced data settings: Matthews correlation coefficient (MCC) and the F1 score.\n    We explore the efficacy of these loss functions both independently and in combination with cross-entropy loss and various batch sampling strategies on diverse medical datasets of 2D fundoscopy and 3D magnetic resonance images.\n    Our findings demonstrate that, compared to conventional loss functions, we achieve notable improvements in overall classification performance, with increases of up to +12% in balanced accuracy and up to +51% in class-wise F1 score for minority classes when utilizing cross-entropy coupled with metrics-derived loss. Additionally, we conduct feature visualization to gain insights into the behavior of these features during training with imbalance-aware loss functions. Our visualization reveals a more pronounced clustering of minority classes in the feature space, consistent with our classification results.\n    Our results underscore the effectiveness of combining cross-entropy loss with class-imbalance-aware loss functions in training more accurate classifiers, particularly for minority classes.",
        "authors": "Daniel Scholz, Ayhan Can Erdur, Josef A Buchner, Jan C Peeken, Daniel Rueckert, Benedikt Wiestler",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.03"
    },
    {
        "number": 149,
        "UID": "F-149",
        "forum": "https://openreview.net/forum?id=eKKWGE5hgF",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Interpretable Uncertainty-Aware Deep Regression with Cohort Saliency Analysis for Three-Slice CT Imaging Studies",
        "abstract": "Obesity is associated with an increased risk of morbidity and mortality. Achieving a healthy body composition, which involves maintaining a balance between fat and muscle mass, is important for metabolic health and preventing chronic diseases. Computed tomography (CT) imaging offers detailed insights into the body\u2019s internal structure, aiding in understanding body composition and its related factors. In this feasibility study, we utilized CT image data from 2,724 subjects from the large metabolic health cohort studies SCAPIS and IGT. We train and evaluate an uncertainty-aware deep regression based ResNet-50 network, which outputs its prediction as mean and variance, for quantification of cross-sectional areas of liver, visceral adipose tissue (VAT), and thigh muscle. This was done using collages of three single-slice CT images from the liver, abdomen, and thigh regions. The model demonstrated promising results with the evaluation metrics \u2013 including R-squared ($R^2$) and mean absolute error (MAE) for predictions. Additionally, for interpretability, the model was evaluated with saliency analysis based on Grad-CAM (Gradient-weighted Class Activation Mapping) at stages 2, 3, and 4 of the network. Deformable image registration to a template subject further enabled cohort saliency analysis that provide group-wise visualization of image regions of importance for associations to biomarkers of interest. We found that the networks focus on relevant regions for each target, according to prior knowledge. The source code is available at: \\url{https://github.com/noumannahmad/dr_3slice_ct}.",
        "authors": "Nouman Ahmad, Johan \u00d6fverstedt, Sambit Tarai, G\u00f6ran Bergstr\u00f6m, H\u00e5kan Ahlstr\u00f6m, Joel Kullberg",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.04"
    },
    {
        "number": 211,
        "UID": "F-211",
        "forum": "https://openreview.net/forum?id=7w5IHAybj3",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Analysis of Transformers for Medical Image Retrieval",
        "abstract": "This paper investigates the application of transformers to medical image retrieval. Although various methods have been attempted in this domain, transformers have not been extensively explored. Leveraging vision transformers, we consider co-attention between image tokens. Two main aspects are investigated: the analysis of various architectures and parameters for transformers and the evaluation of explanation techniques. Specifically, we employ contrastive learning to retrieve attention-based images that consider the relationships between query and database images. Our experiments on diverse medical datasets, such as ISIC 2017, COVID-19 chest X-ray, and Kvasir, using multiple transformer architectures, demonstrate superior performance compared to convolution-based methods and transformers using cross-entropy losses. Further, we conducted a quantitative evaluation of various state-of-the-art explanation techniques using insertion-deletion metrics, in addition to basic qualitative assessments. Among these methods, Transformer Input Sampling (TIS) stands out, showcasing superior performance and enhancing interpretability, thus distinguishing it from black-box models.",
        "authors": "Arvapalli Sai Susmitha, Vinay P. Namboodiri",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.10"
    },
    {
        "number": 234,
        "UID": "F-234",
        "forum": "https://openreview.net/forum?id=Q5CTUZHp5U",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Implicit neural compression for privacy preserving medical image sharing",
        "abstract": "Despite its undeniable success, deep learning for medical imaging with large public datasets leads to an often overlooked risk of leaking sensitive patient information. A person's X-ray, even with proper anonymisation applied, can readily serve as fingerprint and would enable a highly accurate re-identification of the same individual in a large pool of scans. Common practices for reducing privacy risks involve a synthetic deterioration of image quality, e.g. by adding noise or downsampling images, before sharing them publicly. Yet, this also adversely affects the quality of downstream image recognition models trained on such datasets. We propose a novel strategy for finding a better compromise of model quality and privacy preservation by means of implicit neural obfuscation. Our method jointly overfits a neural network to a small batch of patients' X-ray scans and applies a substantial compression - the number of network parameters representing the images is more than 6x smaller than the original pixels. In addition, we introduce a k-anonymity mixing that injects partial information from other patients for each reconstruction. That way identifiable information is efficiently obfuscated, while we manage to maintain the quality of relevant image parts for the intended downstream task. Experimental validation on the public RANZR CLiP dataset demonstrates improved segmentation quality and up to 3 times reduced privacy risks compared to simpler image obfuscation baselines. In contrast to other recent work that learn specific anonymous representations, which no longer resemble visually meaningful scans, our approach remains interpretable and is not tied to a certain downstream network. Source code and a demo dataset are available at https://github.com/mattiaspaul/neuralObfuscation.",
        "authors": "Mattias P Heinrich, Lasse Hansen",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.05"
    },
    {
        "number": 246,
        "UID": "F-246",
        "forum": "https://openreview.net/forum?id=cRfg2vho5K",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Structure Size as Confounder in Uncertainty Based Segmentation Quality Prediction",
        "abstract": "Various uncertainty estimation methods have been proposed for deep learning-based image segmentation models. An uncertainty measure is treated useful if it can be used to accurately predict segmentation quality. Therefore, structure-wise uncertainty measures are frequently correlated with measures like the Dice score. However, it is known that the Dice score highly depends on the size of the structure of interest. It is less well-known that popular structure-wise uncertainty measures also correlate with structure size. Therefore, the structure size acts as confounding variable when trying to quantify the performance of such uncertainty measures via correlation. We investigate this for the popular uncertainty measures structure-wise epistemic uncertainty, mean pairwise Dice and volume variation coefficient based on test-time-augmentation, Monte Carlo Dropout and model ensembles. We propose to use a partial correlation coefficient to address structure size as confounding variable and arrive at lower correlation estimates which better reflect the true relationship between segmentation quality and structure-wise uncertainty.",
        "authors": "Kai Gei\u00dfler, Jochen G. Hirsch, Stefan Heldmann, Hans Meine",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.06"
    },
    {
        "number": 257,
        "UID": "F-257",
        "forum": "https://openreview.net/forum?id=7QIF3J6Buv",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Real-time MR-based 3D motion monitoring using raw k-space data",
        "abstract": "Due to its great soft-tissue contrast and non-invasive nature, magnetic resonance imaging (MRI) is uniquely qualified for motion monitoring during radiotherapy.\nHowever, real-time capabilities are limited by its long acquisition times, particularly in 3D, and require highly undersampling k-space resulting in lower image resolution and image artifacts.\nIn this paper, we propose a simple recurrent neural network (RNN) architecture to continually estimate target motion from single k-space spokes.\nBy directly using the incoming k-space data, additional image reconstruction steps are avoided and less data is required between estimations achieving a latency of only a few milliseconds.\nThe 4D XCAT phantom was used to generate realistic data of the abdomen affected by respiratory and cardiac motion and a simulated lesion inserted into the liver acted as the target.\nWe show that using a Kooshball trajectory to sample 3D k-space gives superior results compared to a stack-of-stars (SoS) trajectory.\nThe RNN quickly learns the motion pattern and can give new motion estimations at a frequency of more than 230 Hz, demonstrating the feasibility of drastically improving latency of MR-based motion monitoring systems.",
        "authors": "Marius Krusen, Floris Ernst",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.09"
    },
    {
        "number": 26,
        "UID": "F-26",
        "forum": "https://openreview.net/forum?id=kcbAZwxCFV",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Leveraging Probabilistic Segmentation Models for Improved Glaucoma Diagnosis: A Clinical Pipeline Approach",
        "abstract": "The accurate segmentation of the optic cup and disc in fundus images is essential for diagnostic processes such as glaucoma detection. The inherent ambiguity in locating these structures often poses a significant challenge, leading to potential misdiagnosis. To model such ambiguities, numerous probabilistic segmentation models have been proposed. In this paper, we investigate the integration of these probabilistic segmentation models into a multistage pipeline closely resembling clinical practice. Our findings indicate that leveraging the uncertainties provided by these models substantially enhances the quality of glaucoma diagnosis compared to relying on a single segmentation only.",
        "authors": "Anna M. Wundram, Paul Fischer, Stephan Wunderlich, Hanna Faber, Lisa M. Koch, Philipp Berens, Christian F. Baumgartner",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.01"
    },
    {
        "number": 270,
        "UID": "F-270",
        "forum": "https://openreview.net/forum?id=3UMzxqDcpY",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "There Are No Shortcuts to Anywhere Worth Going: Identifying Shortcuts in Deep Learning Models for Medical Image Analysis",
        "abstract": "Many studies have reported human-level accuracy (or better) for AI-powered algorithms performing a specific clinical task, such as detecting pathology. However, these results often fail to generalize to other scanners or populations. Several mechanisms have been identified that confound generalization. One such is shortcut learning, where a network erroneously learns to depend on a fragile spurious feature, such as a text label added to the image, rather than scrutinizing the genuinely useful regions of the image. In this way, systems can exhibit misleadingly high test-set results while the labels are present but fail badly elsewhere where the relationship between the label and the spurious feature breaks down. In this paper, we investigate whether it is possible to detect shortcut learning and locate where the shortcut is happening in a neural network. We propose a novel methodology utilizing the sample difficulty metric Prediction Depth (PD) and KL divergence to identify specific layers of a neural network model where the learned features of a shortcut manifest. We demonstrate that our approach can effectively isolate these layers across several shortcuts, model architectures, and datasets. Using this, we show a correlation between the visual complexity of a shortcut, the depth of its feature manifestation within the model, and the extent to which a model relies on it. Finally, we highlight the nuanced relationship between learning rate and shortcut learning.",
        "authors": "Christopher Boland, Keith A Goatman, Sotirios A. Tsaftaris, Sonia Dahdouh",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.07"
    },
    {
        "number": 277,
        "UID": "F-277",
        "forum": "https://openreview.net/forum?id=aVI08wOu03",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Predicting 3D forearm fracture angle from biplanar Xray images with rotational bone pose estimation",
        "abstract": "Two-dimensional X-ray images, while widely used, have limitations to reflect 3D information of the imaged objects. \nSeveral studies have tried to recover such information from multiple X-ray images of the same object. Still, those approaches often fail due to the unrealistic assumption that the target does not move between views and those two views are perfectly orthogonal.\nA problem where 3D information would be highly valuable but is very difficult to assess from 2D X-ray images is the measurement of the actual 3D fracture angles in the forearm. \nTo address this problem, we propose a deep learning-based method that predicts the rotational movement and skeletal posture from biplanar X-ray images, offering a novel and precise solution.\nOur strategy comprises the following steps: (1) automatic segmentation of the ulna and radius bones of the forearm on two X-ray images by a neural network; (2) prediction of the rotational parameters of the bones by a pose prediction network; (3) automatic detection of fracture locations and assessment of the fracture angles on 2D images; and (4) reconstruction of the real 3D fracture angle by inferring it from the 2D fracture information and the skeleton pose parameters collected from the two images. \nOur experiments on X-ray images show that our method can accurately measure 2D fracture angles and infer the pose of the forearm bones. By simulating X-ray images for various types of fractures, we show that our method could provide more accurate measurements of fracture angles in 3D. We are the first attempt for the fully automatic fracture angle measurements on both 2D and 3D versions, and we show the robustness of our method even in extreme cases where the two views are highly nonorthogonal.",
        "authors": "Hanxue Gu, Roy Colglazier, Jikai Zhang, Robert Lark, Benjamin Alman, Maciej A Mazurowski",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.08"
    },
    {
        "number": 28,
        "UID": "F-28",
        "forum": "https://openreview.net/forum?id=jRtUQ2VnNi",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "UnCLe SAM: Unleashing SAM\u2019s Potential for Continual Prostate MRI Segmentation",
        "abstract": "Continual medical image segmentation primarily explores the utilization of U-Net and its derivatives within the realm of medical imaging, posing significant challenges in meeting the demands of shifting domains over time. Foundation models serve as robust knowledge repositories, offering unique advantages such as general applicability, knowledge transferability, and continuous improvements. By leveraging pre-existing domain insights, adaptability, generalization, and performance across diverse tasks can be enhanced.\nIn this work, we show how to deploy Segment Anything Model's (SAM) natural image pretraining for the continual medical image segmentation, where data is sparse.\nWe introduce UnCLe SAM, a novel approach that uses the knowledge of the pre-trained SAM foundation model to make it suitable for continual segmentation in dynamic environments.\nWe demonstrate that UnCLe SAM is a robust alternative to U-Net-based approaches and showcase its state-of-the-art (SOTA) continual medical segmentation capabilities.\nThe primary objective of UnCLe SAM is to strike a delicate balance between model rigidity and plasticity, effectively addressing prevalent pitfalls within CL methodologies.\nWe assess UnCLe SAM through a series of prostate segmentation tasks, applying a set of different CL methods. Comparative evaluations against the SOTA Lifelong nnU-Net framework reveal the potential application of UnCLe SAM in dynamically changing environments like healthcare.\nOur code base will be made public upon acceptance.",
        "authors": "Amin Ranem, Mohamed Afham Mohamed Aflal, Moritz Fuchs, Anirban Mukhopadhyay",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.11"
    },
    {
        "number": 302,
        "UID": "F-302",
        "forum": "https://openreview.net/forum?id=YUtcFW5xRt",
        "Track": "Full Paper",
        "Session": "Poster 2.1",
        "Final Decision": "Poster",
        "title": "Improving Identically Distributed and Out-of-Distribution Medical Image Classification with Segmentation-Guided Attention in Small Dataset Scenarios",
        "abstract": "We propose a new approach for training medical image classification models using segmentation masks, particularly effective in small dataset scenarios. By guiding the model\u2019s attention with segmentation masks toward relevant features, we significantly improve accuracy for diagnosing Hydronephrosis. Evaluation of our model on identically distributed data showed either the same or better performance with improvement up to 0.28 in AUROC and up to 0.33 in AUPRC. Our method showed better generalization ability than baselines, improving from 0.02 to 0.75 in AUROC and from 0.09 to 0.47 in AUPRC for four different out-of-distribution datasets. The results show that models trained on smaller datasets using our approach can achieve comparable results to those trained on datasets 25 times larger.",
        "authors": "Mariia Rizhko, Lauren Erdman, Mandy Rickard, Kunj Sheth, Daniel Alvarez, Kyla N Velaer, Megan A. Bonnett, Christopher S. Cooper, Gregory E. Tasian, John Weaver, Alice Xiang, Armando J. Lorenzo, Anna Goldenberg",
        "Time": "Day 2 \u2014 11:00-12:30",
        "Poster time": "Day 2 \u2014 11:00-12:30",
        "Poster ID": "Poster 2.1 - F.15"
    },
    {
        "number": 33,
        "UID": "F-33",
        "forum": "https://openreview.net/forum?id=1eSuIPZ1cf",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Diffusion X-ray image denoising",
        "abstract": "X-ray imaging is a cornerstone in medical diagnosis, constituting a significant portion of the radiation dose encountered by patients. Excessive radiation poses health risks, particularly for pediatric patients, but despite the imperative to reduce radiation doses, conventional image processing methods for X-ray denoising often struggle with heuristic parameter calibration and prolonged execution times. Deep Learning solutions have emerged as promising alternatives, but their effectiveness varies, and challenges persist in preserving image quality.\nThis paper presents an exploration of diffusion models for planar X-ray image denoising, a novel approach that to our knowledge has not been yet investigated in this domain. We perform real time denoising of Poisson noise while preserving image resolution and structural similarity. The results indicate that diffusion models show promise for planar X-ray image denoising, offering a potential improvement in the optimization of diagnostic utility amid dose reduction efforts.",
        "authors": "Daniel Sanderson, Pablo M. Olmos, Carlos Fern\u00e1ndez Del Cerro, Manuel Desco, M\u00f3nica Abella",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.02"
    },
    {
        "number": 47,
        "UID": "F-47",
        "forum": "https://openreview.net/forum?id=F6rhgpGkAy",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Training-free Prompt Placement by Propagation for SAM Predictions in Bone CT Scans",
        "abstract": "The Segment Anything Model (SAM) is an interactive foundation segmentation model, showing impressive results for 2D natural images using prompts such as points and boxes. Transferring these results to medical image segmentation is challenging due to the 3D nature of medical images and the high demand of manual interaction. As a 2D architecture, SAM is applied slice-per-slice to a 3D medical scan. This hinders the application of SAM for volumetric medical scans since at least one prompt per class for each single slice is needed. In our work, the applicability is improve by reducing the number of necessary user-generated prompts. We introduce and evaluate multiple training-free strategies to automatically place box prompts in bone CT volumes, given only one initial box prompt per class. \nThe average performance of our methods ranges from 54.22% Dice to 88.26% Dice. At the same time, the number of annotated pixels is reduced significantly from a few millions to two pixels per class. These promising results underline the potential of foundation models in medical image segmentation, paving the way for annotation-efficient, general approaches.",
        "authors": "Caroline Magg, Lukas P.E. Verweij, Maaike A. ter Wee, George S. Buijs, Johannes G.G. Dobbe, Geert J. Streekstra, Leendert Blankevoort, Clara I. S\u00e1nchez",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.12"
    },
    {
        "number": 87,
        "UID": "F-87",
        "forum": "https://openreview.net/forum?id=QTFVYcV92b",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays",
        "abstract": "Difference visual question answering (diff-VQA) is a challenging task that requires answering complex questions based on differences between a pair of images. This task is particularly important in reading chest X-ray images because radiologists often compare multiple images of the same patient taken at different times to track disease progression and changes in its severity in their clinical practice. However, previous works focused on designing specific network architectures for the diff-VQA task, missing opportunities to enhance the model's performance using a pretrained vision-language model (VLM). Here, we introduce a novel VLM called PLURAL, which is pretrained on natural and longitudinal chest X-ray data for the diff-VQA task. The model is developed using a step-by-step approach, starting with being pretrained on natural images and texts, followed by being trained using longitudinal chest X-ray data. The longitudinal data consist of pairs of X-ray images, along with question-answer sets and radiologist\u2019s reports that describe the changes in lung abnormalities and diseases over time. Our experimental results show that the PLURAL model outperforms state-of-the-art methods not only in diff-VQA for longitudinal X-rays but also in conventional VQA for a single X-ray image. Through extensive experiments, we demonstrate the effectiveness of the proposed VLM architecture and pretraining method in improving the model\u2019s performance.",
        "authors": "Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - F.13"
    },
    {
        "number": 102,
        "UID": "F-102",
        "forum": "https://openreview.net/forum?id=gFIepubv7E",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "[Citation needed] Data usage and citation practices in medical imaging conferences",
        "abstract": "Medical imaging papers often focus on methodology, but the quality of the algorithms and the validity of the conclusions are highly dependent on the datasets used. As creating datasets requires a lot of effort, researchers often use publicly available datasets, there is however no adopted standard for citing the datasets used in scientific papers, leading to difficulty in tracking dataset usage.\nIn this work, we present two open-source tools we created that could help with the detection of dataset usage, a pipeline using OpenAlex and full-text analysis, and a PDF annotation software used in our study to manually label the presence of datasets. We applied both tools on a study of the usage of 20 publicly available medical datasets in papers from MICCAI and MIDL. \nWe compute the proportion and the evolution between 2013 and 2023 of 3 types of presence in a paper: cited, mentioned in the full text, cited and mentioned. Our findings demonstrate the concentration of the usage of a limited set of datasets. We also highlight different citing practices, making the automation of tracking difficult.",
        "authors": "Th\u00e9o Sourget, Ahmet Akko\u00e7, Stinna Winther, Christine Lyngbye Galsgaard, Amelia Jim\u00e9nez-S\u00e1nchez, Dovile Juodelyte, Caroline Petitjean, Veronika Cheplygina",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.2 - O.02"
    },
    {
        "number": 102,
        "UID": "O-102",
        "forum": "https://openreview.net/forum?id=gFIepubv7E",
        "Track": "Full Paper",
        "Session": "Oral 3.2 - Foundation Models",
        "Final Decision": "Oral",
        "title": "[Citation needed] Data usage and citation practices in medical imaging conferences",
        "abstract": "Medical imaging papers often focus on methodology, but the quality of the algorithms and the validity of the conclusions are highly dependent on the datasets used. As creating datasets requires a lot of effort, researchers often use publicly available datasets, there is however no adopted standard for citing the datasets used in scientific papers, leading to difficulty in tracking dataset usage.\nIn this work, we present two open-source tools we created that could help with the detection of dataset usage, a pipeline using OpenAlex and full-text analysis, and a PDF annotation software used in our study to manually label the presence of datasets. We applied both tools on a study of the usage of 20 publicly available medical datasets in papers from MICCAI and MIDL. \nWe compute the proportion and the evolution between 2013 and 2023 of 3 types of presence in a paper: cited, mentioned in the full text, cited and mentioned. Our findings demonstrate the concentration of the usage of a limited set of datasets. We also highlight different citing practices, making the automation of tracking difficult.",
        "authors": "Th\u00e9o Sourget, Ahmet Akko\u00e7, Stinna Winther, Christine Lyngbye Galsgaard, Amelia Jim\u00e9nez-S\u00e1nchez, Dovile Juodelyte, Caroline Petitjean, Veronika Cheplygina",
        "Time": "Day 3 \u2014 14:30-15:15",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.2 - O.02",
        "Chairs": "Tal Arbel & Ismail Ben Ayed"
    },
    {
        "number": 134,
        "UID": "F-134",
        "forum": "https://openreview.net/forum?id=sN3sDKkGeN",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models",
        "abstract": "Medical image segmentation allows quantifying target structure size and shape, aiding in disease diagnosis, prognosis, surgery planning, and comprehension. Building upon recent advancements in foundation Vision-Language Models (VLMs) from natural image-text pairs, several studies have proposed adapting them to Vision-Language Segmentation Models (VLSMs) that allow using language text as an additional input to segmentation models. Introducing auxiliary information via text with human-in-the-loop prompting during inference opens up unique opportunities, such as open vocabulary segmentation and potentially more robust segmentation models against out-of-distribution data.\nAlthough transfer learning from natural to medical images has been explored for image-only segmentation models, the joint representation of vision-language in segmentation problems remains underexplored. This study introduces the first systematic study on transferring VLSMs to 2D medical images, using carefully curated $11$ datasets encompassing diverse modalities and insightful language prompts and experiments. Our findings demonstrate that although VLSMs show competitive performance compared to image-only models\nfor segmentation after finetuning in limited medical image datasets, not all VLSMs utilize the additional information from language prompts, with image features playing a dominant role. While VLSMs exhibit enhanced performance in handling pooled datasets with diverse\nmodalities and show potential robustness to domain shifts compared to conventional segmentation models, our results suggest that novel approaches are required to enable VLSMs to leverage the various auxiliary information available through language prompts. The code and datasets are available at https://github.com/naamiinepal/medvlsm.",
        "authors": "Kanchan Poudel, Manish Dhakal, Prasiddha Bhandari, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.2 - O.04"
    },
    {
        "number": 134,
        "UID": "O-134",
        "forum": "https://openreview.net/forum?id=sN3sDKkGeN",
        "Track": "Full Paper",
        "Session": "Oral 3.2 - Foundation Models",
        "Final Decision": "Oral",
        "title": "Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models",
        "abstract": "Medical image segmentation allows quantifying target structure size and shape, aiding in disease diagnosis, prognosis, surgery planning, and comprehension. Building upon recent advancements in foundation Vision-Language Models (VLMs) from natural image-text pairs, several studies have proposed adapting them to Vision-Language Segmentation Models (VLSMs) that allow using language text as an additional input to segmentation models. Introducing auxiliary information via text with human-in-the-loop prompting during inference opens up unique opportunities, such as open vocabulary segmentation and potentially more robust segmentation models against out-of-distribution data.\nAlthough transfer learning from natural to medical images has been explored for image-only segmentation models, the joint representation of vision-language in segmentation problems remains underexplored. This study introduces the first systematic study on transferring VLSMs to 2D medical images, using carefully curated $11$ datasets encompassing diverse modalities and insightful language prompts and experiments. Our findings demonstrate that although VLSMs show competitive performance compared to image-only models\nfor segmentation after finetuning in limited medical image datasets, not all VLSMs utilize the additional information from language prompts, with image features playing a dominant role. While VLSMs exhibit enhanced performance in handling pooled datasets with diverse\nmodalities and show potential robustness to domain shifts compared to conventional segmentation models, our results suggest that novel approaches are required to enable VLSMs to leverage the various auxiliary information available through language prompts. The code and datasets are available at https://github.com/naamiinepal/medvlsm.",
        "authors": "Kanchan Poudel, Manish Dhakal, Prasiddha Bhandari, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal",
        "Time": "Day 3 \u2014 14:30-15:15",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.2 - O.04",
        "Chairs": "Tal Arbel & Ismail Ben Ayed"
    },
    {
        "number": 31,
        "UID": "F-31",
        "forum": "https://openreview.net/forum?id=Y1BeK8dTno",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "ICL-SAM: Synergizing In-context Learning Model and SAM in Medical Image Segmentation",
        "abstract": "Medical image segmentation, a field facing domain shifts due to diverse imaging modal- ities and biomedical domains, has made strides with the development of robust models. The In-Context Learning (ICL) model, like UniverSeg, demonstrates robustness to domain shifts with support image-label pairs in varied medical imaging segmentation tasks. How- ever, its performance is still unsatisfied. On the other hand, the Segment Anything Model (SAM) stands out as a powerful universal segmentation model. In this work, we intro- duce a novel methodology, ICL-SAM, that integrates the superior performance of SAM with the ICL model to create more effective segmentation models within the in-context learning paradigm. Our approach employs SAM to refine segmentation results from ICL model and leverages ICL model to generate prompts for SAM, eliminating the need for manual prompt provision. Additionally, we introduce a semantic confidence map gener- ation method into our framework to guide the prediction of both ICL model and SAM, thereby further enhancing segmentation accuracy. Our method has been extensively eval- uated across multiple medical imaging contexts, including fundus, MRI, and CT images, spanning five datasets. The results demonstrate significant performance improvements, particularly in settings with few support pairs, where our method can achieve over a 10% increase in the Dice coefficient compared to cutting edge ICL model. Our code will be publicly available.",
        "authors": "Jiesi Hu, Yang Shang, Yanwu Yang, Guo Xutao, Hanyang Peng, Ting Ma",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.2 - O.03"
    },
    {
        "number": 59,
        "UID": "F-59",
        "forum": "https://openreview.net/forum?id=LNdU9RTv3L",
        "Track": "Full Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Slide-SAM: Medical SAM Meets Sliding Window",
        "abstract": "The Segment Anything Model (SAM) has achieved a notable success in two-dimensional image segmentation in natural images. However, the substantial gap between medical and natural images hinders its direct application to medical image segmentation tasks. Particularly in 3D medical images, SAM struggles to learn contextual relationships between slices, limiting its practical applicability. Moreover, applying 2D SAM to 3D images requires prompting the entire volume, which is time- and label-consuming. To address these problems, we propose Slide-SAM, which treats a stack of three adjacent slices as a prediction window. It firstly takes three slices from a 3D volume and point- or bounding box prompts on the central slice as inputs to predict segmentation masks for all three slices. Subsequently, the masks of the top and bottom slices are then used to generate new prompts for adjacent slices. Finally, step-wise prediction can be achieved by sliding the prediction window forward or backward through the entire volume. Our model is trained on multiple public and private medical datasets and demonstrates its effectiveness through extensive 3D segmetnation experiments, with the help of minimal prompts. Code is available at https://github.com/Curli-quan/Slide-SAM.",
        "authors": "Quan Quan, Fenghe Tang, Zikang Xu, Heqin Zhu, S Kevin Zhou",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.2 - O.01"
    },
    {
        "number": 59,
        "UID": "O-59",
        "forum": "https://openreview.net/forum?id=LNdU9RTv3L",
        "Track": "Full Paper",
        "Session": "Oral 3.2 - Foundation Models",
        "Final Decision": "Oral",
        "title": "Slide-SAM: Medical SAM Meets Sliding Window",
        "abstract": "The Segment Anything Model (SAM) has achieved a notable success in two-dimensional image segmentation in natural images. However, the substantial gap between medical and natural images hinders its direct application to medical image segmentation tasks. Particularly in 3D medical images, SAM struggles to learn contextual relationships between slices, limiting its practical applicability. Moreover, applying 2D SAM to 3D images requires prompting the entire volume, which is time- and label-consuming. To address these problems, we propose Slide-SAM, which treats a stack of three adjacent slices as a prediction window. It firstly takes three slices from a 3D volume and point- or bounding box prompts on the central slice as inputs to predict segmentation masks for all three slices. Subsequently, the masks of the top and bottom slices are then used to generate new prompts for adjacent slices. Finally, step-wise prediction can be achieved by sliding the prediction window forward or backward through the entire volume. Our model is trained on multiple public and private medical datasets and demonstrates its effectiveness through extensive 3D segmetnation experiments, with the help of minimal prompts. Code is available at https://github.com/Curli-quan/Slide-SAM.",
        "authors": "Quan Quan, Fenghe Tang, Zikang Xu, Heqin Zhu, S Kevin Zhou",
        "Time": "Day 3 \u2014 14:30-15:15",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.2 - O.01",
        "Chairs": "Tal Arbel & Ismail Ben Ayed"
    },
    {
        "number": 103,
        "UID": "S-103",
        "forum": "https://openreview.net/forum?id=3eE0XxTDlY",
        "Track": "Short Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Masked attention as a mechanism for improving interpretability of Vision Transformers",
        "abstract": "Vision Transformers are at the heart of the current surge of interest in foundation models for histopathology. They process images by breaking them into smaller patches following a regular grid, regardless of their content. Yet, not all parts of an image are equally relevant for its understanding. This is particularly true in computational pathology where background is completely non-informative and may introduce artefacts that could mislead predictions. To address this issue, we propose a novel method that explicitly masks background in Vision Transformers' attention mechanism. This ensures tokens corresponding to background patches do not contribute to the final image representation, thereby improving model robustness and interpretability. We validate our approach using prostate cancer grading from whole-slide images as a case study. Our results demonstrate that it achieves comparable performance with plain self-attention while providing more accurate and clinically meaningful attention heatmaps.",
        "authors": "Cl\u00e9ment Grisi, Geert Litjens, Jeroen van der Laak",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - S.10"
    },
    {
        "number": 113,
        "UID": "S-113",
        "forum": "https://openreview.net/forum?id=u2gMy4IvQy",
        "Track": "Short Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Explainability in Deep Learning Segmentation Models for Breast Cancer by Analogy with Texture Analysis",
        "abstract": "Despite their predictive capabilities and rapid advancement, the black-box nature of Artificial Intelligence (AI) models, particularly in healthcare, has sparked debate regarding their trustworthiness and accountability. In response, the field of Explainable AI (XAI) has emerged, aiming to create transparent AI technologies. We present a novel approach to enhance AI interpretability by leveraging texture analysis, with a focus on cancer datasets. By focusing on specific texture features and their correlations with a prediction outcome extracted from medical images, our proposed methodology aims to elucidate the underlying mechanics of AI, improve AI trustworthiness, and facilitate human understanding. The code is available at https://github.com/xrai-lib/xai-texture.",
        "authors": "Md Masum Billah, Pragati Manandhar, Sarosh Krishan, Alejandro Cedillo G\u00e1mez, Hergys Rexha, Sebastien Lafond, Kurt K. Benke, Sepinoud Azimi, Janan Arslan",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - S.11"
    },
    {
        "number": 114,
        "UID": "S-114",
        "forum": "https://openreview.net/forum?id=UDzWBT3Rgu",
        "Track": "Short Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "The Potential of Acknowledging the Unknown: Single Positive Multi-label Learning in Medical Image Processing",
        "abstract": "In the wake of high complexity and resource constraints, the manual annotation of the medical images involves a severe trade-off between the annotated labels per sample and the dataset size, besides inducing a high count of false-negative labels due to the underlying intricacies in recognizing all potential pathologies or conditions. Single Positive Multi-Label (SPML) learning aims to address the case of label noise by using only a single positive label per sample for the training, considering the remaining labels as uncertain. As SPML is yet to be fully explored in the realm of medical imaging, therefore, this work focuses on investigating the state-of-the-art SPML loss functions subsuming the Generalized Assume Negative (G-AN) and Entropy Maximization (EM) losses on different training sample counts. Additionally, we investigate the influence of the asymmetric pseudo-labeling with EM loss in minimizing the effect of label uncertainty induced by SPML learning.",
        "authors": "Helen Schneider, Priya Priya, Aditya Parikh, Christian Bauckhage, Rafet Sifa",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - S.04"
    },
    {
        "number": 124,
        "UID": "S-124",
        "forum": "https://openreview.net/forum?id=hrTiw1X7yq",
        "Track": "Short Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Uncertainty Quantification in DL Models for Cervical Cytology",
        "abstract": "Deep Learning (DL) has demonstrated significant promise in digital pathological applications both histopathology and cytopathology. However, the majority of these works primarily concentrate on evaluating the general performance of the models and overlook the crucial requirement for uncertainty quantification which is necessary for real-world clinical application. In this study, we examine the change in predictive performance and the identification of mispredictions through the incorporation of uncertainty estimates for DL-based Cervical cancer classification. Specifically, we evaluate the efficacy of three methods\u2014Monte Carlo(MC) Dropout, Ensemble Method, and Test Time Augmentation(TTA) using three metrics: variance, entropy, and sample mean uncertainty. The results demonstrate that integrating uncertainty estimates improves the model\u2019s predictive capacity in high-confidence regions, while also serving as an indicator for the model\u2019s mispredictions in low-confidence regions.",
        "authors": "Shubham Ojha, Aditya Narendra",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - S.01"
    },
    {
        "number": 160,
        "UID": "S-160",
        "forum": "https://openreview.net/forum?id=PfCY5BLNHc",
        "Track": "Short Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Calibration and Uncertainty Estimation Challenges in Self-Supervised Chest X-ray Pathology Classification Models",
        "abstract": "Uncertainty quantification is crucial for the safe deployment of AI systems in clinical radiology. We analyze the calibration of CheXzero (Tiu et al., 2022), a high-performance self-supervised model for chest X-ray pathology detection, on two external datasets and evaluate the effectiveness of two common uncertainty estimation methods: Maximum Softmax Probabilities (MSP) and Monte Carlo Dropout. Our analysis reveals poor calibration on both external datasets, with Expected Calibration Error (ECE) scores ranging from 0.12 to 0.41. Furthermore, we find that the model\u2019s prediction accuracy does not correlate with the uncertainty scores derived from MSP and Monte Carlo Dropout. These findings highlight the need for more robust uncertainty quantification methods to ensure the trustworthiness of AI-assisted clinical decision-making.",
        "authors": "Jenny Xu, Pranav Rajpurkar",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - S.06"
    },
    {
        "number": 163,
        "UID": "S-163",
        "forum": "https://openreview.net/forum?id=rnQUJLbODk",
        "Track": "Short Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "The Do\u2019s and Don\u2019ts of Grad-CAM in Image Segmentation as demonstrated on the Synapse multi-organ CT Dataset",
        "abstract": "The field of eXplainable Artificial Intelligence (xAI) has drawn an immense interest over the last decade. This interest, however, is almost exclusively focused on image classification, whereas other computer vision domains have remained relatively neglected. Recently, however, methods developed in the context of image classification have been extended to other domains such as image segmentation. One such method is Seg-Grad-CAM which is an extension of Grad-CAM. The present paper aims to highlight some of the nuances associated with the utilization of Seg-Grad-CAM in order to generate saliency maps for image segmentation, and instead highlights an alternate application, namely investigating information flow, which can better utilize the capabilities of Seg-Grad-CAM and similar methods. Sample demonstration is provided on an image from the Synapse multi-organ CT dataset.",
        "authors": "Syed Nouman Hasany, Fabrice MERIAUDEAU, Caroline Petitjean",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - S.02"
    },
    {
        "number": 32,
        "UID": "S-32",
        "forum": "https://openreview.net/forum?id=9p7cq7kcvb",
        "Track": "Short Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Constrained non-negative networks for a more explainable and interpretable classification",
        "abstract": "Interpretability and explainability of deep networks are essential for medical image analysis. Easily explainable networks with intrinsic properties and decisions based on radiological signs and not spurious confounders are highly desirable. The guaranteed monotonic relation between the input and the output of monotonic networks could be used to design such intrinsically explainable networks, but they are rarely used for images: state-of-the-art architectures are often very shallow due to convergence problems. Identifying the critical importance of weights initialization, we propose a recipe to transform any architecture into a trainable monotonic network. By using the monotonic property, adding a calibration and constraining the training in an unsupervised way, we propose a network more explainable with human-readable counterfactual examples but also more interpretable with a decision more based on the radiological signs of the pathology. Especially, we outperform state-of-the-art methods for weakly supervised anomaly detection.",
        "authors": "Valentine Wargnier Dauchelle, Thomas Grenier, Fran\u00e7oise Durand-Dubief, Fran\u00e7ois Cotton, Micha\u00ebl Sdika",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - S.05"
    },
    {
        "number": 49,
        "UID": "S-49",
        "forum": "https://openreview.net/forum?id=0Pod0c2Av6",
        "Track": "Short Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Counterfactual Analysis for Digital Histopathology Slides Using Human Interpretable Features",
        "abstract": "Recent advancements in deep learning techniques have greatly improved the precision and efficiency of computational pathology processes, facilitating diagnosis, outcome forecasting, and identification of genetic markers and disease progression. However, a significant challenge hindering the integration of these computational tools into clinical practice is the lack of interpretability of their results. In this paper, we propose a novel method for counterfactual analysis on histopathology slides to provide clear and understandable explanations based on human interpretable features for predictive tasks at the slide level. Our method addresses the challenge of generating interpretable explanations for high-dimensional tabular data in a computationally efficient manner, outperforming state-of-the-art methods by generating explanations approximately 10 times faster. This advancement holds promise for enhancing the adoption and effectiveness of deep learning models in clinical settings, ultimately improving patient care and outcomes.",
        "authors": "Hakim Benkirane, Maria Vakalopoulou, Stefan Michiels, Paul-Henry Courn\u00e8de, William Lotter",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - S.07"
    },
    {
        "number": 87,
        "UID": "S-87",
        "forum": "https://openreview.net/forum?id=klsHiruuQe",
        "Track": "Short Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Improving Model Generalization to Out-of-Domain Data in Histopathology: Leveraging Simple Techniques for External Validation",
        "abstract": "This study explores strategies to enhance the generalizability of WSI classification models across diverse centers, crucial for clinical adoption. By leveraging optimal transport and Leave-One-Center-Out training policies, we achieve a 3\\% increase in the Youden Index, demonstrating their complementary roles in improving model performance. These approaches offer promising avenues for mitigating the need for extensive calibration samples, addressing challenges in external validation and ensuring robustness in clinical settings.",
        "authors": "Melanie Lubrano, Nathan Bigaud, Thalyssa Baiocco-Rodrigues, Fabien Brulport, Alexandre Filiot, David Lin",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - S.08"
    },
    {
        "number": 96,
        "UID": "S-96",
        "forum": "https://openreview.net/forum?id=t5K9gEpDKN",
        "Track": "Short Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Towards causal prediction on Magnetic Resonance Imaging including non-imaging data",
        "abstract": "Deep learning methods can detect correlations in data but they cannot determine underlying causal relationships. Understanding causality, however, is essential because spurious correlations can obscure the true relationships in the data. In many large studies, imaging data is accompanied by additional tabular (non-imaging) clinical data. Our aim is to use the non-imaging information to learn a multi-modal feature representation that can make predictions based on learned causal dependencies while avoiding spurious correlations. This work presents our first preliminary results and outlines our future investigations.",
        "authors": "Louisa Fay, Florian Marencke, Bin Yang, Sergios Gatidis, Thomas Kuestner",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - S.03"
    },
    {
        "number": 97,
        "UID": "S-97",
        "forum": "https://openreview.net/forum?id=y5onNeda4Y",
        "Track": "Short Paper",
        "Session": "Poster 3.1",
        "Final Decision": "Poster",
        "title": "Hallucinating for Diagnosing: One-Shot Medical Image Classification Leveraging Score-Based Generative Models",
        "abstract": "Deep learning models in data-scarce domains, such as medical imaging, often suffer from poor performance due to the challenges of acquiring large amounts of labeled data. Few-shot learning offers a promising solution to this problem. This work proposes a novel framework to jointly train a score-based generative model for high-quality sample hallucination and a meta-learning framework for one-shot classification. We evaluate our approach on MRI scans of prostate cancer, aiming to classify tumors based on severity. Our preliminary experiments demonstrate promising results, indicating the efficacy of our proposed method in improving classification performance. Future work will involve further analysis using a diverse set of score models and prototypical meta-learning techniques, as well as evaluation of the effectiveness of our framework in other medical imaging tasks.",
        "authors": "Eva Pachetti, Sara Colantonio",
        "Time": "Day 3 \u2014 11:00-12:30",
        "Poster time": "Day 3 \u2014 11:00-12:30",
        "Poster ID": "Poster 3.1 - S.09"
    },
    {
        "number": 200,
        "UID": "F-200",
        "forum": "https://openreview.net/forum?id=VDvv7NJG9b",
        "Track": "Full Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Evaluating ChatGPT's Performance in Generating and Assessing Dutch Radiology Report Impressions",
        "abstract": "The integration of Large Language Models (LLMs), such as ChatGPT, in radiology could\noffer insight and interpretation to the increasing number of radiological findings generated\nby Artificial Intelligence (AI). However, the complexity of medical text presents many chal-\nlenges for LLMs, particularly in uncommon languages such as Dutch. This study therefore\naims to evaluate ChatGPT\u2019s ability to generate accurate \u2018Impression\u2019 sections of radiol-\nogy reports, and its effectiveness in evaluating these sections compared against human\nradiologist judgments. We utilized a dataset of CT-thorax radiology reports to fine-tune\nChatGPT and then conducted a reader study with two radiologists and GPT-4 out-of-the-\nbox to evaluate the AI-generated \u2018Impression\u2019 sections in comparison to the originals. The\nresults revealed that human experts rated original impressions higher than AI-generated\nones across correctness, completeness, and conciseness, highlighting a gap in the AI\u2019s abil-\nity to generate clinically reliable medical text. Additionally, GPT-4\u2019s evaluations were\nmore favorable towards AI-generated content, indicating limitations in its out-of-the-box\nuse as an evaluator in specialized domains. The study emphasizes the need for cautious\nintegration of LLMs into medical domains and the importance of expert validation, yet\nalso acknowledges the inherent subjectivity in interpreting and evaluating medical reports.",
        "authors": "Luc Builtjes, Monique Brink, Bram van Ginneken, Alessa Hering",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - F.01"
    },
    {
        "number": 206,
        "UID": "F-206",
        "forum": "https://openreview.net/forum?id=TEYGCx02p3",
        "Track": "Full Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Zero-Shot Medical Image Segmentation Based on Sparse Prompt Using Finetuned SAM",
        "abstract": "Segmentation of medical images plays a critical role in various clinical applications, facilitat- ing precise diagnosis, treatment planning, and disease monitoring. However, the scarcity of annotated data poses a significant challenge for training deep learning models in the medical imaging domain. In this paper, we propose a novel approach for minimally-guided zero-shot segmentation of medical images using the Segment Anything Model (SAM), orig- inally trained on natural images. The method leverages SAM\u2019s ability to segment arbitrary objects in natural scenes and adapts it to the medical domain without the need for labeled medical data, except for a few foreground and background points on the test image it- self. To this end, we introduce a two-stage process, involving the extraction of an initial mask from self-similarity maps and test-time fine-tuning of SAM. We run experiments on diverse medical imaging datasets, including AMOS22, MoNuSeg and the Gland segmen- tation (GlaS) challenge, and demonstrate the effectiveness of our approach.",
        "authors": "Tal Shaharabany, Lior Wolf",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - F.02"
    },
    {
        "number": 1,
        "UID": "S-1",
        "forum": "https://openreview.net/forum?id=6OSZqiODga",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Leveraging Latents for Efficient Thermography Classification and Segmentation",
        "abstract": "Breast cancer is a prominent health concern worldwide, currently being the secondmost common and second-deadliest type of cancer in women. While current breast cancer\ndiagnosis mainly relies on mammography imaging, in recent years the use of thermography\nfor breast cancer imaging has been garnering growing popularity. Thermographic imaging relies on infrared cameras to capture body-emitted heat distributions. While these\nheat signatures have proven useful for computer-vision systems for accurate breast cancer\nsegmentation and classification, prior work often relies on handcrafted feature engineering\nor complex architectures, potentially limiting the comparability and applicability of these\nmethods. In this work, we present a novel algorithm for both breast cancer classification and\nsegmentation. Rather than focusing efforts on manual feature and architecture engineering,\nour algorithm focuses on leveraging an informative, learned feature space, thus making our\nsolution simpler to use and extend to other frameworks and downstream tasks, as well as\nmore applicable to data-scarce settings. Our classification produces SOTA results, while we\nare the first work to produce segmentation regions studied in this paper. Code for reproducing all experiments is available at github.com/tamirshor7/Latents-Guided-Thermography.",
        "authors": "Tamir Shor, Chaim Baskin, Alexander Bronstein",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.03"
    },
    {
        "number": 109,
        "UID": "S-109",
        "forum": "https://openreview.net/forum?id=6TrjwzbBko",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Curriculum-learning for Vessel Occlusion Detection in Multi-site Brain CT Angiographies",
        "abstract": "Deep learning models often fail to generalize to target data due to shifts between target and training data distributions. Hence, their impact in the real world is limited. To solve this, including training data from more sites may not be enough, as new data may also present shifts with the original training data, complicating the learning process. We hypothesize that curriculum-learning may provide more robust models against training site shifts by sorting these sites in order of increased difficulty. In this work, we focus on Vessel Occlusion detection in CT angiographies from stroke-suspected patients from three sites, training first on large homogeneous balanced sites, which we hypothesize are easier to learn. Next, we incorporate small heterogeneous imbalanced sites, which may be more complex. Our approach is compared to training only on a large homogeneous site (single-site training) and to training on all sites (pooled-site training). We reach a 2% improvement in FROC and AUROC scores. Thus, adequately ordering the training sites based on simple characteristics such as label balance or data size may improve model robustness.",
        "authors": "Andr\u00e9s Mart\u00ednez Mora, Michael Baumgartner, Gianluca Brugnara, Maximilian Zenk, Yannick Kirchhoff, Aditya Rastogi, Alexander Radbruch, Martin Bendszus, Clara I. S\u00e1nchez, Philipp Vollmuth, Klaus Maier-Hein",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.17"
    },
    {
        "number": 110,
        "UID": "S-110",
        "forum": "https://openreview.net/forum?id=RuNxaXlXc1",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Boosting 3D Neuron Segmentation with 2D Vision Transformer Pre-trained on Natural Images",
        "abstract": "Neuron reconstruction, one of the fundamental tasks in neuroscience, rebuilds neuronal morphology from 3D light microscope imaging data. It plays a critical role in analyzing the structure-function relationship of neurons in the nervous system. However, due to the scarcity of neuron datasets and high-quality SWC annotations, it is still challenging to develop robust segmentation methods for single neuron reconstruction. To address this limitation, we aim to distill the consensus knowledge from massive natural image data to aid the segmentation model in learning the complex neuron structures. Specifically, in this work, we propose a novel training paradigm that leverages a 2D Vision Transformer model pre-trained on large-scale natural images to initialize our Transformer-based 3D neuron segmentation model with a tailored 2D-to-3D weight transferring strategy. Our method builds a knowledge sharing connection between the abundant natural and the scarce neuron image domains to improve the 3D neuron segmentation ability in a data-efficiency manner. Evaluated on a popular benchmark, BigNeuron, our method enhances neuron segmentation performance by 8.71% over the model trained from scratch with the same amount of training samples.",
        "authors": "Yik San Cheng, Runkai Zhao, Heng Wang, Hanchuan Peng, Weidong Cai",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.18"
    },
    {
        "number": 121,
        "UID": "S-121",
        "forum": "https://openreview.net/forum?id=VSD5PG2VtT",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Bottom-up Instance Segmentation of Catheters for Chest X-ray Images",
        "abstract": "Chest X-ray (CXR) is frequently used in emergency departments and intensive care units to verify the proper placement of central lines and tubes and to rule out related complications. \nThe automation of the X-ray reading process can be a valuable support tool for non-specialist technicians and minimize reporting delays due to non-availability of experts.\nWhile existing solutions for automated catheter segmentation and malposition detection show promising results, the disentanglement of individual catheters remains an open challenge, especially in complex cases where multiple devices appear superimposed in the X-ray projection. \nIn this paper, we propose a deep learning approach based on associative embeddings for catheter instance segmentation, able to effectively handle device intersections.",
        "authors": "Francesca Boccardi, Axel Saalbach, Heinrich Schulz, Samuele Salti, Ilyas Sirazitdinov",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.10"
    },
    {
        "number": 13,
        "UID": "S-13",
        "forum": "https://openreview.net/forum?id=FNBQOPj18N",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "eva: Evaluation framework for pathology foundation models",
        "abstract": "In computational pathology, self-supervised learning trained models surpass supervised ones in scale and performance. However, the benchmarking of these models remains a challenge due to the diversity in tasks and evaluation methods. To address this, we introduce eva (available at https://kaiko-ai.github.io/eva), an open-source framework for evaluating computational pathology foundation models (FMs). eva is designed to be modular and adaptable to both off-the-shelf and customized datasets, metrics, evaluation protocols and model architectures. We benchmark leading pathology FMs across diverse downstream classification tasks, establishing the first public reproducible pathology FM leaderboard and advocating for standardized FM evaluation practices.",
        "authors": "kaiko.ai, Ioannis Gatopoulos, Nicolas K\u00e4nzig, Roman Moser, Sebastian Ot\u00e1lora",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.24"
    },
    {
        "number": 131,
        "UID": "S-131",
        "forum": "https://openreview.net/forum?id=VSxcdlVoyd",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Predicting non-visible future tumour from baseline low dose CT using deep learned features",
        "abstract": "Studies have shown that yearly screening with low-dose computed tomography (LDCT) effectively reduces lung cancer mortality (NLST Research Team, 2011b). With the increas- ing number of deep learning tools that are trained on large collection of scans it has been possible to automatically report lung nodules (Venkadesh et al., 2023). We hypothesized that deep learning might also be able to predict the risk of future malignancies based on LDCT imaging in which no tumours are presently visible. This would provide a triage mechanism for identifying patients who would benefit from yearly screening versus those who might attend biannual screening. We use data from The National Lung Screen Trial (NLST) (NLST Research Team, 2011a) and compare the accuracy of multiple pre-trained classification models from the Keras library to predict a slice-by-slice risk of future tumour occurrence. The best performing model can achieved an AUC of 0.741 demonstrating a successful classifier.",
        "authors": "Sheng Yu, Yan Wen, Carolyn Horst, Balaji Ganeshan, Spencer Angus Thomas, Reyer Zwiggelaar, Richard Lee, Xujiong Ye, Matthew Blackledge",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.19"
    },
    {
        "number": 140,
        "UID": "S-140",
        "forum": "https://openreview.net/forum?id=PYNwysgFeP",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "ViM-UNet: Vision Mamba for Biomedical Segmentation",
        "abstract": "CNNs, most notably the UNet, are the default architecture for biomedical segmentation. Transformer-based approaches, such as UNETR, have been proposed to replace them, benefiting from a global field of view, but suffering from larger runtimes and higher parameter counts. The recent Vision Mamba architecture offers a compelling alternative to transformers, also providing a global field of view, but at higher efficiency. Here, we introduce ViM-UNet, a novel segmentation architecture based on it and compare it to UNet and UNETR for two challenging microscopy instance segmentation tasks.\nWe find that it performs similarly or better than UNet, depending on the task, and outperforms UNETR while being more efficient. Our code is open source and documented at https://github.com/constantinpape/torch-em/blob/main/vimunet.md",
        "authors": "Anwai Archit, Constantin Pape",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.11"
    },
    {
        "number": 144,
        "UID": "S-144",
        "forum": "https://openreview.net/forum?id=RnaTcnSI8n",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "On the Importance of Expert Knowledge to Improve Foundation Models for Retinal Fundus Images",
        "abstract": "Foundation models are currently revolutionizing the medical image analysis community. Pre-trained on large data sources, such networks provide efficient transferability to downstream tasks. In this context, a myriad of foundation models leveraging large amounts of general medical data and increasing network sizes are appearing in the literature. In this short paper, we study the importance of incorporating domain-specific expert knowledge during pre-training of specialized foundation models in the context of fundus retina images. In particular, we focus on introducing the expert knowledge-driven vision-language model FLAIR (Silva-Rodriguez et al., 2023), comparing its benefits to larger-scale generalists and domain-specific self-supervised models.",
        "authors": "Julio Silva-Rodr\u00edguez, Hadi Chakor, Riadh kobbi, Jose Dolz, Ismail Ben Ayed",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.22"
    },
    {
        "number": 149,
        "UID": "S-149",
        "forum": "https://openreview.net/forum?id=zDOZ0IhLFF",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "SegmentWithSAM: 3D Slicer Extension for Segment Anything Model (SAM)",
        "abstract": "The development of reliable automated deep learning-based algorithms for the segmentation of medical images is heavily reliant on training data in the form of images along with outlines of the objects of interest. However, manual annotation of medical images is very time-consuming and often requires highly specialized expertise. Here, we provide software that incorporates the recently developed and highly impactful Segment Anything Model (SAM) into the popular software for the visualization and annotation of medical images, 3D Slicer. SAM has been developed to segment any object with prompt-based user guidance. It has been shown to be successful in aiding some annotations in medical imaging. The software described in this paper allows to leverage the power of SAM while using the highly convenient and publicly available 3D Slicer software. Our code is publicly available on https://github.com/mazurowski-lab/SlicerSegmentWithSAM, and it can be installed directly from the Extension Manager of 3D Slicer.",
        "authors": "Zafer Yildiz, Hanxue Gu, Jikai Zhang, Jichen Yang, Maciej A Mazurowski",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.25"
    },
    {
        "number": 15,
        "UID": "S-15",
        "forum": "https://openreview.net/forum?id=GVxHxL3HIp",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models",
        "abstract": "Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-\ntrained large language models has recently emerged as an effective approach to perform\ntransfer learning on computer vision tasks. However, the effectiveness of PEFT on medical\nvision foundation models is still unclear and remains to be explored. As a proof of concept,\nwe conducted a detailed empirical study on applying PEFT to chest radiography foundation\nmodels. Specifically, we delved into LoRA, a representative PEFT method, and compared\nit against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation\nmodels across three well-established chest radiograph datasets. Our results showed that\nLoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using\nfewer than 1% tunable parameters. Combining LoRA with foundation models, we set up\nnew state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of\n80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more\nattention from the community in the use of PEFT for transfer learning on medical imaging\ntasks. Code and models are available at https://github.com/RL4M/MED-PEFT.",
        "authors": "Chenyu Lian, Hong-Yu Zhou, Yizhou Yu, Liansheng Wang",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.26"
    },
    {
        "number": 154,
        "UID": "S-154",
        "forum": "https://openreview.net/forum?id=6RCm2tH2tE",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Monitoring Disease Progression with Stable Diffusion- A Visual Approach",
        "abstract": "Monitoring disease progression is quintessential for effective  healthcare by providing early diagnosis, timely interventions and improved patient outcomes. In the recent years, diffusion models have become extremely popular for synthetic medical image generation.    \nIn this work, we combine Stable Diffusion with image editing through textual guidance and cross-attention reweighing to predict disease progression for patients enrolled for a Retinopathy of Prematurity \n(ROP) disease screening program. Our technique provides effective visualization for monitoring disease severity over time on new subjects",
        "authors": "Sourav Kumar, Shell Xu Hu, Aaron S Coyner, Wei-Chun Lin, Susan Ostmo, Deniz Erdogmus, RV Paul Chan, Michael F. Chiang, J. Peter Campbell, Jayashree Kalpathy-cramer, Timothy Hospedales, Praveer Singh",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.23"
    },
    {
        "number": 156,
        "UID": "S-156",
        "forum": "https://openreview.net/forum?id=s7zGrPfsTR",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "SAM-Geo3D: A Geometrical Method to Extend SAM to 3D",
        "abstract": "Segment Anything Model (SAM) offers a promising approach for image segmentation tasks. However, SAM works in 2D making it less useful when segmenting cross-sectional images, such as MRIs. To address this, we proposed SAM-Geo3D, a geometrical method that extends SAM into the 3D manner. Given a few prompt points on a target component, SAM-Geo3D segments the component through all slices in 3D without onerous deep-learning-based training. We validated SAM-Geo3D on five knee MRI volumes. Results showed that SAM-Geo3D outperforms SAM when using the same, limited number of input prompt points.",
        "authors": "Jikai Zhang, Zafer Yildiz, Hanxue Gu, Haoyu Dong, Maciej A Mazurowski",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.20"
    },
    {
        "number": 159,
        "UID": "S-159",
        "forum": "https://openreview.net/forum?id=2OqtpQmoya",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "CW nnU-Net for Universal Lesion Segmentation Challenge on 3D Computed Tomography",
        "abstract": "3D lesion segmentation of oncological computed tomography (CT) is a crucial step in precisely monitoring changes in lesion/tumor growth, which enables the extraction of meaningful information from medical images, aiding in diagnosis, treatment planning and monitoring of diseases. In this research, we developed a highly efficient and effective CW nnU-Net and ensemble models for 3D lesion segmentation on CT for the Universal Lesion Segmentation (ULS) Challenge, which will be held jointly with 2024 medical imaging with deep learning (MIDL) conference at Paris, France.  The proposed approach was built with a reasonably cheap Nvidia RTX 4080 GPU card and outperformed the baseline models in both development and test phase. In the final test phase, the proposed model ranks as the 3rd place among 577 participants worldwide, achieving a Challenge Score of 0.73, Segmentation DICE of 0.70 and Consistency DICE of 0.79. In the development phase, the proposed CW nnU-Net achieved a Challenge Score of 0.81, Segmentation DICE of 0.78 and Consistency DICE of 0.92.\nFor computational efficiency, CW nnU-Net takes only 3.25s for processing each VOI on the Grand Challenge platform server with a single T4 GPU and less than 2s using a local PC with RTX4080.",
        "authors": "Ching-Wei Wang, Ting-Sheng Su, Hikam Muzakkyi, Yu-Ching Lee",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.12"
    },
    {
        "number": 161,
        "UID": "S-161",
        "forum": "https://openreview.net/forum?id=HxTZr9yA0N",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "The Advanced Toolbox for Multitask Medical Imaging Consistency (ATOMMIC): A framework to facilitate Deep Learning in Magnetic Resonance Imaging",
        "abstract": "Integrating Deep Learning (DL) into medical imaging, particularly in Magnetic Resonance Imaging (MRI), has marked a significant advancement in the field, enhancing the efficiency and accuracy of tasks such as image reconstruction, segmentation, and quantitative parameter map estimation. Despite these advancements, existing frameworks have limited support to perform multiple tasks simultaneously, essential for optimizing the workflow from data acquisition to analysis. Addressing this gap, we introduce the Advanced Toolbox for Multitask Medical Imaging Consistency (ATOMMIC), a novel open-source toolbox designed to facilitate the integration of multiple MRI tasks within a unified MultiTask Learning (MTL) framework. ATOMMIC supports a wide range of DL models and datasets, allowing for seamless and consistent execution of multiple tasks. By enabling joint task execution and supporting complex and real-valued data, ATOMMIC allows to streamline various DL applications in MRI reconstruction and analysis.",
        "authors": "Dimitrios Karkalousos, Ivana Isgum, Henk Marquering, Matthan W. A. Caan",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.01"
    },
    {
        "number": 167,
        "UID": "S-167",
        "forum": "https://openreview.net/forum?id=aBPGMpcgYT",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Evaluation of pseudo-healthy reconstruction for anomaly detection in brain FDG PET",
        "abstract": "We propose an evaluation procedure based on the simulation of realistic abnormal images to validate pseudo-healthy reconstruction methods when no ground truth is available. We apply this framework to the reconstruction of 3D brain FDG PET using a convolutional variational autoencoder. This work has recently been published at MELBA.",
        "authors": "Ravi Hassanaly, Camille Brianceau, Ma\u00eblys Solal, Olivier Colliot, Ninon Burgos",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.32"
    },
    {
        "number": 169,
        "UID": "S-169",
        "forum": "https://openreview.net/forum?id=ZlNe82sCnH",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Consistent Direct Diffusion Bridge with Injection for MRI Reconstruction",
        "abstract": "In MRI, lengthy acquisition times often cause motion artifacts. We propose a new method, Consistent Direct Diffusion Bridge with Injection (CCDBI), which leverages diffusionbased image priors to reconstruct MRI from undersampled k-space data. Unlike traditional diffusion methods starting with Gaussian noise, CCDBI begins sampling from actual measurements, improving accuracy by aligning with the target image. By combining information from noisy image domain and k-space adaptively, CCDBI ensures consistency in the Direct Diffusion Bridge (DDB), enhancing reconstruction quality. Experimental results on IXI and OASIS-2 datasets demonstrate CCDBI\u2019s superiority over existing algorithms.",
        "authors": "Pyatkovskiy Sergey, Jaejun Yoo",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.02"
    },
    {
        "number": 18,
        "UID": "S-18",
        "forum": "https://openreview.net/forum?id=OP2Cf9ypTK",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations",
        "abstract": "Curating annotations for medical image segmentation is a labor-intensive task that requires domain expertise, resulting in \"narrowly\" focused deep learning (DL) models with limited translational utility. We explore the potential of the Segment Anything Model (SAM) for crowd-sourcing \"sparse\" annotations from non-experts to generate \"dense\" segmentation masks for training 3D nnU-Net models. Our results indicate that while SAM-generated annotations exhibit high mean Dice scores compared to ground-truth annotations, SAM nnU-Net models perform significantly worse than ground-truth nnU-Net models.",
        "authors": "Pranav Kulkarni, Adway Kanhere, Dharmam Savani, Andrew Chan, Devina Chatterjee, Paul Yi, Vishwa Sanjay Parekh",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.29"
    },
    {
        "number": 180,
        "UID": "S-180",
        "forum": "https://openreview.net/forum?id=JOwJe2DGp4",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Can Frozen Transformers in Large Language Models Help with Medical Image Segmentation?",
        "abstract": "Transformer models shine in medical image segmentation by harnessing their self-attention mechanism to capture global information, thus boosting segmentation accuracy. Recent research has unveiled that large language models (LLMs), trained solely on text, surprisingly excel at visual tasks even without language, through a simple strategy: integrating a frozen transformer block from pre-trained LLMs as a direct visual token processor. This paper applies this approach to medical image segmentation by combining frozen transformer blocks with TransUNet. Experiments are conducted on BTCV, ACDC, ISIC 2017, CVC-ClinicDB, CVC-ColonDB and BUSI datasets, demonstrating some improvements compared to the baseline.",
        "authors": "Juntao Jiang, Yong Liu",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.28"
    },
    {
        "number": 2,
        "UID": "S-2",
        "forum": "https://openreview.net/forum?id=LbNeleKFaM",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Validating Noisy Label Learning in Noisy Clinical Datasets",
        "abstract": "Noise in ground truth labels limits model performance; in response, learning with noisy labels (LNL) has received much attention in recent years. However, most research has been applied to competition datasets where a clean (noiseless) test set is available. A gap exists in applying LNL to practical datasets such as colposcopy, where such clean sets are not available. By synthesizing additional noise, targeted to mimic real-world errors, to the training labels, and using an imperfect test set, we demonstrate that LNL methods outperform traditional learning, thus bridging this gap.",
        "authors": "Andr\u00e9 Aquilina, Emmanouil Papagiannakis",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.13"
    },
    {
        "number": 25,
        "UID": "S-25",
        "forum": "https://openreview.net/forum?id=acNu3glOTh",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Rethinking Perceptual Metrics for Medical Image Translation",
        "abstract": "Modern medical image translation methods use generative models for tasks such as the conversion of CT images to MRI. Evaluating these methods typically relies on some chosen downstream task in the target domain, such as segmentation. On the other hand, task-agnostic metrics are attractive, such as the network feature-based perceptual metrics (e.g., FID) that are common to image translation in general computer vision. In this paper, we investigate evaluation metrics for medical image translation on two medical image translation tasks (GE breast MRI to Siemens breast MRI and lumbar spine MRI to CT), tested on various state-of-the-art translation methods. We show that perceptual metrics do not generally correlate with segmentation metrics due to them extending poorly to the anatomical constraints of this sub-field, with FID being especially inconsistent. However, we find that the lesser-used *pixel-level* SWD metric may be useful for subtle intra-modality translation. Our results demonstrate the need for further research into helpful metrics for medical image translation.",
        "authors": "Nicholas Konz, Yuwen Chen, Hanxue Gu, Haoyu Dong, Maciej A Mazurowski",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.14"
    },
    {
        "number": 26,
        "UID": "S-26",
        "forum": "https://openreview.net/forum?id=CjiWTuFx3r",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Boost Your Medical Deep-Learning Training By Lazy Loading",
        "abstract": "In recent years, the growing volume size of medical datasets has posed a significant challenge for medical deep learning training pipelines, often leading to inefficiencies stemming from data I/O bottlenecks. Addressing this issue, we present a simply yet effective trick, lazy loading strategy, leveraging memory-mapping mechanisms to boost training processes. By dynamically loading only the target slices of large medical datasets into active memory, our method minimizes the reading time and conserves memory. This paper mainly aims to remind community to realize the advantages of the lazy loading strategy, which could substantially boost the efficiency of deep learning training process in the medical domain.",
        "authors": "Chenglong Wang, Chengxiu Zhang, Yun Liu, Guang Yang",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.04"
    },
    {
        "number": 31,
        "UID": "S-31",
        "forum": "https://openreview.net/forum?id=pZYPNhuefs",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Meta-Learning for Segmention of In Situ Hybridization Gene Expression Images",
        "abstract": "Segmentation of biomedical images is often ambiguous and complicated by noise, varying contrasts, and imaging artifacts. We address the challenge of segmenting images of brain tissue in which gene expression has been localized using in situ hybridization. Since gene expression patterns differ widely between genes, it can be difficult to correctly discriminate pixels positive for gene expression. In testing different segmentation networks, we observed that each network had its own trade-offs between sensitivity and precision. To exploit the benefits of all trained networks, we developed a meta-network that learns to combine multiple segmentation maps from diverse segmentation architectures to generate a final segmentation that best matches the ground-truth label. In our experiments, the meta-network outperforms ensembles that simply average segmentation maps.",
        "authors": "Charissa Poon, Michal Byra, Tomomi Shimogori, Henrik Skibbe",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.05"
    },
    {
        "number": 38,
        "UID": "S-38",
        "forum": "https://openreview.net/forum?id=IHmvNgX34A",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Ultrasound tumor detection using an adapted Mask-RCNN with a continuous objectness score",
        "abstract": "This paper introduces a novel approach for training Mask-RCNN models using a normalized continuous\nobjectness score and corresponding loss function, eliminating the need for binary objectness labels. The\nmethod is evaluated on an ultrasound dataset of breast and colorectal tumors samples, achieving a\nprecision of 0.963, sensitivity of 0.974, specificity of 0.960 and IoU of 0.651, improving the precision\nand specificity with comparable sensitivity and IoU compared to a conventional Mask-RCNN baseline.",
        "authors": "Mark Wijkhuizen, Lennard van Karnenbeek, Freija Geldof, Theo J.M. Ruers, Behdad Dashtbozorg",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.06"
    },
    {
        "number": 39,
        "UID": "S-39",
        "forum": "https://openreview.net/forum?id=dBh5Sarrlj",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "An Uncertainty-Distillation- and Voxel-Contrast-based Framework for One-shot Segmentation of Novel White Matter Tracts",
        "abstract": "Diffusion-MRI-based white matter (WM) tract segmentation plays an important role in analyzing WM characteristics in healthy and diseased brains. The uncommon (novel) tract segmentation is important to the success of clinical brain operation and the reduction of postoperative complications. The massive WM tract annotations are time-consuming and need experienced neuroanatomists. Novel tract segmentation using only one annotated scan alleviates the above problems but is challenging.  Existing fine-tuning-based studies achieve promising results but suffer from the feature overlap problem. In the work, we propose an uncertainty-distillation- and voxel-contrast-based one-shot novel WM tract segmentation framework, which includes an uncertainty distillation module to transfer semantic segmentation knowledge from base tracts to novel tracts and a voxel-wise multi-label contrastive module to adjust the feature embedding space so as to alleviate the feature overlap problem. We compare our method with several state-of-the-art (SOTA) methods that are designed to predict novel tract segmentation. The experimental results demonstrate that our method improves the one-shot segmentation accuracy of novel tracts in five experimental settings.",
        "authors": "Hao Xu, Tengfei Xue, Dongnan Liu, Fan Zhang, Carl-fredrik Westin, Ron Ron Kikinis, Lauren Jean O'Donnell, Weidong Cai",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.15"
    },
    {
        "number": 42,
        "UID": "S-42",
        "forum": "https://openreview.net/forum?id=LrihI0cqZm",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Multi-pathology MRI lesion segmentation in a multi-centre cohort of patients with focal epilepsy: a MELD study",
        "abstract": "Drug-resistant focal epilepsy can be caused by structural lesions for which surgery can be curative if the lesion is found and fully resected. We built a large multi-centre MRI dataset of 1181 patients with focal epilepsy and 1009 healthy controls to train a state-of-the-art nnU-Net model to segment a range of aetiologies on T1w MRI scans. The model was able to detect different aetiological causes of focal epilepsy with a sensitivity of 73\\% in patients and a specificity of 90\\% in controls.",
        "authors": "Mathilde Ripart, MELD consortium, Sophie Adler, Konrad Wagstyl",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.16"
    },
    {
        "number": 52,
        "UID": "S-52",
        "forum": "https://openreview.net/forum?id=CWC4amo4qn",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "A Novel Dataset for Nuclei Segmentation in Melanoma Histopathology",
        "abstract": "The presence of tumor-infiltrating lymphocytes (TILs) in melanoma is associated with decreased recurrence of primary melanoma and increased survival in metastatic melanoma patients treated with immune checkpoint inhibition. Existing nuclei segmentation models performance is low due to the ability of melanocytes to mimic other cell types and due to existing melanoma specific models utilizing older, sub-optimal techniques. In addition, existing models do not provide tissue annotations necessary for determining the localization of TILs whereas this might also hold predictive value. To address this, we created a melanoma specific dataset with nuclei and tissue annotations. In this paper we describe the methodology used to create the dataset. In addition, we provide preliminary baseline benchmarks.",
        "authors": "Mark Schuiveling, Willeke A.M. Blokx, Gerben E. Breimer, Karijn P.M. Suijkerbuijk, Daniel Eek, Mitko Veta",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.07"
    },
    {
        "number": 72,
        "UID": "S-72",
        "forum": "https://openreview.net/forum?id=FoXmgwbuIt",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Segment Anything Model for Instance Segmentation in Kidney Histopathology Images",
        "abstract": "Kidney transplantation offers a significant improvement in the quality of life for individuals\nwith irreversible kidney failure. Early detection of rejection through pathologists\u2019 assessment\nof kidney biopsies is critical to ensure long-term graft survival. Traditional assessment\nmethods rely on semi-quantitative estimations from a pathologist while implementing deep\nlearning models holds promise for providing more accurate measurements. Large annotated\ndatasets, required to train such models, are challenging to obtain for kidney tissue. In this\nstudy, we fine-tune and modify the Segment Anything Model (SAM) to facilitate instance\nsegmentation on whole-slide imaging (WSI) data. Leveraging SAM\u2019s zero-shot capability,\nwe accelerate dataset creation by automatically obtaining annotations which we refine and\nlabel. We demonstrate promising results with limited annotated slides for training. Additionally,\nour approach allows for iterative dataset expansion to enhance model performance\nover time. Code is available at: https://github.com/JurreWeijer/SAM-Nephro.",
        "authors": "Jurre Weijer, Sanne Truijen, Tri Nguyen, Mitko Veta, Nikolas Stathonikos",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.30"
    },
    {
        "number": 83,
        "UID": "S-83",
        "forum": "https://openreview.net/forum?id=1l2p85loZw",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Impact of Layer Selection in Histopathology Foundation Models on Downstream Task Performance",
        "abstract": "Self-supervised vision transformer models trained on large histopathology datasets are increasingly used as feature encoders for downstream tasks. However, their final layer might not be optimal for all tasks due to the mismatch between the pre-training and downstream objectives. We investigate the influence of the layer selection in five public, transformer-based histopathology encoders on downstream task performance both on patch- and slide- level. Our results demonstrate that choosing a different layer for feature encoding can lead to performance improvements up to eleven percent depending on the task and the model.",
        "authors": "Witali Aswolinskiy, Martin Paulikat, Christian Aichmueller",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.21"
    },
    {
        "number": 85,
        "UID": "S-85",
        "forum": "https://openreview.net/forum?id=u6pyk0RIpL",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Learning CT Segmentation from Label Masks Only",
        "abstract": "Training segmentation models for CT scans in the absence of input data is a challenging problem. Methods based on generative adversarial networks translate images from other modalities but still require additional data and training. Synthesizing images directly from segmentation masks using heuristics can overcome this limitation. However, capabilities for model generalization remain underexplored for these methods. In this study, we generate synthetic data for liver segmentation using organ labels and prior CT knowledge. Ground truth labels serve as a source of information about global structures and are filled with artificial textures in various settings. Segmentation models trained on synthetic data demonstrate sufficient generalization to real CT data, highlighting a perspective of a simple yet powerful approach to data bootstrapping.",
        "authors": "Artyom Tsanda, Hannes Nickisch, Tobias Wissel, Tobias Klinder, Tobias Knopp, Michael Grass",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.08"
    },
    {
        "number": 91,
        "UID": "S-91",
        "forum": "https://openreview.net/forum?id=B3xO0c2Q3h",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Unraveling Systematic Biases in Brain Segmentation: Insights from Synthetic Training",
        "abstract": "This study examines how the quality of ground truth labels affects brain MRI segmentation models. We investigate the potential of synthetic learning to mitigate systematic biases present in training labels. Through a validation on high-quality datasets, in the Putamen region, known for systematic segmentation errors like the inclusion of parts of the Claustrum, we demonstrate the effectiveness of the synthetic data approach in correcting these errors and enhancing segmentation accuracy. \nOur findings highlight the limitations of pseudo-ground truth labels derived from automated techniques and underscores the importance of precise, expert-validated labels for accurate, unbiased validation.",
        "authors": "Romain Valabregue, Ines Khemir, Guillaume Auzias, Fran\u00e7ois Rousseau, Mehdi OUNISSI",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.09"
    },
    {
        "number": 94,
        "UID": "S-94",
        "forum": "https://openreview.net/forum?id=SXDCFv29pk",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "From Generalist to Specialist: Incorporating Domain-Knowledge into Flamingo for Chest X-Ray Report Generation",
        "abstract": "Automating the generation of accurate and reliable radiological reports from chest X-\nray images represents a significant challenge in medical image computing. In this context,\nVision-Language Models (VLMs), particularly the Flamingo architecture which achieves\nstate-of-the-art performance across various vision-language tasks, offers promising solu-\ntions. This study evaluates the effectiveness of OpenFlamingo and its medical adaptation\nMedFlamingo, a version further pre-trained on medical data, in generating radiological\nreports. Our evaluation compares the zero-shot capabilities of OpenFlamingo and Med-\nFlamingo against fine-tuning and training from scratch. Our results demonstrate that\nfine-tuning consistently boosts model performance, with fine-tuned MedFlamingo outper-\nforming its OpenFlamingo counterpart. Moreover, while training Flamingo from scratch\ndoes not match the efficacy of fine-tuning, it nevertheless surpasses zero-shot performance.\nThis study underscores the potential of domain-specific fine-tuning in enhancing automated\nradiological report generation, paving the way for more accurate and efficient diagnostic\nworkflows.",
        "authors": "Raphael Stock, Stefan Denner, Yannick Kirchhoff, Constantin Ulrich, Maximilian Rouven Rokuss, Saikat Roy, Nico Disch, Klaus Maier-Hein",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.31"
    },
    {
        "number": 99,
        "UID": "S-99",
        "forum": "https://openreview.net/forum?id=jQeqDrkANf",
        "Track": "Short Paper",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Spirochetosis detection in colon histopathology images via fine-tuning and boosting techniques using foundation models",
        "abstract": "Spirochetes are bacteria that can be found on the boundaries of colon epithelial tissue, causing several diseases ranging from spirochetosis, inflammatory bowel disease, to cancer. Despite their relevance, spirochetes often remain undetected in histological analysis.\nWe propose the first computational pathology approach to characterize spirochetes, leveraging prior spatial knowledge to detect spirochetes in whole-slide images of colon polyps and biopsies, and differentiate these bacteria as belonging to normal or abnormal tissue.\nWe focus on transfer learning by fine-tuning state-of-the-art computational pathology foundation models and by training an additional XGBoost classifier on downstream tasks.",
        "authors": "Agata Polejowska, Fazael Ayatollahi, Ayse Selcen Oguz Erdogan, Francesco Ciompi, Annemarie Boleij",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.2 - S.27"
    },
    {
        "number": 1,
        "UID": "MO-2024-005",
        "forum": "https://www.melba-journal.org/papers/2024:005.html",
        "Track": "MELBA Journal-to-conference",
        "Session": "Oral 3.3 - MELBA Journal-to-conference",
        "Final Decision": "Oral",
        "title": "HyperPredict: Estimating Hyperparameter Effects for Instance-Specific Regularization in Deformable Image Registration",
        "abstract": "Methods for medical image registration infer geometric transformations that align pairs, or groups, of images by maximising an image similarity metric. This problem is ill-posed as several solutions may have equivalent likelihoods, also optimising purely for image similarity can yield implausible deformable transformations. For these reasons regularization terms are essential to obtain meaningful registration results. However, this requires the introduction of at least one hyperparameter, often termed \u03bb, which serves as a trade-off between loss terms. In some approaches and situations, the quality of the estimated transformation greatly depends on hyperparameter choice, and different choices may be required depending on the characteristics of the data. Analyzing the effect of these hyperparameters requires labelled data, which is not commonly available at test-time. In this paper, we propose a novel method for evaluating the influence of hyperparameters and subsequently selecting an optimal value for given pair of images. Our approach, which we call HyperPredict, implements a Multi-Layer Perceptron that learns the effect of selecting particular hyperparameters for registering an image pair by predicting the resulting segmentation overlap and measures of deformation smoothness. This approach enables us to select optimal hyperparameters at test time without requiring labelled data, removing the need for a one-size-fits-all cross-validation approach. Furthermore, the criteria used to define optimal hyperparameter is flexible post-training, allowing us to efficiently choose specific properties (e.g. overlap of specific anatomical regions of interest, smoothness/plausibility of the final displacement field). We evaluate our proposed method on the OASIS brain MR standard benchmark dataset using a recent deep learning approach (cLapIRN) and an algorithmic method (Niftyreg). Our results demonstrate good performance in predicting the effects of regularization hyperparameters and highlight the benefits of our image-pair specific approach to hyperparameter selection.",
        "authors": "Aisha Lawal Shuaibu, Ivor J. A. Simpson",
        "Time": "Day 3 \u2014 16:30-17:15",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.1 - M.01",
        "Chairs": "Tal Arbel & Hugues Talbot"
    },
    {
        "number": 1,
        "UID": "MP-2024-005",
        "forum": "https://www.melba-journal.org/papers/2024:005.html",
        "Track": "MELBA Journal-to-conference",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "HyperPredict: Estimating Hyperparameter Effects for Instance-Specific Regularization in Deformable Image Registration",
        "abstract": "Methods for medical image registration infer geometric transformations that align pairs, or groups, of images by maximising an image similarity metric. This problem is ill-posed as several solutions may have equivalent likelihoods, also optimising purely for image similarity can yield implausible deformable transformations. For these reasons regularization terms are essential to obtain meaningful registration results. However, this requires the introduction of at least one hyperparameter, often termed \u03bb, which serves as a trade-off between loss terms. In some approaches and situations, the quality of the estimated transformation greatly depends on hyperparameter choice, and different choices may be required depending on the characteristics of the data. Analyzing the effect of these hyperparameters requires labelled data, which is not commonly available at test-time. In this paper, we propose a novel method for evaluating the influence of hyperparameters and subsequently selecting an optimal value for given pair of images. Our approach, which we call HyperPredict, implements a Multi-Layer Perceptron that learns the effect of selecting particular hyperparameters for registering an image pair by predicting the resulting segmentation overlap and measures of deformation smoothness. This approach enables us to select optimal hyperparameters at test time without requiring labelled data, removing the need for a one-size-fits-all cross-validation approach. Furthermore, the criteria used to define optimal hyperparameter is flexible post-training, allowing us to efficiently choose specific properties (e.g. overlap of specific anatomical regions of interest, smoothness/plausibility of the final displacement field). We evaluate our proposed method on the OASIS brain MR standard benchmark dataset using a recent deep learning approach (cLapIRN) and an algorithmic method (Niftyreg). Our results demonstrate good performance in predicting the effects of regularization hyperparameters and highlight the benefits of our image-pair specific approach to hyperparameter selection.",
        "authors": "Aisha Lawal Shuaibu, Ivor J. A. Simpson",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.1 - M.01"
    },
    {
        "number": 2,
        "UID": "MO-2024-010",
        "forum": "https://www.melba-journal.org/papers/2024:010.html",
        "Track": "MELBA Journal-to-conference",
        "Session": "Oral 3.3 - MELBA Journal-to-conference",
        "Final Decision": "Oral",
        "title": "Leveraging SO(3)-steerable convolutions for pose-robust semantic segmentation in 3D medical data",
        "abstract": "Convolutional neural networks (CNNs) allow for parameter sharing and translational equivariance by using convolutional kernels in their linear layers. By restricting these kernels to be SO(3)-steerable, CNNs can further improve parameter sharing and equivariance. These equivariant convolutional layers have several advantages over standard convolutional layers, including increased robustness to unseen poses, smaller network size, and improved sample efficiency. Despite this, most segmentation networks used in medical image analysis continue to rely on standard convolutional kernels. In this paper, we present a new family of segmentation networks that use equivariant voxel convolutions based on spherical harmonics. These SE(3)-equivariant volumetric segmentation networks, which are robust to data poses not seen during training, do not require rotation-based data augmentation during training. In addition, we demonstrate improved segmentation performance in MRI brain tumor and healthy brain structure segmentation tasks, with enhanced robustness to reduced amounts of training data and improved parameter efficiency. Code to reproduce our results, and to implement the equivariant segmentation networks for other tasks is available at http://github.com/SCAN-NRAD/e3nn_Unet.",
        "authors": "Ivan Diaz, Mario Geiger, Richard Iain McKinley",
        "Time": "Day 3 \u2014 16:30-17:15",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.1 - M.02",
        "Chairs": "Tal Arbel & Hugues Talbot"
    },
    {
        "number": 2,
        "UID": "MP-2024-010",
        "forum": "https://www.melba-journal.org/papers/2024:010.html",
        "Track": "MELBA Journal-to-conference",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Leveraging SO(3)-steerable convolutions for pose-robust semantic segmentation in 3D medical data",
        "abstract": "Convolutional neural networks (CNNs) allow for parameter sharing and translational equivariance by using convolutional kernels in their linear layers. By restricting these kernels to be SO(3)-steerable, CNNs can further improve parameter sharing and equivariance. These equivariant convolutional layers have several advantages over standard convolutional layers, including increased robustness to unseen poses, smaller network size, and improved sample efficiency. Despite this, most segmentation networks used in medical image analysis continue to rely on standard convolutional kernels. In this paper, we present a new family of segmentation networks that use equivariant voxel convolutions based on spherical harmonics. These SE(3)-equivariant volumetric segmentation networks, which are robust to data poses not seen during training, do not require rotation-based data augmentation during training. In addition, we demonstrate improved segmentation performance in MRI brain tumor and healthy brain structure segmentation tasks, with enhanced robustness to reduced amounts of training data and improved parameter efficiency. Code to reproduce our results, and to implement the equivariant segmentation networks for other tasks is available at http://github.com/SCAN-NRAD/e3nn_Unet.",
        "authors": "Ivan Diaz, Mario Geiger, Richard Iain McKinley",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.1 - M.02"
    },
    {
        "number": 3,
        "UID": "MO-2024-001",
        "forum": "https://www.melba-journal.org/papers/2024:001.html",
        "Track": "MELBA Journal-to-conference",
        "Session": "Oral 3.3 - MELBA Journal-to-conference",
        "Final Decision": "Oral",
        "title": "Score-Based Generative Models for PET Image Reconstruction",
        "abstract": "Score-based generative models have demonstrated highly promising results for medical image reconstruction tasks in magnetic resonance imaging or computed tomography. However, their application to Positron Emission Tomography (PET) is still largely unexplored. PET image reconstruction involves a variety of challenges, including Poisson noise with high variance and a wide dynamic range. To address these challenges, we propose several PET-specific adaptations of score-based generative models. The proposed framework is developed for both 2D and 3D PET. In addition, we provide an extension to guided reconstruction using magnetic resonance images. We validate the approach through extensive 2D and 3D in-silico experiments with a model trained on patient-realistic data without lesions, and evaluate on data without lesions as well as out-of-distribution data with lesions. This demonstrates the proposed method\u2019s robustness and significant potential for improved PET reconstruction.",
        "authors": "Imraj RD Singh, Alexander Denker, Riccardo Barbano, \u017deljko Kereta, Bangti Jin, Kris Thielemans, Peter Maass, Simon Arridge",
        "Time": "Day 3 \u2014 16:30-17:15",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.1 - M.03",
        "Chairs": "Tal Arbel & Hugues Talbot"
    },
    {
        "number": 3,
        "UID": "MO-2024-001",
        "forum": "https://www.melba-journal.org/papers/2024:001.html",
        "Track": "MELBA Journal-to-conference",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Score-Based Generative Models for PET Image Reconstruction",
        "abstract": "Score-based generative models have demonstrated highly promising results for medical image reconstruction tasks in magnetic resonance imaging or computed tomography. However, their application to Positron Emission Tomography (PET) is still largely unexplored. PET image reconstruction involves a variety of challenges, including Poisson noise with high variance and a wide dynamic range. To address these challenges, we propose several PET-specific adaptations of score-based generative models. The proposed framework is developed for both 2D and 3D PET. In addition, we provide an extension to guided reconstruction using magnetic resonance images. We validate the approach through extensive 2D and 3D in-silico experiments with a model trained on patient-realistic data without lesions, and evaluate on data without lesions as well as out-of-distribution data with lesions. This demonstrates the proposed method\u2019s robustness and significant potential for improved PET reconstruction.",
        "authors": "Imraj RD Singh, Alexander Denker, Riccardo Barbano, \u017deljko Kereta, Bangti Jin, Kris Thielemans, Peter Maass, Simon Arridge",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.1 - M.03"
    },
    {
        "number": 4,
        "UID": "MO-2023-002",
        "forum": "https://www.melba-journal.org/papers/2023:002.html",
        "Track": "MELBA Journal-to-conference",
        "Session": "Poster 3.2",
        "Final Decision": "Poster",
        "title": "Are we using appropriate segmentation metrics? Identifying correlates of human expert perception for CNN training beyond rolling the DICE coefficient",
        "abstract": "Metrics optimized in complex machine learning tasks are often selected in an ad-hoc manner. It is unknown how they align with human expert perception. We explore the correlations between established quantitative segmentation quality metrics and qualitative evaluations by professionally trained human raters. Therefore, we conduct psychophysical experiments for two complex biomedical semantic segmentation problems. We discover that current standard metrics and loss functions correlate only moderately with the segmentation quality assessment of experts. Importantly, this effect is particularly pronounced for clinically relevant structures, such as the enhancing tumor compartment of glioma in brain magnetic resonance and grey matter in ultrasound imaging. It is often unclear how to optimize abstract metrics, such as human expert perception, in convolutional neural network (CNN) training. To cope with this challenge, we propose a novel strategy employing techniques of classical statistics to create complementary compound loss functions to better approximate human expert perception. Across all rating experiments, human experts consistently scored computer-generated segmentations better than the human-curated reference labels. Our results, therefore, strongly question many current practices in medical image segmentation and provide meaningful cues for future research.",
        "authors": "Florian Kofler, Ivan Ezhov, Fabian Isensee, Fabian Balsiger, Christoph Berger, Maximilian Koerner, Beatrice Demiray, Julia Rackerseder, Johannes Paetzold, Hongwei Li, Suprosanna Shit, Richard McKinley, Marie Piraud, Spyridon Bakas, Claus Zimmer, Nassir Navab, Jan Kirschke, Benedikt Wiestler, Bjoern Menze",
        "Time": "Day 3 \u2014 15:30-16:30",
        "Poster time": "Day 3 \u2014 15:30-16:30",
        "Poster ID": "Poster 3.1 - M.04"
    }
]